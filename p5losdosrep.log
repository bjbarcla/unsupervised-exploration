"Dataset","Xval Accuracy","Xval Precision","Xval F1 Score","Xval Recall","Xval Aggregate","Test Set Accuracy","Test Set Precision","Test Set F1 Score","Test Set Recall","Test Set Aggregate","Train Time"
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.pkl
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.traintime
"ds4 (Unmodified)",0.885,0.871,0.827,0.799,0.846,0.845,0.803,0.765,0.743,0.789,154.79517817497253
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-30.pkl
Iteration 1, loss = 0.52721885
Iteration 2, loss = 0.43816442
Iteration 3, loss = 0.41326021
Iteration 4, loss = 0.39687683
Iteration 5, loss = 0.38534292
Iteration 6, loss = 0.37626501
Iteration 7, loss = 0.36938238
Iteration 8, loss = 0.36351645
Iteration 9, loss = 0.35884815
Iteration 10, loss = 0.35523206
Iteration 11, loss = 0.35216342
Iteration 12, loss = 0.34911500
Iteration 13, loss = 0.34698072
Iteration 14, loss = 0.34465197
Iteration 15, loss = 0.34316102
Iteration 16, loss = 0.34105160
Iteration 17, loss = 0.33979452
Iteration 18, loss = 0.33832988
Iteration 19, loss = 0.33683174
Iteration 20, loss = 0.33552757
Iteration 21, loss = 0.33419556
Iteration 22, loss = 0.33351626
Iteration 23, loss = 0.33249587
Iteration 24, loss = 0.33122236
Iteration 25, loss = 0.33009917
Iteration 26, loss = 0.32999986
Iteration 27, loss = 0.32910819
Iteration 28, loss = 0.32850083
Iteration 29, loss = 0.32783501
Iteration 30, loss = 0.32754520
Iteration 31, loss = 0.32683017
Iteration 32, loss = 0.32665067
Iteration 33, loss = 0.32556088
Iteration 34, loss = 0.32522611
Iteration 35, loss = 0.32438246
Iteration 36, loss = 0.32485486
Iteration 37, loss = 0.32409989
Iteration 38, loss = 0.32337248
Iteration 39, loss = 0.32286572
Iteration 40, loss = 0.32274215
Iteration 41, loss = 0.32239637
Iteration 42, loss = 0.32167948
Iteration 43, loss = 0.32131837
Iteration 44, loss = 0.32184441
Iteration 45, loss = 0.32070700
Iteration 46, loss = 0.32081990
Iteration 47, loss = 0.32046730
Iteration 48, loss = 0.31979466
Iteration 49, loss = 0.31925548
Iteration 50, loss = 0.31909189
Iteration 51, loss = 0.31887561
Iteration 52, loss = 0.31880710
Iteration 53, loss = 0.31871974
Iteration 54, loss = 0.31846778
Iteration 55, loss = 0.31812705
Iteration 56, loss = 0.31795224
Iteration 57, loss = 0.31750794
Iteration 58, loss = 0.31788960
Iteration 59, loss = 0.31685210
Iteration 60, loss = 0.31804258
Iteration 61, loss = 0.31666714
Iteration 62, loss = 0.31632087
Iteration 63, loss = 0.31639504
Iteration 64, loss = 0.31605015
Iteration 65, loss = 0.31572774
Iteration 66, loss = 0.31611639
Iteration 67, loss = 0.31633156
Iteration 68, loss = 0.31572302
Iteration 69, loss = 0.31476342
Iteration 70, loss = 0.31484296
Iteration 71, loss = 0.31520534
Iteration 72, loss = 0.31461359
Iteration 73, loss = 0.31531180
Iteration 74, loss = 0.31403008
Iteration 75, loss = 0.31376627
Iteration 76, loss = 0.31326772
Iteration 77, loss = 0.31401660
Iteration 78, loss = 0.31320113
Iteration 79, loss = 0.31359897
Iteration 80, loss = 0.31333339
Iteration 81, loss = 0.31323587
Iteration 82, loss = 0.31322168
Iteration 83, loss = 0.31260319
Iteration 84, loss = 0.31265837
Iteration 85, loss = 0.31265071
Iteration 86, loss = 0.31293517
Iteration 87, loss = 0.31203533
Iteration 88, loss = 0.31220152
Iteration 89, loss = 0.31218880
Iteration 90, loss = 0.31222759
Iteration 91, loss = 0.31146992
Iteration 92, loss = 0.31184401
Iteration 93, loss = 0.31121152
Iteration 94, loss = 0.31082210
Iteration 95, loss = 0.31123675
Iteration 96, loss = 0.31070033
Iteration 97, loss = 0.31075119
Iteration 98, loss = 0.31078837
Iteration 99, loss = 0.31079993
Iteration 100, loss = 0.31047057
Iteration 101, loss = 0.31073111
Iteration 102, loss = 0.30991741
Iteration 103, loss = 0.31067547
Iteration 104, loss = 0.31047062
Iteration 105, loss = 0.31058906
Iteration 106, loss = 0.30993109
Iteration 107, loss = 0.30982702
Iteration 108, loss = 0.30934541
Iteration 109, loss = 0.30944812
Iteration 110, loss = 0.30887005
Iteration 111, loss = 0.31020394
Iteration 112, loss = 0.30944482
Iteration 113, loss = 0.30956777
Iteration 114, loss = 0.30864696
Iteration 115, loss = 0.30947032
Iteration 116, loss = 0.30859702
Iteration 117, loss = 0.30924140
Iteration 118, loss = 0.30866187
Iteration 119, loss = 0.30800554
Iteration 120, loss = 0.30867743
Iteration 121, loss = 0.30808678
Iteration 122, loss = 0.30835876
Iteration 123, loss = 0.30830531
Iteration 124, loss = 0.30787382
Iteration 125, loss = 0.30781538
Iteration 126, loss = 0.30750663
Iteration 127, loss = 0.30771397
Iteration 128, loss = 0.30773279
Iteration 129, loss = 0.30696893
Iteration 130, loss = 0.30688400
Iteration 131, loss = 0.30762531
Iteration 132, loss = 0.30717479
Iteration 133, loss = 0.30694846
Iteration 134, loss = 0.30666710
Iteration 135, loss = 0.30695493
Iteration 136, loss = 0.30680670
Iteration 137, loss = 0.30668815
Iteration 138, loss = 0.30706488
Iteration 139, loss = 0.30672964
Iteration 140, loss = 0.30618405
Iteration 141, loss = 0.30625645
Iteration 142, loss = 0.30614037
Iteration 143, loss = 0.30560211
Iteration 144, loss = 0.30600577
Iteration 145, loss = 0.30611679
Iteration 146, loss = 0.30621537
Iteration 147, loss = 0.30580759
Iteration 148, loss = 0.30569703
Iteration 149, loss = 0.30567734
Iteration 150, loss = 0.30538542
Iteration 151, loss = 0.30505150
Iteration 152, loss = 0.30542454
Iteration 153, loss = 0.30489207
Iteration 154, loss = 0.30540491
Iteration 155, loss = 0.30566546
Iteration 156, loss = 0.30537494
Iteration 157, loss = 0.30599170
Iteration 158, loss = 0.30427691
Iteration 159, loss = 0.30531555
Iteration 160, loss = 0.30452352
Iteration 161, loss = 0.30475959
Iteration 162, loss = 0.30506549
Iteration 163, loss = 0.30406970
Iteration 164, loss = 0.30498472
Iteration 165, loss = 0.30433337
Iteration 166, loss = 0.30443744
Iteration 167, loss = 0.30435398
Iteration 168, loss = 0.30446830
Iteration 169, loss = 0.30363719
Iteration 170, loss = 0.30375858
Iteration 171, loss = 0.30349379
Iteration 172, loss = 0.30378959
Iteration 173, loss = 0.30397689
Iteration 174, loss = 0.30295122
Iteration 175, loss = 0.30368891
Iteration 176, loss = 0.30374639
Iteration 177, loss = 0.30359833
Iteration 178, loss = 0.30341768
Iteration 179, loss = 0.30386364
Iteration 180, loss = 0.30348709
Iteration 181, loss = 0.30336361
Iteration 182, loss = 0.30332485
Iteration 183, loss = 0.30297821
Iteration 184, loss = 0.30329255
Iteration 185, loss = 0.30311299
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_30d_kmeans_augment-iter-1.pkl
"ds4 (pca_30d_kmeans_augment)",0.870,0.829,0.815,0.803,0.829,0.849,0.797,0.784,0.773,0.801,121.02192497253418
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-30.pkl
Iteration 1, loss = 0.52721885
Iteration 2, loss = 0.43816442
Iteration 3, loss = 0.41326021
Iteration 4, loss = 0.39687683
Iteration 5, loss = 0.38534292
Iteration 6, loss = 0.37626501
Iteration 7, loss = 0.36938238
Iteration 8, loss = 0.36351645
Iteration 9, loss = 0.35884815
Iteration 10, loss = 0.35523206
Iteration 11, loss = 0.35216342
Iteration 12, loss = 0.34911500
Iteration 13, loss = 0.34698072
Iteration 14, loss = 0.34465197
Iteration 15, loss = 0.34316102
Iteration 16, loss = 0.34105160
Iteration 17, loss = 0.33979452
Iteration 18, loss = 0.33832988
Iteration 19, loss = 0.33683174
Iteration 20, loss = 0.33552757
Iteration 21, loss = 0.33419556
Iteration 22, loss = 0.33351626
Iteration 23, loss = 0.33249587
Iteration 24, loss = 0.33122236
Iteration 25, loss = 0.33009917
Iteration 26, loss = 0.32999986
Iteration 27, loss = 0.32910819
Iteration 28, loss = 0.32850083
Iteration 29, loss = 0.32783501
Iteration 30, loss = 0.32754520
Iteration 31, loss = 0.32683017
Iteration 32, loss = 0.32665067
Iteration 33, loss = 0.32556088
Iteration 34, loss = 0.32522611
Iteration 35, loss = 0.32438246
Iteration 36, loss = 0.32485486
Iteration 37, loss = 0.32409989
Iteration 38, loss = 0.32337248
Iteration 39, loss = 0.32286572
Iteration 40, loss = 0.32274215
Iteration 41, loss = 0.32239637
Iteration 42, loss = 0.32167948
Iteration 43, loss = 0.32131837
Iteration 44, loss = 0.32184441
Iteration 45, loss = 0.32070700
Iteration 46, loss = 0.32081990
Iteration 47, loss = 0.32046730
Iteration 48, loss = 0.31979466
Iteration 49, loss = 0.31925548
Iteration 50, loss = 0.31909189
Iteration 51, loss = 0.31887561
Iteration 52, loss = 0.31880710
Iteration 53, loss = 0.31871974
Iteration 54, loss = 0.31846778
Iteration 55, loss = 0.31812705
Iteration 56, loss = 0.31795224
Iteration 57, loss = 0.31750794
Iteration 58, loss = 0.31788960
Iteration 59, loss = 0.31685210
Iteration 60, loss = 0.31804258
Iteration 61, loss = 0.31666714
Iteration 62, loss = 0.31632087
Iteration 63, loss = 0.31639504
Iteration 64, loss = 0.31605015
Iteration 65, loss = 0.31572774
Iteration 66, loss = 0.31611639
Iteration 67, loss = 0.31633156
Iteration 68, loss = 0.31572302
Iteration 69, loss = 0.31476342
Iteration 70, loss = 0.31484296
Iteration 71, loss = 0.31520534
Iteration 72, loss = 0.31461359
Iteration 73, loss = 0.31531180
Iteration 74, loss = 0.31403008
Iteration 75, loss = 0.31376627
Iteration 76, loss = 0.31326772
Iteration 77, loss = 0.31401660
Iteration 78, loss = 0.31320113
Iteration 79, loss = 0.31359897
Iteration 80, loss = 0.31333339
Iteration 81, loss = 0.31323587
Iteration 82, loss = 0.31322168
Iteration 83, loss = 0.31260319
Iteration 84, loss = 0.31265837
Iteration 85, loss = 0.31265071
Iteration 86, loss = 0.31293517
Iteration 87, loss = 0.31203533
Iteration 88, loss = 0.31220152
Iteration 89, loss = 0.31218880
Iteration 90, loss = 0.31222759
Iteration 91, loss = 0.31146992
Iteration 92, loss = 0.31184401
Iteration 93, loss = 0.31121152
Iteration 94, loss = 0.31082210
Iteration 95, loss = 0.31123675
Iteration 96, loss = 0.31070033
Iteration 97, loss = 0.31075119
Iteration 98, loss = 0.31078837
Iteration 99, loss = 0.31079993
Iteration 100, loss = 0.31047057
Iteration 101, loss = 0.31073111
Iteration 102, loss = 0.30991741
Iteration 103, loss = 0.31067547
Iteration 104, loss = 0.31047062
Iteration 105, loss = 0.31058906
Iteration 106, loss = 0.30993109
Iteration 107, loss = 0.30982702
Iteration 108, loss = 0.30934541
Iteration 109, loss = 0.30944812
Iteration 110, loss = 0.30887005
Iteration 111, loss = 0.31020394
Iteration 112, loss = 0.30944482
Iteration 113, loss = 0.30956777
Iteration 114, loss = 0.30864696
Iteration 115, loss = 0.30947032
Iteration 116, loss = 0.30859702
Iteration 117, loss = 0.30924140
Iteration 118, loss = 0.30866187
Iteration 119, loss = 0.30800554
Iteration 120, loss = 0.30867743
Iteration 121, loss = 0.30808678
Iteration 122, loss = 0.30835876
Iteration 123, loss = 0.30830531
Iteration 124, loss = 0.30787382
Iteration 125, loss = 0.30781538
Iteration 126, loss = 0.30750663
Iteration 127, loss = 0.30771397
Iteration 128, loss = 0.30773279
Iteration 129, loss = 0.30696893
Iteration 130, loss = 0.30688400
Iteration 131, loss = 0.30762531
Iteration 132, loss = 0.30717479
Iteration 133, loss = 0.30694846
Iteration 134, loss = 0.30666710
Iteration 135, loss = 0.30695493
Iteration 136, loss = 0.30680670
Iteration 137, loss = 0.30668815
Iteration 138, loss = 0.30706488
Iteration 139, loss = 0.30672964
Iteration 140, loss = 0.30618405
Iteration 141, loss = 0.30625645
Iteration 142, loss = 0.30614037
Iteration 143, loss = 0.30560211
Iteration 144, loss = 0.30600577
Iteration 145, loss = 0.30611679
Iteration 146, loss = 0.30621537
Iteration 147, loss = 0.30580759
Iteration 148, loss = 0.30569703
Iteration 149, loss = 0.30567734
Iteration 150, loss = 0.30538542
Iteration 151, loss = 0.30505150
Iteration 152, loss = 0.30542454
Iteration 153, loss = 0.30489207
Iteration 154, loss = 0.30540491
Iteration 155, loss = 0.30566546
Iteration 156, loss = 0.30537494
Iteration 157, loss = 0.30599170
Iteration 158, loss = 0.30427691
Iteration 159, loss = 0.30531555
Iteration 160, loss = 0.30452352
Iteration 161, loss = 0.30475959
Iteration 162, loss = 0.30506549
Iteration 163, loss = 0.30406970
Iteration 164, loss = 0.30498472
Iteration 165, loss = 0.30433337
Iteration 166, loss = 0.30443744
Iteration 167, loss = 0.30435398
Iteration 168, loss = 0.30446830
Iteration 169, loss = 0.30363719
Iteration 170, loss = 0.30375858
Iteration 171, loss = 0.30349379
Iteration 172, loss = 0.30378959
Iteration 173, loss = 0.30397689
Iteration 174, loss = 0.30295122
Iteration 175, loss = 0.30368891
Iteration 176, loss = 0.30374639
Iteration 177, loss = 0.30359833
Iteration 178, loss = 0.30341768
Iteration 179, loss = 0.30386364
Iteration 180, loss = 0.30348709
Iteration 181, loss = 0.30336361
Iteration 182, loss = 0.30332485
Iteration 183, loss = 0.30297821
Iteration 184, loss = 0.30329255
Iteration 185, loss = 0.30311299
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_30d_gmm_augment-iter-1.pkl
"ds4 (pca_30d_gmm_augment)",0.870,0.829,0.815,0.803,0.829,0.849,0.797,0.784,0.773,0.801,127.66212558746338
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-55.pkl
Iteration 1, loss = 0.43516291
Iteration 2, loss = 0.37567558
Iteration 3, loss = 0.36333909
Iteration 4, loss = 0.35460009
Iteration 5, loss = 0.34705918
Iteration 6, loss = 0.34146296
Iteration 7, loss = 0.33672903
Iteration 8, loss = 0.33234691
Iteration 9, loss = 0.32916200
Iteration 10, loss = 0.32661552
Iteration 11, loss = 0.32405521
Iteration 12, loss = 0.32169498
Iteration 13, loss = 0.31960505
Iteration 14, loss = 0.31744935
Iteration 15, loss = 0.31629932
Iteration 16, loss = 0.31468866
Iteration 17, loss = 0.31427008
Iteration 18, loss = 0.31254802
Iteration 19, loss = 0.31101917
Iteration 20, loss = 0.31063655
Iteration 21, loss = 0.30888110
Iteration 22, loss = 0.30852425
Iteration 23, loss = 0.30740311
Iteration 24, loss = 0.30674678
Iteration 25, loss = 0.30583505
Iteration 26, loss = 0.30524563
Iteration 27, loss = 0.30452594
Iteration 28, loss = 0.30369637
Iteration 29, loss = 0.30302239
Iteration 30, loss = 0.30247798
Iteration 31, loss = 0.30186127
Iteration 32, loss = 0.30129328
Iteration 33, loss = 0.30084321
Iteration 34, loss = 0.30050560
Iteration 35, loss = 0.29935912
Iteration 36, loss = 0.29927478
Iteration 37, loss = 0.29846112
Iteration 38, loss = 0.29740092
Iteration 39, loss = 0.29791807
Iteration 40, loss = 0.29645468
Iteration 41, loss = 0.29677441
Iteration 42, loss = 0.29642593
Iteration 43, loss = 0.29563288
Iteration 44, loss = 0.29464292
Iteration 45, loss = 0.29524436
Iteration 46, loss = 0.29390488
Iteration 47, loss = 0.29435716
Iteration 48, loss = 0.29329767
Iteration 49, loss = 0.29284505
Iteration 50, loss = 0.29243589
Iteration 51, loss = 0.29209137
Iteration 52, loss = 0.29212697
Iteration 53, loss = 0.29193178
Iteration 54, loss = 0.29051318
Iteration 55, loss = 0.29056978
Iteration 56, loss = 0.29026803
Iteration 57, loss = 0.29070357
Iteration 58, loss = 0.28942799
Iteration 59, loss = 0.28892794
Iteration 60, loss = 0.28947865
Iteration 61, loss = 0.28844125
Iteration 62, loss = 0.28824421
Iteration 63, loss = 0.28811920
Iteration 64, loss = 0.28756966
Iteration 65, loss = 0.28732647
Iteration 66, loss = 0.28682466
Iteration 67, loss = 0.28724659
Iteration 68, loss = 0.28619035
Iteration 69, loss = 0.28601372
Iteration 70, loss = 0.28632128
Iteration 71, loss = 0.28575528
Iteration 72, loss = 0.28491686
Iteration 73, loss = 0.28507344
Iteration 74, loss = 0.28460305
Iteration 75, loss = 0.28365177
Iteration 76, loss = 0.28392169
Iteration 77, loss = 0.28350584
Iteration 78, loss = 0.28282434
Iteration 79, loss = 0.28307399
Iteration 80, loss = 0.28267298
Iteration 81, loss = 0.28268103
Iteration 82, loss = 0.28305509
Iteration 83, loss = 0.28186138
Iteration 84, loss = 0.28215151
Iteration 85, loss = 0.28151046
Iteration 86, loss = 0.28111844
Iteration 87, loss = 0.28075747
Iteration 88, loss = 0.28047538
Iteration 89, loss = 0.27978691
Iteration 90, loss = 0.28007953
Iteration 91, loss = 0.27975335
Iteration 92, loss = 0.27992303
Iteration 93, loss = 0.27981644
Iteration 94, loss = 0.27912512
Iteration 95, loss = 0.27877307
Iteration 96, loss = 0.27912546
Iteration 97, loss = 0.27834315
Iteration 98, loss = 0.27820988
Iteration 99, loss = 0.27912570
Iteration 100, loss = 0.27763392
Iteration 101, loss = 0.27794041
Iteration 102, loss = 0.27841508
Iteration 103, loss = 0.27757974
Iteration 104, loss = 0.27721038
Iteration 105, loss = 0.27734230
Iteration 106, loss = 0.27684123
Iteration 107, loss = 0.27663675
Iteration 108, loss = 0.27696837
Iteration 109, loss = 0.27619614
Iteration 110, loss = 0.27575949
Iteration 111, loss = 0.27631367
Iteration 112, loss = 0.27563923
Iteration 113, loss = 0.27569217
Iteration 114, loss = 0.27541570
Iteration 115, loss = 0.27531669
Iteration 116, loss = 0.27523640
Iteration 117, loss = 0.27493779
Iteration 118, loss = 0.27469139
Iteration 119, loss = 0.27438217
Iteration 120, loss = 0.27429638
Iteration 121, loss = 0.27396911
Iteration 122, loss = 0.27425347
Iteration 123, loss = 0.27398699
Iteration 124, loss = 0.27366515
Iteration 125, loss = 0.27336622
Iteration 126, loss = 0.27368636
Iteration 127, loss = 0.27337884
Iteration 128, loss = 0.27277511
Iteration 129, loss = 0.27276664
Iteration 130, loss = 0.27295983
Iteration 131, loss = 0.27163643
Iteration 132, loss = 0.27272240
Iteration 133, loss = 0.27248710
Iteration 134, loss = 0.27232405
Iteration 135, loss = 0.27236327
Iteration 136, loss = 0.27127834
Iteration 137, loss = 0.27220724
Iteration 138, loss = 0.27147114
Iteration 139, loss = 0.27069770
Iteration 140, loss = 0.27082684
Iteration 141, loss = 0.27145036
Iteration 142, loss = 0.27053826
Iteration 143, loss = 0.27114431
Iteration 144, loss = 0.27143325
Iteration 145, loss = 0.27084433
Iteration 146, loss = 0.27047702
Iteration 147, loss = 0.27005794
Iteration 148, loss = 0.26943454
Iteration 149, loss = 0.26978362
Iteration 150, loss = 0.26964057
Iteration 151, loss = 0.26944901
Iteration 152, loss = 0.27024695
Iteration 153, loss = 0.26995406
Iteration 154, loss = 0.26907205
Iteration 155, loss = 0.26915689
Iteration 156, loss = 0.26876551
Iteration 157, loss = 0.26987019
Iteration 158, loss = 0.26956809
Iteration 159, loss = 0.26895985
Iteration 160, loss = 0.26901764
Iteration 161, loss = 0.26865459
Iteration 162, loss = 0.26843599
Iteration 163, loss = 0.26799306
Iteration 164, loss = 0.26779579
Iteration 165, loss = 0.26803014
Iteration 166, loss = 0.26771023
Iteration 167, loss = 0.26843113
Iteration 168, loss = 0.26763602
Iteration 169, loss = 0.26810739
Iteration 170, loss = 0.26769997
Iteration 171, loss = 0.26771644
Iteration 172, loss = 0.26717248
Iteration 173, loss = 0.26751899
Iteration 174, loss = 0.26684218
Iteration 175, loss = 0.26751357
Iteration 176, loss = 0.26709618
Iteration 177, loss = 0.26666540
Iteration 178, loss = 0.26674574
Iteration 179, loss = 0.26596221
Iteration 180, loss = 0.26624437
Iteration 181, loss = 0.26640453
Iteration 182, loss = 0.26654267
Iteration 183, loss = 0.26626256
Iteration 184, loss = 0.26607509
Iteration 185, loss = 0.26587888
Iteration 186, loss = 0.26570316
Iteration 187, loss = 0.26484045
Iteration 188, loss = 0.26570583
Iteration 189, loss = 0.26553144
Iteration 190, loss = 0.26511596
Iteration 191, loss = 0.26496945
Iteration 192, loss = 0.26551457
Iteration 193, loss = 0.26550408
Iteration 194, loss = 0.26478766
Iteration 195, loss = 0.26556465
Iteration 196, loss = 0.26475331
Iteration 197, loss = 0.26455321
Iteration 198, loss = 0.26503598
Iteration 199, loss = 0.26457703
Iteration 200, loss = 0.26357610
Iteration 201, loss = 0.26446578
Iteration 202, loss = 0.26372968
Iteration 203, loss = 0.26474691
Iteration 204, loss = 0.26436070
Iteration 205, loss = 0.26447836
Iteration 206, loss = 0.26441950
Iteration 207, loss = 0.26333675
Iteration 208, loss = 0.26348796
Iteration 209, loss = 0.26390018
Iteration 210, loss = 0.26327354
Iteration 211, loss = 0.26353140
Iteration 212, loss = 0.26327746
Iteration 213, loss = 0.26393203
Iteration 214, loss = 0.26263390
Iteration 215, loss = 0.26367879
Iteration 216, loss = 0.26337890
Iteration 217, loss = 0.26362801
Iteration 218, loss = 0.26262788
Iteration 219, loss = 0.26292073
Iteration 220, loss = 0.26249806
Iteration 221, loss = 0.26331348
Iteration 222, loss = 0.26262575
Iteration 223, loss = 0.26336888
Iteration 224, loss = 0.26249197
Iteration 225, loss = 0.26286180
Iteration 226, loss = 0.26284656
Iteration 227, loss = 0.26272043
Iteration 228, loss = 0.26288397
Iteration 229, loss = 0.26219356
Iteration 230, loss = 0.26182525
Iteration 231, loss = 0.26220154
Iteration 232, loss = 0.26273902
Iteration 233, loss = 0.26184064
Iteration 234, loss = 0.26263671
Iteration 235, loss = 0.26242859
Iteration 236, loss = 0.26140673
Iteration 237, loss = 0.26197893
Iteration 238, loss = 0.26189518
Iteration 239, loss = 0.26121585
Iteration 240, loss = 0.26218191
Iteration 241, loss = 0.26145895
Iteration 242, loss = 0.26162041
Iteration 243, loss = 0.26137238
Iteration 244, loss = 0.26111215
Iteration 245, loss = 0.26104768
Iteration 246, loss = 0.26146147
Iteration 247, loss = 0.26237442
Iteration 248, loss = 0.26152494
Iteration 249, loss = 0.26095757
Iteration 250, loss = 0.26149137
Iteration 251, loss = 0.26131173
Iteration 252, loss = 0.26048882
Iteration 253, loss = 0.26135668
Iteration 254, loss = 0.26051963
Iteration 255, loss = 0.26083665
Iteration 256, loss = 0.26152797
Iteration 257, loss = 0.26070808
Iteration 258, loss = 0.26068244
Iteration 259, loss = 0.26118945
Iteration 260, loss = 0.26044383
Iteration 261, loss = 0.26060164
Iteration 262, loss = 0.26035046
Iteration 263, loss = 0.26054152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_55d_kmeans_augment-iter-1.pkl
"ds4 (pca_55d_kmeans_augment)",0.892,0.849,0.853,0.857,0.863,0.838,0.778,0.778,0.779,0.793,182.14307284355164
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-55.pkl
Iteration 1, loss = 0.43516291
Iteration 2, loss = 0.37567558
Iteration 3, loss = 0.36333909
Iteration 4, loss = 0.35460009
Iteration 5, loss = 0.34705918
Iteration 6, loss = 0.34146296
Iteration 7, loss = 0.33672903
Iteration 8, loss = 0.33234691
Iteration 9, loss = 0.32916200
Iteration 10, loss = 0.32661552
Iteration 11, loss = 0.32405521
Iteration 12, loss = 0.32169498
Iteration 13, loss = 0.31960505
Iteration 14, loss = 0.31744935
Iteration 15, loss = 0.31629932
Iteration 16, loss = 0.31468866
Iteration 17, loss = 0.31427008
Iteration 18, loss = 0.31254802
Iteration 19, loss = 0.31101917
Iteration 20, loss = 0.31063655
Iteration 21, loss = 0.30888110
Iteration 22, loss = 0.30852425
Iteration 23, loss = 0.30740311
Iteration 24, loss = 0.30674678
Iteration 25, loss = 0.30583505
Iteration 26, loss = 0.30524563
Iteration 27, loss = 0.30452594
Iteration 28, loss = 0.30369637
Iteration 29, loss = 0.30302239
Iteration 30, loss = 0.30247798
Iteration 31, loss = 0.30186127
Iteration 32, loss = 0.30129328
Iteration 33, loss = 0.30084321
Iteration 34, loss = 0.30050560
Iteration 35, loss = 0.29935912
Iteration 36, loss = 0.29927478
Iteration 37, loss = 0.29846112
Iteration 38, loss = 0.29740092
Iteration 39, loss = 0.29791807
Iteration 40, loss = 0.29645468
Iteration 41, loss = 0.29677441
Iteration 42, loss = 0.29642593
Iteration 43, loss = 0.29563288
Iteration 44, loss = 0.29464292
Iteration 45, loss = 0.29524436
Iteration 46, loss = 0.29390488
Iteration 47, loss = 0.29435716
Iteration 48, loss = 0.29329767
Iteration 49, loss = 0.29284505
Iteration 50, loss = 0.29243589
Iteration 51, loss = 0.29209137
Iteration 52, loss = 0.29212697
Iteration 53, loss = 0.29193178
Iteration 54, loss = 0.29051318
Iteration 55, loss = 0.29056978
Iteration 56, loss = 0.29026803
Iteration 57, loss = 0.29070357
Iteration 58, loss = 0.28942799
Iteration 59, loss = 0.28892794
Iteration 60, loss = 0.28947865
Iteration 61, loss = 0.28844125
Iteration 62, loss = 0.28824421
Iteration 63, loss = 0.28811920
Iteration 64, loss = 0.28756966
Iteration 65, loss = 0.28732647
Iteration 66, loss = 0.28682466
Iteration 67, loss = 0.28724659
Iteration 68, loss = 0.28619035
Iteration 69, loss = 0.28601372
Iteration 70, loss = 0.28632128
Iteration 71, loss = 0.28575528
Iteration 72, loss = 0.28491686
Iteration 73, loss = 0.28507344
Iteration 74, loss = 0.28460305
Iteration 75, loss = 0.28365177
Iteration 76, loss = 0.28392169
Iteration 77, loss = 0.28350584
Iteration 78, loss = 0.28282434
Iteration 79, loss = 0.28307399
Iteration 80, loss = 0.28267298
Iteration 81, loss = 0.28268103
Iteration 82, loss = 0.28305509
Iteration 83, loss = 0.28186138
Iteration 84, loss = 0.28215151
Iteration 85, loss = 0.28151046
Iteration 86, loss = 0.28111844
Iteration 87, loss = 0.28075747
Iteration 88, loss = 0.28047538
Iteration 89, loss = 0.27978691
Iteration 90, loss = 0.28007953
Iteration 91, loss = 0.27975335
Iteration 92, loss = 0.27992303
Iteration 93, loss = 0.27981644
Iteration 94, loss = 0.27912512
Iteration 95, loss = 0.27877307
Iteration 96, loss = 0.27912546
Iteration 97, loss = 0.27834315
Iteration 98, loss = 0.27820988
Iteration 99, loss = 0.27912570
Iteration 100, loss = 0.27763392
Iteration 101, loss = 0.27794041
Iteration 102, loss = 0.27841508
Iteration 103, loss = 0.27757974
Iteration 104, loss = 0.27721038
Iteration 105, loss = 0.27734230
Iteration 106, loss = 0.27684123
Iteration 107, loss = 0.27663675
Iteration 108, loss = 0.27696837
Iteration 109, loss = 0.27619614
Iteration 110, loss = 0.27575949
Iteration 111, loss = 0.27631367
Iteration 112, loss = 0.27563923
Iteration 113, loss = 0.27569217
Iteration 114, loss = 0.27541570
Iteration 115, loss = 0.27531669
Iteration 116, loss = 0.27523640
Iteration 117, loss = 0.27493779
Iteration 118, loss = 0.27469139
Iteration 119, loss = 0.27438217
Iteration 120, loss = 0.27429638
Iteration 121, loss = 0.27396911
Iteration 122, loss = 0.27425347
Iteration 123, loss = 0.27398699
Iteration 124, loss = 0.27366515
Iteration 125, loss = 0.27336622
Iteration 126, loss = 0.27368636
Iteration 127, loss = 0.27337884
Iteration 128, loss = 0.27277511
Iteration 129, loss = 0.27276664
Iteration 130, loss = 0.27295983
Iteration 131, loss = 0.27163643
Iteration 132, loss = 0.27272240
Iteration 133, loss = 0.27248710
Iteration 134, loss = 0.27232405
Iteration 135, loss = 0.27236327
Iteration 136, loss = 0.27127834
Iteration 137, loss = 0.27220724
Iteration 138, loss = 0.27147114
Iteration 139, loss = 0.27069770
Iteration 140, loss = 0.27082684
Iteration 141, loss = 0.27145036
Iteration 142, loss = 0.27053826
Iteration 143, loss = 0.27114431
Iteration 144, loss = 0.27143325
Iteration 145, loss = 0.27084433
Iteration 146, loss = 0.27047702
Iteration 147, loss = 0.27005794
Iteration 148, loss = 0.26943454
Iteration 149, loss = 0.26978362
Iteration 150, loss = 0.26964057
Iteration 151, loss = 0.26944901
Iteration 152, loss = 0.27024695
Iteration 153, loss = 0.26995406
Iteration 154, loss = 0.26907205
Iteration 155, loss = 0.26915689
Iteration 156, loss = 0.26876551
Iteration 157, loss = 0.26987019
Iteration 158, loss = 0.26956809
Iteration 159, loss = 0.26895985
Iteration 160, loss = 0.26901764
Iteration 161, loss = 0.26865459
Iteration 162, loss = 0.26843599
Iteration 163, loss = 0.26799306
Iteration 164, loss = 0.26779579
Iteration 165, loss = 0.26803014
Iteration 166, loss = 0.26771023
Iteration 167, loss = 0.26843113
Iteration 168, loss = 0.26763602
Iteration 169, loss = 0.26810739
Iteration 170, loss = 0.26769997
Iteration 171, loss = 0.26771644
Iteration 172, loss = 0.26717248
Iteration 173, loss = 0.26751899
Iteration 174, loss = 0.26684218
Iteration 175, loss = 0.26751357
Iteration 176, loss = 0.26709618
Iteration 177, loss = 0.26666540
Iteration 178, loss = 0.26674574
Iteration 179, loss = 0.26596221
Iteration 180, loss = 0.26624437
Iteration 181, loss = 0.26640453
Iteration 182, loss = 0.26654267
Iteration 183, loss = 0.26626256
Iteration 184, loss = 0.26607509
Iteration 185, loss = 0.26587888
Iteration 186, loss = 0.26570316
Iteration 187, loss = 0.26484045
Iteration 188, loss = 0.26570583
Iteration 189, loss = 0.26553144
Iteration 190, loss = 0.26511596
Iteration 191, loss = 0.26496945
Iteration 192, loss = 0.26551457
Iteration 193, loss = 0.26550408
Iteration 194, loss = 0.26478766
Iteration 195, loss = 0.26556465
Iteration 196, loss = 0.26475331
Iteration 197, loss = 0.26455321
Iteration 198, loss = 0.26503598
Iteration 199, loss = 0.26457703
Iteration 200, loss = 0.26357610
Iteration 201, loss = 0.26446578
Iteration 202, loss = 0.26372968
Iteration 203, loss = 0.26474691
Iteration 204, loss = 0.26436070
Iteration 205, loss = 0.26447836
Iteration 206, loss = 0.26441950
Iteration 207, loss = 0.26333675
Iteration 208, loss = 0.26348796
Iteration 209, loss = 0.26390018
Iteration 210, loss = 0.26327354
Iteration 211, loss = 0.26353140
Iteration 212, loss = 0.26327746
Iteration 213, loss = 0.26393203
Iteration 214, loss = 0.26263390
Iteration 215, loss = 0.26367879
Iteration 216, loss = 0.26337890
Iteration 217, loss = 0.26362801
Iteration 218, loss = 0.26262788
Iteration 219, loss = 0.26292073
Iteration 220, loss = 0.26249806
Iteration 221, loss = 0.26331348
Iteration 222, loss = 0.26262575
Iteration 223, loss = 0.26336888
Iteration 224, loss = 0.26249197
Iteration 225, loss = 0.26286180
Iteration 226, loss = 0.26284656
Iteration 227, loss = 0.26272043
Iteration 228, loss = 0.26288397
Iteration 229, loss = 0.26219356
Iteration 230, loss = 0.26182525
Iteration 231, loss = 0.26220154
Iteration 232, loss = 0.26273902
Iteration 233, loss = 0.26184064
Iteration 234, loss = 0.26263671
Iteration 235, loss = 0.26242859
Iteration 236, loss = 0.26140673
Iteration 237, loss = 0.26197893
Iteration 238, loss = 0.26189518
Iteration 239, loss = 0.26121585
Iteration 240, loss = 0.26218191
Iteration 241, loss = 0.26145895
Iteration 242, loss = 0.26162041
Iteration 243, loss = 0.26137238
Iteration 244, loss = 0.26111215
Iteration 245, loss = 0.26104768
Iteration 246, loss = 0.26146147
Iteration 247, loss = 0.26237442
Iteration 248, loss = 0.26152494
Iteration 249, loss = 0.26095757
Iteration 250, loss = 0.26149137
Iteration 251, loss = 0.26131173
Iteration 252, loss = 0.26048882
Iteration 253, loss = 0.26135668
Iteration 254, loss = 0.26051963
Iteration 255, loss = 0.26083665
Iteration 256, loss = 0.26152797
Iteration 257, loss = 0.26070808
Iteration 258, loss = 0.26068244
Iteration 259, loss = 0.26118945
Iteration 260, loss = 0.26044383
Iteration 261, loss = 0.26060164
Iteration 262, loss = 0.26035046
Iteration 263, loss = 0.26054152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_55d_gmm_augment-iter-1.pkl
"ds4 (pca_55d_gmm_augment)",0.892,0.849,0.853,0.857,0.863,0.838,0.778,0.778,0.779,0.793,189.85667085647583
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-30.pkl
Iteration 1, loss = 0.52994808
Iteration 2, loss = 0.43682526
Iteration 3, loss = 0.40968581
Iteration 4, loss = 0.39233117
Iteration 5, loss = 0.37953415
Iteration 6, loss = 0.37168819
Iteration 7, loss = 0.36482175
Iteration 8, loss = 0.35972097
Iteration 9, loss = 0.35494407
Iteration 10, loss = 0.35132040
Iteration 11, loss = 0.34829126
Iteration 12, loss = 0.34527125
Iteration 13, loss = 0.34323908
Iteration 14, loss = 0.34101084
Iteration 15, loss = 0.33962559
Iteration 16, loss = 0.33757819
Iteration 17, loss = 0.33679387
Iteration 18, loss = 0.33460060
Iteration 19, loss = 0.33338014
Iteration 20, loss = 0.33184934
Iteration 21, loss = 0.33057752
Iteration 22, loss = 0.32958391
Iteration 23, loss = 0.32808929
Iteration 24, loss = 0.32705081
Iteration 25, loss = 0.32569476
Iteration 26, loss = 0.32532283
Iteration 27, loss = 0.32461618
Iteration 28, loss = 0.32393561
Iteration 29, loss = 0.32350971
Iteration 30, loss = 0.32240885
Iteration 31, loss = 0.32185006
Iteration 32, loss = 0.32139411
Iteration 33, loss = 0.32039903
Iteration 34, loss = 0.32053457
Iteration 35, loss = 0.31895034
Iteration 36, loss = 0.31940421
Iteration 37, loss = 0.31901577
Iteration 38, loss = 0.31749582
Iteration 39, loss = 0.31719517
Iteration 40, loss = 0.31724244
Iteration 41, loss = 0.31643249
Iteration 42, loss = 0.31614454
Iteration 43, loss = 0.31534858
Iteration 44, loss = 0.31543906
Iteration 45, loss = 0.31475916
Iteration 46, loss = 0.31459694
Iteration 47, loss = 0.31407256
Iteration 48, loss = 0.31390490
Iteration 49, loss = 0.31361696
Iteration 50, loss = 0.31249229
Iteration 51, loss = 0.31310235
Iteration 52, loss = 0.31245696
Iteration 53, loss = 0.31189945
Iteration 54, loss = 0.31184073
Iteration 55, loss = 0.31161186
Iteration 56, loss = 0.31095037
Iteration 57, loss = 0.31079612
Iteration 58, loss = 0.31138354
Iteration 59, loss = 0.31078314
Iteration 60, loss = 0.31122461
Iteration 61, loss = 0.31025140
Iteration 62, loss = 0.30931959
Iteration 63, loss = 0.30979952
Iteration 64, loss = 0.30934670
Iteration 65, loss = 0.30872739
Iteration 66, loss = 0.30932254
Iteration 67, loss = 0.30907590
Iteration 68, loss = 0.30896623
Iteration 69, loss = 0.30765274
Iteration 70, loss = 0.30810780
Iteration 71, loss = 0.30812477
Iteration 72, loss = 0.30822518
Iteration 73, loss = 0.30808639
Iteration 74, loss = 0.30712063
Iteration 75, loss = 0.30742713
Iteration 76, loss = 0.30644533
Iteration 77, loss = 0.30695466
Iteration 78, loss = 0.30703370
Iteration 79, loss = 0.30667928
Iteration 80, loss = 0.30626934
Iteration 81, loss = 0.30613700
Iteration 82, loss = 0.30619137
Iteration 83, loss = 0.30645911
Iteration 84, loss = 0.30546172
Iteration 85, loss = 0.30520692
Iteration 86, loss = 0.30600517
Iteration 87, loss = 0.30490850
Iteration 88, loss = 0.30526088
Iteration 89, loss = 0.30522451
Iteration 90, loss = 0.30525657
Iteration 91, loss = 0.30480227
Iteration 92, loss = 0.30512221
Iteration 93, loss = 0.30476650
Iteration 94, loss = 0.30459454
Iteration 95, loss = 0.30449290
Iteration 96, loss = 0.30392967
Iteration 97, loss = 0.30431041
Iteration 98, loss = 0.30422241
Iteration 99, loss = 0.30381752
Iteration 100, loss = 0.30357287
Iteration 101, loss = 0.30348610
Iteration 102, loss = 0.30320359
Iteration 103, loss = 0.30407304
Iteration 104, loss = 0.30328680
Iteration 105, loss = 0.30394396
Iteration 106, loss = 0.30283043
Iteration 107, loss = 0.30330143
Iteration 108, loss = 0.30259996
Iteration 109, loss = 0.30275295
Iteration 110, loss = 0.30200982
Iteration 111, loss = 0.30338191
Iteration 112, loss = 0.30284465
Iteration 113, loss = 0.30229611
Iteration 114, loss = 0.30201131
Iteration 115, loss = 0.30271527
Iteration 116, loss = 0.30207947
Iteration 117, loss = 0.30212986
Iteration 118, loss = 0.30132029
Iteration 119, loss = 0.30137297
Iteration 120, loss = 0.30210984
Iteration 121, loss = 0.30153316
Iteration 122, loss = 0.30095715
Iteration 123, loss = 0.30157010
Iteration 124, loss = 0.30108693
Iteration 125, loss = 0.30100745
Iteration 126, loss = 0.30112081
Iteration 127, loss = 0.30081437
Iteration 128, loss = 0.30127956
Iteration 129, loss = 0.30049561
Iteration 130, loss = 0.30031512
Iteration 131, loss = 0.30094159
Iteration 132, loss = 0.30023409
Iteration 133, loss = 0.30024281
Iteration 134, loss = 0.29962763
Iteration 135, loss = 0.29999334
Iteration 136, loss = 0.29978594
Iteration 137, loss = 0.30002045
Iteration 138, loss = 0.30033471
Iteration 139, loss = 0.30044308
Iteration 140, loss = 0.29897369
Iteration 141, loss = 0.29899624
Iteration 142, loss = 0.29913127
Iteration 143, loss = 0.29896641
Iteration 144, loss = 0.29940269
Iteration 145, loss = 0.29957118
Iteration 146, loss = 0.30000118
Iteration 147, loss = 0.29877051
Iteration 148, loss = 0.29902325
Iteration 149, loss = 0.29961861
Iteration 150, loss = 0.29915434
Iteration 151, loss = 0.29847493
Iteration 152, loss = 0.29855528
Iteration 153, loss = 0.29795061
Iteration 154, loss = 0.29880697
Iteration 155, loss = 0.29821563
Iteration 156, loss = 0.29823013
Iteration 157, loss = 0.29901214
Iteration 158, loss = 0.29759087
Iteration 159, loss = 0.29841119
Iteration 160, loss = 0.29823044
Iteration 161, loss = 0.29759421
Iteration 162, loss = 0.29765281
Iteration 163, loss = 0.29765605
Iteration 164, loss = 0.29832467
Iteration 165, loss = 0.29738017
Iteration 166, loss = 0.29723760
Iteration 167, loss = 0.29693027
Iteration 168, loss = 0.29793431
Iteration 169, loss = 0.29721810
Iteration 170, loss = 0.29713689
Iteration 171, loss = 0.29653526
Iteration 172, loss = 0.29707424
Iteration 173, loss = 0.29684186
Iteration 174, loss = 0.29696099
Iteration 175, loss = 0.29740984
Iteration 176, loss = 0.29720639
Iteration 177, loss = 0.29669526
Iteration 178, loss = 0.29693839
Iteration 179, loss = 0.29670169
Iteration 180, loss = 0.29634517
Iteration 181, loss = 0.29683943
Iteration 182, loss = 0.29585374
Iteration 183, loss = 0.29696452
Iteration 184, loss = 0.29654679
Iteration 185, loss = 0.29603505
Iteration 186, loss = 0.29586879
Iteration 187, loss = 0.29585934
Iteration 188, loss = 0.29582211
Iteration 189, loss = 0.29578747
Iteration 190, loss = 0.29587003
Iteration 191, loss = 0.29556520
Iteration 192, loss = 0.29598449
Iteration 193, loss = 0.29585242
Iteration 194, loss = 0.29646007
Iteration 195, loss = 0.29538497
Iteration 196, loss = 0.29614131
Iteration 197, loss = 0.29585665
Iteration 198, loss = 0.29511871
Iteration 199, loss = 0.29484373
Iteration 200, loss = 0.29573276
Iteration 201, loss = 0.29475857
Iteration 202, loss = 0.29471171
Iteration 203, loss = 0.29556983
Iteration 204, loss = 0.29526075
Iteration 205, loss = 0.29531250
Iteration 206, loss = 0.29492341
Iteration 207, loss = 0.29558251
Iteration 208, loss = 0.29401410
Iteration 209, loss = 0.29540165
Iteration 210, loss = 0.29415942
Iteration 211, loss = 0.29476946
Iteration 212, loss = 0.29457302
Iteration 213, loss = 0.29468434
Iteration 214, loss = 0.29426324
Iteration 215, loss = 0.29480961
Iteration 216, loss = 0.29523870
Iteration 217, loss = 0.29405130
Iteration 218, loss = 0.29445452
Iteration 219, loss = 0.29404021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_30d_kmeans_augment-iter-1.pkl
"ds4 (rp_30d_kmeans_augment)",0.876,0.837,0.822,0.811,0.837,0.848,0.798,0.783,0.771,0.800,142.1603901386261
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-30.pkl
Iteration 1, loss = 0.52994808
Iteration 2, loss = 0.43682526
Iteration 3, loss = 0.40968581
Iteration 4, loss = 0.39233117
Iteration 5, loss = 0.37953415
Iteration 6, loss = 0.37168819
Iteration 7, loss = 0.36482175
Iteration 8, loss = 0.35972097
Iteration 9, loss = 0.35494407
Iteration 10, loss = 0.35132040
Iteration 11, loss = 0.34829126
Iteration 12, loss = 0.34527125
Iteration 13, loss = 0.34323908
Iteration 14, loss = 0.34101084
Iteration 15, loss = 0.33962559
Iteration 16, loss = 0.33757819
Iteration 17, loss = 0.33679387
Iteration 18, loss = 0.33460060
Iteration 19, loss = 0.33338014
Iteration 20, loss = 0.33184934
Iteration 21, loss = 0.33057752
Iteration 22, loss = 0.32958391
Iteration 23, loss = 0.32808929
Iteration 24, loss = 0.32705081
Iteration 25, loss = 0.32569476
Iteration 26, loss = 0.32532283
Iteration 27, loss = 0.32461618
Iteration 28, loss = 0.32393561
Iteration 29, loss = 0.32350971
Iteration 30, loss = 0.32240885
Iteration 31, loss = 0.32185006
Iteration 32, loss = 0.32139411
Iteration 33, loss = 0.32039903
Iteration 34, loss = 0.32053457
Iteration 35, loss = 0.31895034
Iteration 36, loss = 0.31940421
Iteration 37, loss = 0.31901577
Iteration 38, loss = 0.31749582
Iteration 39, loss = 0.31719517
Iteration 40, loss = 0.31724244
Iteration 41, loss = 0.31643249
Iteration 42, loss = 0.31614454
Iteration 43, loss = 0.31534858
Iteration 44, loss = 0.31543906
Iteration 45, loss = 0.31475916
Iteration 46, loss = 0.31459694
Iteration 47, loss = 0.31407256
Iteration 48, loss = 0.31390490
Iteration 49, loss = 0.31361696
Iteration 50, loss = 0.31249229
Iteration 51, loss = 0.31310235
Iteration 52, loss = 0.31245696
Iteration 53, loss = 0.31189945
Iteration 54, loss = 0.31184073
Iteration 55, loss = 0.31161186
Iteration 56, loss = 0.31095037
Iteration 57, loss = 0.31079612
Iteration 58, loss = 0.31138354
Iteration 59, loss = 0.31078314
Iteration 60, loss = 0.31122461
Iteration 61, loss = 0.31025140
Iteration 62, loss = 0.30931959
Iteration 63, loss = 0.30979952
Iteration 64, loss = 0.30934670
Iteration 65, loss = 0.30872739
Iteration 66, loss = 0.30932254
Iteration 67, loss = 0.30907590
Iteration 68, loss = 0.30896623
Iteration 69, loss = 0.30765274
Iteration 70, loss = 0.30810780
Iteration 71, loss = 0.30812477
Iteration 72, loss = 0.30822518
Iteration 73, loss = 0.30808639
Iteration 74, loss = 0.30712063
Iteration 75, loss = 0.30742713
Iteration 76, loss = 0.30644533
Iteration 77, loss = 0.30695466
Iteration 78, loss = 0.30703370
Iteration 79, loss = 0.30667928
Iteration 80, loss = 0.30626934
Iteration 81, loss = 0.30613700
Iteration 82, loss = 0.30619137
Iteration 83, loss = 0.30645911
Iteration 84, loss = 0.30546172
Iteration 85, loss = 0.30520692
Iteration 86, loss = 0.30600517
Iteration 87, loss = 0.30490850
Iteration 88, loss = 0.30526088
Iteration 89, loss = 0.30522451
Iteration 90, loss = 0.30525657
Iteration 91, loss = 0.30480227
Iteration 92, loss = 0.30512221
Iteration 93, loss = 0.30476650
Iteration 94, loss = 0.30459454
Iteration 95, loss = 0.30449290
Iteration 96, loss = 0.30392967
Iteration 97, loss = 0.30431041
Iteration 98, loss = 0.30422241
Iteration 99, loss = 0.30381752
Iteration 100, loss = 0.30357287
Iteration 101, loss = 0.30348610
Iteration 102, loss = 0.30320359
Iteration 103, loss = 0.30407304
Iteration 104, loss = 0.30328680
Iteration 105, loss = 0.30394396
Iteration 106, loss = 0.30283043
Iteration 107, loss = 0.30330143
Iteration 108, loss = 0.30259996
Iteration 109, loss = 0.30275295
Iteration 110, loss = 0.30200982
Iteration 111, loss = 0.30338191
Iteration 112, loss = 0.30284465
Iteration 113, loss = 0.30229611
Iteration 114, loss = 0.30201131
Iteration 115, loss = 0.30271527
Iteration 116, loss = 0.30207947
Iteration 117, loss = 0.30212986
Iteration 118, loss = 0.30132029
Iteration 119, loss = 0.30137297
Iteration 120, loss = 0.30210984
Iteration 121, loss = 0.30153316
Iteration 122, loss = 0.30095715
Iteration 123, loss = 0.30157010
Iteration 124, loss = 0.30108693
Iteration 125, loss = 0.30100745
Iteration 126, loss = 0.30112081
Iteration 127, loss = 0.30081437
Iteration 128, loss = 0.30127956
Iteration 129, loss = 0.30049561
Iteration 130, loss = 0.30031512
Iteration 131, loss = 0.30094159
Iteration 132, loss = 0.30023409
Iteration 133, loss = 0.30024281
Iteration 134, loss = 0.29962763
Iteration 135, loss = 0.29999334
Iteration 136, loss = 0.29978594
Iteration 137, loss = 0.30002045
Iteration 138, loss = 0.30033471
Iteration 139, loss = 0.30044308
Iteration 140, loss = 0.29897369
Iteration 141, loss = 0.29899624
Iteration 142, loss = 0.29913127
Iteration 143, loss = 0.29896641
Iteration 144, loss = 0.29940269
Iteration 145, loss = 0.29957118
Iteration 146, loss = 0.30000118
Iteration 147, loss = 0.29877051
Iteration 148, loss = 0.29902325
Iteration 149, loss = 0.29961861
Iteration 150, loss = 0.29915434
Iteration 151, loss = 0.29847493
Iteration 152, loss = 0.29855528
Iteration 153, loss = 0.29795061
Iteration 154, loss = 0.29880697
Iteration 155, loss = 0.29821563
Iteration 156, loss = 0.29823013
Iteration 157, loss = 0.29901214
Iteration 158, loss = 0.29759087
Iteration 159, loss = 0.29841119
Iteration 160, loss = 0.29823044
Iteration 161, loss = 0.29759421
Iteration 162, loss = 0.29765281
Iteration 163, loss = 0.29765605
Iteration 164, loss = 0.29832467
Iteration 165, loss = 0.29738017
Iteration 166, loss = 0.29723760
Iteration 167, loss = 0.29693027
Iteration 168, loss = 0.29793431
Iteration 169, loss = 0.29721810
Iteration 170, loss = 0.29713689
Iteration 171, loss = 0.29653526
Iteration 172, loss = 0.29707424
Iteration 173, loss = 0.29684186
Iteration 174, loss = 0.29696099
Iteration 175, loss = 0.29740984
Iteration 176, loss = 0.29720639
Iteration 177, loss = 0.29669526
Iteration 178, loss = 0.29693839
Iteration 179, loss = 0.29670169
Iteration 180, loss = 0.29634517
Iteration 181, loss = 0.29683943
Iteration 182, loss = 0.29585374
Iteration 183, loss = 0.29696452
Iteration 184, loss = 0.29654679
Iteration 185, loss = 0.29603505
Iteration 186, loss = 0.29586879
Iteration 187, loss = 0.29585934
Iteration 188, loss = 0.29582211
Iteration 189, loss = 0.29578747
Iteration 190, loss = 0.29587003
Iteration 191, loss = 0.29556520
Iteration 192, loss = 0.29598449
Iteration 193, loss = 0.29585242
Iteration 194, loss = 0.29646007
Iteration 195, loss = 0.29538497
Iteration 196, loss = 0.29614131
Iteration 197, loss = 0.29585665
Iteration 198, loss = 0.29511871
Iteration 199, loss = 0.29484373
Iteration 200, loss = 0.29573276
Iteration 201, loss = 0.29475857
Iteration 202, loss = 0.29471171
Iteration 203, loss = 0.29556983
Iteration 204, loss = 0.29526075
Iteration 205, loss = 0.29531250
Iteration 206, loss = 0.29492341
Iteration 207, loss = 0.29558251
Iteration 208, loss = 0.29401410
Iteration 209, loss = 0.29540165
Iteration 210, loss = 0.29415942
Iteration 211, loss = 0.29476946
Iteration 212, loss = 0.29457302
Iteration 213, loss = 0.29468434
Iteration 214, loss = 0.29426324
Iteration 215, loss = 0.29480961
Iteration 216, loss = 0.29523870
Iteration 217, loss = 0.29405130
Iteration 218, loss = 0.29445452
Iteration 219, loss = 0.29404021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_30d_gmm_augment-iter-1.pkl
"ds4 (rp_30d_gmm_augment)",0.876,0.837,0.822,0.811,0.837,0.848,0.798,0.783,0.771,0.800,153.18214178085327
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-57.pkl
Iteration 1, loss = 0.42747304
Iteration 2, loss = 0.38328030
Iteration 3, loss = 0.36889547
Iteration 4, loss = 0.35893433
Iteration 5, loss = 0.35264565
Iteration 6, loss = 0.34745763
Iteration 7, loss = 0.34400342
Iteration 8, loss = 0.34024353
Iteration 9, loss = 0.33731696
Iteration 10, loss = 0.33509760
Iteration 11, loss = 0.33250354
Iteration 12, loss = 0.33151405
Iteration 13, loss = 0.32957575
Iteration 14, loss = 0.32807836
Iteration 15, loss = 0.32560149
Iteration 16, loss = 0.32508509
Iteration 17, loss = 0.32353988
Iteration 18, loss = 0.32307456
Iteration 19, loss = 0.32137493
Iteration 20, loss = 0.32102015
Iteration 21, loss = 0.31929578
Iteration 22, loss = 0.31869752
Iteration 23, loss = 0.31786213
Iteration 24, loss = 0.31703174
Iteration 25, loss = 0.31607813
Iteration 26, loss = 0.31612341
Iteration 27, loss = 0.31472217
Iteration 28, loss = 0.31504735
Iteration 29, loss = 0.31402623
Iteration 30, loss = 0.31344845
Iteration 31, loss = 0.31216857
Iteration 32, loss = 0.31197169
Iteration 33, loss = 0.31214547
Iteration 34, loss = 0.31133005
Iteration 35, loss = 0.31156713
Iteration 36, loss = 0.30965489
Iteration 37, loss = 0.30897406
Iteration 38, loss = 0.30898303
Iteration 39, loss = 0.30841000
Iteration 40, loss = 0.30829076
Iteration 41, loss = 0.30732234
Iteration 42, loss = 0.30752783
Iteration 43, loss = 0.30616675
Iteration 44, loss = 0.30754747
Iteration 45, loss = 0.30550252
Iteration 46, loss = 0.30513040
Iteration 47, loss = 0.30538099
Iteration 48, loss = 0.30482635
Iteration 49, loss = 0.30438356
Iteration 50, loss = 0.30435694
Iteration 51, loss = 0.30352448
Iteration 52, loss = 0.30384153
Iteration 53, loss = 0.30292609
Iteration 54, loss = 0.30295289
Iteration 55, loss = 0.30126776
Iteration 56, loss = 0.30196262
Iteration 57, loss = 0.30154307
Iteration 58, loss = 0.30087793
Iteration 59, loss = 0.30089487
Iteration 60, loss = 0.30083520
Iteration 61, loss = 0.30103843
Iteration 62, loss = 0.29935646
Iteration 63, loss = 0.29983021
Iteration 64, loss = 0.29992048
Iteration 65, loss = 0.29961947
Iteration 66, loss = 0.29918857
Iteration 67, loss = 0.29817311
Iteration 68, loss = 0.29930329
Iteration 69, loss = 0.29876220
Iteration 70, loss = 0.29805499
Iteration 71, loss = 0.29678801
Iteration 72, loss = 0.29758252
Iteration 73, loss = 0.29730244
Iteration 74, loss = 0.29680486
Iteration 75, loss = 0.29740360
Iteration 76, loss = 0.29683322
Iteration 77, loss = 0.29700883
Iteration 78, loss = 0.29561421
Iteration 79, loss = 0.29596365
Iteration 80, loss = 0.29607222
Iteration 81, loss = 0.29566737
Iteration 82, loss = 0.29560057
Iteration 83, loss = 0.29612352
Iteration 84, loss = 0.29525840
Iteration 85, loss = 0.29570114
Iteration 86, loss = 0.29435176
Iteration 87, loss = 0.29415457
Iteration 88, loss = 0.29436606
Iteration 89, loss = 0.29421330
Iteration 90, loss = 0.29387421
Iteration 91, loss = 0.29445195
Iteration 92, loss = 0.29352000
Iteration 93, loss = 0.29314197
Iteration 94, loss = 0.29428830
Iteration 95, loss = 0.29415287
Iteration 96, loss = 0.29306517
Iteration 97, loss = 0.29243889
Iteration 98, loss = 0.29303959
Iteration 99, loss = 0.29213658
Iteration 100, loss = 0.29187426
Iteration 101, loss = 0.29256406
Iteration 102, loss = 0.29323398
Iteration 103, loss = 0.29125082
Iteration 104, loss = 0.29194473
Iteration 105, loss = 0.29146260
Iteration 106, loss = 0.29161295
Iteration 107, loss = 0.29195893
Iteration 108, loss = 0.29161014
Iteration 109, loss = 0.29153208
Iteration 110, loss = 0.29091228
Iteration 111, loss = 0.29113135
Iteration 112, loss = 0.29037632
Iteration 113, loss = 0.29061271
Iteration 114, loss = 0.29062877
Iteration 115, loss = 0.29039704
Iteration 116, loss = 0.29051294
Iteration 117, loss = 0.28947521
Iteration 118, loss = 0.28948541
Iteration 119, loss = 0.28982898
Iteration 120, loss = 0.29027915
Iteration 121, loss = 0.28919396
Iteration 122, loss = 0.28992735
Iteration 123, loss = 0.28944494
Iteration 124, loss = 0.28918724
Iteration 125, loss = 0.28929798
Iteration 126, loss = 0.28907712
Iteration 127, loss = 0.28870326
Iteration 128, loss = 0.28841708
Iteration 129, loss = 0.28922305
Iteration 130, loss = 0.28893410
Iteration 131, loss = 0.28824876
Iteration 132, loss = 0.28790719
Iteration 133, loss = 0.28866729
Iteration 134, loss = 0.28837231
Iteration 135, loss = 0.28844431
Iteration 136, loss = 0.28809320
Iteration 137, loss = 0.28777457
Iteration 138, loss = 0.28773126
Iteration 139, loss = 0.28795274
Iteration 140, loss = 0.28837300
Iteration 141, loss = 0.28791193
Iteration 142, loss = 0.28788656
Iteration 143, loss = 0.28765489
Iteration 144, loss = 0.28762560
Iteration 145, loss = 0.28728177
Iteration 146, loss = 0.28712102
Iteration 147, loss = 0.28697542
Iteration 148, loss = 0.28683845
Iteration 149, loss = 0.28619057
Iteration 150, loss = 0.28605415
Iteration 151, loss = 0.28623596
Iteration 152, loss = 0.28730798
Iteration 153, loss = 0.28638811
Iteration 154, loss = 0.28650356
Iteration 155, loss = 0.28608666
Iteration 156, loss = 0.28615468
Iteration 157, loss = 0.28627144
Iteration 158, loss = 0.28695218
Iteration 159, loss = 0.28622761
Iteration 160, loss = 0.28586580
Iteration 161, loss = 0.28574970
Iteration 162, loss = 0.28583121
Iteration 163, loss = 0.28480726
Iteration 164, loss = 0.28556409
Iteration 165, loss = 0.28510283
Iteration 166, loss = 0.28596412
Iteration 167, loss = 0.28557734
Iteration 168, loss = 0.28531573
Iteration 169, loss = 0.28546118
Iteration 170, loss = 0.28475634
Iteration 171, loss = 0.28519233
Iteration 172, loss = 0.28456750
Iteration 173, loss = 0.28456614
Iteration 174, loss = 0.28561308
Iteration 175, loss = 0.28448121
Iteration 176, loss = 0.28385982
Iteration 177, loss = 0.28497331
Iteration 178, loss = 0.28555115
Iteration 179, loss = 0.28348313
Iteration 180, loss = 0.28407834
Iteration 181, loss = 0.28374031
Iteration 182, loss = 0.28489449
Iteration 183, loss = 0.28387163
Iteration 184, loss = 0.28380297
Iteration 185, loss = 0.28367085
Iteration 186, loss = 0.28368302
Iteration 187, loss = 0.28384287
Iteration 188, loss = 0.28385250
Iteration 189, loss = 0.28486741
Iteration 190, loss = 0.28395959
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_57d_kmeans_augment-iter-1.pkl
"ds4 (rp_57d_kmeans_augment)",0.881,0.855,0.825,0.805,0.842,0.845,0.797,0.770,0.752,0.791,128.1618037223816
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-57.pkl
Iteration 1, loss = 0.42747304
Iteration 2, loss = 0.38328030
Iteration 3, loss = 0.36889547
Iteration 4, loss = 0.35893433
Iteration 5, loss = 0.35264565
Iteration 6, loss = 0.34745763
Iteration 7, loss = 0.34400342
Iteration 8, loss = 0.34024353
Iteration 9, loss = 0.33731696
Iteration 10, loss = 0.33509760
Iteration 11, loss = 0.33250354
Iteration 12, loss = 0.33151405
Iteration 13, loss = 0.32957575
Iteration 14, loss = 0.32807836
Iteration 15, loss = 0.32560149
Iteration 16, loss = 0.32508509
Iteration 17, loss = 0.32353988
Iteration 18, loss = 0.32307456
Iteration 19, loss = 0.32137493
Iteration 20, loss = 0.32102015
Iteration 21, loss = 0.31929578
Iteration 22, loss = 0.31869752
Iteration 23, loss = 0.31786213
Iteration 24, loss = 0.31703174
Iteration 25, loss = 0.31607813
Iteration 26, loss = 0.31612341
Iteration 27, loss = 0.31472217
Iteration 28, loss = 0.31504735
Iteration 29, loss = 0.31402623
Iteration 30, loss = 0.31344845
Iteration 31, loss = 0.31216857
Iteration 32, loss = 0.31197169
Iteration 33, loss = 0.31214547
Iteration 34, loss = 0.31133005
Iteration 35, loss = 0.31156713
Iteration 36, loss = 0.30965489
Iteration 37, loss = 0.30897406
Iteration 38, loss = 0.30898303
Iteration 39, loss = 0.30841000
Iteration 40, loss = 0.30829076
Iteration 41, loss = 0.30732234
Iteration 42, loss = 0.30752783
Iteration 43, loss = 0.30616675
Iteration 44, loss = 0.30754747
Iteration 45, loss = 0.30550252
Iteration 46, loss = 0.30513040
Iteration 47, loss = 0.30538099
Iteration 48, loss = 0.30482635
Iteration 49, loss = 0.30438356
Iteration 50, loss = 0.30435694
Iteration 51, loss = 0.30352448
Iteration 52, loss = 0.30384153
Iteration 53, loss = 0.30292609
Iteration 54, loss = 0.30295289
Iteration 55, loss = 0.30126776
Iteration 56, loss = 0.30196262
Iteration 57, loss = 0.30154307
Iteration 58, loss = 0.30087793
Iteration 59, loss = 0.30089487
Iteration 60, loss = 0.30083520
Iteration 61, loss = 0.30103843
Iteration 62, loss = 0.29935646
Iteration 63, loss = 0.29983021
Iteration 64, loss = 0.29992048
Iteration 65, loss = 0.29961947
Iteration 66, loss = 0.29918857
Iteration 67, loss = 0.29817311
Iteration 68, loss = 0.29930329
Iteration 69, loss = 0.29876220
Iteration 70, loss = 0.29805499
Iteration 71, loss = 0.29678801
Iteration 72, loss = 0.29758252
Iteration 73, loss = 0.29730244
Iteration 74, loss = 0.29680486
Iteration 75, loss = 0.29740360
Iteration 76, loss = 0.29683322
Iteration 77, loss = 0.29700883
Iteration 78, loss = 0.29561421
Iteration 79, loss = 0.29596365
Iteration 80, loss = 0.29607222
Iteration 81, loss = 0.29566737
Iteration 82, loss = 0.29560057
Iteration 83, loss = 0.29612352
Iteration 84, loss = 0.29525840
Iteration 85, loss = 0.29570114
Iteration 86, loss = 0.29435176
Iteration 87, loss = 0.29415457
Iteration 88, loss = 0.29436606
Iteration 89, loss = 0.29421330
Iteration 90, loss = 0.29387421
Iteration 91, loss = 0.29445195
Iteration 92, loss = 0.29352000
Iteration 93, loss = 0.29314197
Iteration 94, loss = 0.29428830
Iteration 95, loss = 0.29415287
Iteration 96, loss = 0.29306517
Iteration 97, loss = 0.29243889
Iteration 98, loss = 0.29303959
Iteration 99, loss = 0.29213658
Iteration 100, loss = 0.29187426
Iteration 101, loss = 0.29256406
Iteration 102, loss = 0.29323398
Iteration 103, loss = 0.29125082
Iteration 104, loss = 0.29194473
Iteration 105, loss = 0.29146260
Iteration 106, loss = 0.29161295
Iteration 107, loss = 0.29195893
Iteration 108, loss = 0.29161014
Iteration 109, loss = 0.29153208
Iteration 110, loss = 0.29091228
Iteration 111, loss = 0.29113135
Iteration 112, loss = 0.29037632
Iteration 113, loss = 0.29061271
Iteration 114, loss = 0.29062877
Iteration 115, loss = 0.29039704
Iteration 116, loss = 0.29051294
Iteration 117, loss = 0.28947521
Iteration 118, loss = 0.28948541
Iteration 119, loss = 0.28982898
Iteration 120, loss = 0.29027915
Iteration 121, loss = 0.28919396
Iteration 122, loss = 0.28992735
Iteration 123, loss = 0.28944494
Iteration 124, loss = 0.28918724
Iteration 125, loss = 0.28929798
Iteration 126, loss = 0.28907712
Iteration 127, loss = 0.28870326
Iteration 128, loss = 0.28841708
Iteration 129, loss = 0.28922305
Iteration 130, loss = 0.28893410
Iteration 131, loss = 0.28824876
Iteration 132, loss = 0.28790719
Iteration 133, loss = 0.28866729
Iteration 134, loss = 0.28837231
Iteration 135, loss = 0.28844431
Iteration 136, loss = 0.28809320
Iteration 137, loss = 0.28777457
Iteration 138, loss = 0.28773126
Iteration 139, loss = 0.28795274
Iteration 140, loss = 0.28837300
Iteration 141, loss = 0.28791193
Iteration 142, loss = 0.28788656
Iteration 143, loss = 0.28765489
Iteration 144, loss = 0.28762560
Iteration 145, loss = 0.28728177
Iteration 146, loss = 0.28712102
Iteration 147, loss = 0.28697542
Iteration 148, loss = 0.28683845
Iteration 149, loss = 0.28619057
Iteration 150, loss = 0.28605415
Iteration 151, loss = 0.28623596
Iteration 152, loss = 0.28730798
Iteration 153, loss = 0.28638811
Iteration 154, loss = 0.28650356
Iteration 155, loss = 0.28608666
Iteration 156, loss = 0.28615468
Iteration 157, loss = 0.28627144
Iteration 158, loss = 0.28695218
Iteration 159, loss = 0.28622761
Iteration 160, loss = 0.28586580
Iteration 161, loss = 0.28574970
Iteration 162, loss = 0.28583121
Iteration 163, loss = 0.28480726
Iteration 164, loss = 0.28556409
Iteration 165, loss = 0.28510283
Iteration 166, loss = 0.28596412
Iteration 167, loss = 0.28557734
Iteration 168, loss = 0.28531573
Iteration 169, loss = 0.28546118
Iteration 170, loss = 0.28475634
Iteration 171, loss = 0.28519233
Iteration 172, loss = 0.28456750
Iteration 173, loss = 0.28456614
Iteration 174, loss = 0.28561308
Iteration 175, loss = 0.28448121
Iteration 176, loss = 0.28385982
Iteration 177, loss = 0.28497331
Iteration 178, loss = 0.28555115
Iteration 179, loss = 0.28348313
Iteration 180, loss = 0.28407834
Iteration 181, loss = 0.28374031
Iteration 182, loss = 0.28489449
Iteration 183, loss = 0.28387163
Iteration 184, loss = 0.28380297
Iteration 185, loss = 0.28367085
Iteration 186, loss = 0.28368302
Iteration 187, loss = 0.28384287
Iteration 188, loss = 0.28385250
Iteration 189, loss = 0.28486741
Iteration 190, loss = 0.28395959
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_57d_gmm_augment-iter-1.pkl
"ds4 (rp_57d_gmm_augment)",0.881,0.855,0.825,0.805,0.842,0.845,0.797,0.770,0.752,0.791,131.2284276485443
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.pkl
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.traintime
"ds1 (Unmodified)",0.815,0.834,0.791,0.778,0.804,0.798,0.814,0.773,0.761,0.787,703.2312424182892
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-pca-10.pkl
Iteration 1, loss = 0.49272073
Iteration 2, loss = 0.46881461
Iteration 3, loss = 0.45715758
Iteration 4, loss = 0.45147792
Iteration 5, loss = 0.44773286
Iteration 6, loss = 0.44466451
Iteration 7, loss = 0.44354645
Iteration 8, loss = 0.44102558
Iteration 9, loss = 0.44034186
Iteration 10, loss = 0.44039592
Iteration 11, loss = 0.43908152
Iteration 12, loss = 0.43821673
Iteration 13, loss = 0.43808078
Iteration 14, loss = 0.43693683
Iteration 15, loss = 0.43647511
Iteration 16, loss = 0.43601266
Iteration 17, loss = 0.43489623
Iteration 18, loss = 0.43449528
Iteration 19, loss = 0.43452492
Iteration 20, loss = 0.43335385
Iteration 21, loss = 0.43470556
Iteration 22, loss = 0.43342295
Iteration 23, loss = 0.43354893
Iteration 24, loss = 0.43238213
Iteration 25, loss = 0.43313748
Iteration 26, loss = 0.43167718
Iteration 27, loss = 0.43081337
Iteration 28, loss = 0.43070858
Iteration 29, loss = 0.43062232
Iteration 30, loss = 0.43020006
Iteration 31, loss = 0.42916662
Iteration 32, loss = 0.43022581
Iteration 33, loss = 0.42917283
Iteration 34, loss = 0.42832379
Iteration 35, loss = 0.42780526
Iteration 36, loss = 0.42898084
Iteration 37, loss = 0.42787581
Iteration 38, loss = 0.42744394
Iteration 39, loss = 0.42733995
Iteration 40, loss = 0.42825050
Iteration 41, loss = 0.42697015
Iteration 42, loss = 0.42613117
Iteration 43, loss = 0.42687967
Iteration 44, loss = 0.42610512
Iteration 45, loss = 0.42599644
Iteration 46, loss = 0.42532952
Iteration 47, loss = 0.42647073
Iteration 48, loss = 0.42573924
Iteration 49, loss = 0.42493492
Iteration 50, loss = 0.42575646
Iteration 51, loss = 0.42482094
Iteration 52, loss = 0.42499474
Iteration 53, loss = 0.42488746
Iteration 54, loss = 0.42382703
Iteration 55, loss = 0.42391833
Iteration 56, loss = 0.42408459
Iteration 57, loss = 0.42363009
Iteration 58, loss = 0.42386010
Iteration 59, loss = 0.42232066
Iteration 60, loss = 0.42330298
Iteration 61, loss = 0.42262138
Iteration 62, loss = 0.42283702
Iteration 63, loss = 0.42240347
Iteration 64, loss = 0.42273781
Iteration 65, loss = 0.42173754
Iteration 66, loss = 0.42183522
Iteration 67, loss = 0.42268044
Iteration 68, loss = 0.42191351
Iteration 69, loss = 0.42143682
Iteration 70, loss = 0.42141272
Iteration 71, loss = 0.42132376
Iteration 72, loss = 0.42096232
Iteration 73, loss = 0.42146226
Iteration 74, loss = 0.42102216
Iteration 75, loss = 0.42091283
Iteration 76, loss = 0.42064503
Iteration 77, loss = 0.42074097
Iteration 78, loss = 0.41975645
Iteration 79, loss = 0.42055704
Iteration 80, loss = 0.41988691
Iteration 81, loss = 0.42027672
Iteration 82, loss = 0.41958327
Iteration 83, loss = 0.41918877
Iteration 84, loss = 0.41905931
Iteration 85, loss = 0.41892431
Iteration 86, loss = 0.41906951
Iteration 87, loss = 0.41900237
Iteration 88, loss = 0.41855289
Iteration 89, loss = 0.41822461
Iteration 90, loss = 0.41811171
Iteration 91, loss = 0.41827748
Iteration 92, loss = 0.41902676
Iteration 93, loss = 0.41796535
Iteration 94, loss = 0.41846257
Iteration 95, loss = 0.41803013
Iteration 96, loss = 0.41787646
Iteration 97, loss = 0.41695989
Iteration 98, loss = 0.41820177
Iteration 99, loss = 0.41781983
Iteration 100, loss = 0.41701947
Iteration 101, loss = 0.41747410
Iteration 102, loss = 0.41687925
Iteration 103, loss = 0.41699373
Iteration 104, loss = 0.41676256
Iteration 105, loss = 0.41611658
Iteration 106, loss = 0.41633841
Iteration 107, loss = 0.41678402
Iteration 108, loss = 0.41640840
Iteration 109, loss = 0.41610507
Iteration 110, loss = 0.41668519
Iteration 111, loss = 0.41619129
Iteration 112, loss = 0.41691277
Iteration 113, loss = 0.41499740
Iteration 114, loss = 0.41585983
Iteration 115, loss = 0.41524634
Iteration 116, loss = 0.41477634
Iteration 117, loss = 0.41470811
Iteration 118, loss = 0.41471994
Iteration 119, loss = 0.41501884
Iteration 120, loss = 0.41419214
Iteration 121, loss = 0.41456348
Iteration 122, loss = 0.41467362
Iteration 123, loss = 0.41446078
Iteration 124, loss = 0.41492747
Iteration 125, loss = 0.41450348
Iteration 126, loss = 0.41461291
Iteration 127, loss = 0.41420071
Iteration 128, loss = 0.41424512
Iteration 129, loss = 0.41431922
Iteration 130, loss = 0.41411811
Iteration 131, loss = 0.41316498
Iteration 132, loss = 0.41344770
Iteration 133, loss = 0.41394380
Iteration 134, loss = 0.41356579
Iteration 135, loss = 0.41328616
Iteration 136, loss = 0.41374969
Iteration 137, loss = 0.41223499
Iteration 138, loss = 0.41214476
Iteration 139, loss = 0.41283132
Iteration 140, loss = 0.41179208
Iteration 141, loss = 0.41319337
Iteration 142, loss = 0.41245218
Iteration 143, loss = 0.41246383
Iteration 144, loss = 0.41250951
Iteration 145, loss = 0.41172789
Iteration 146, loss = 0.41219744
Iteration 147, loss = 0.41240804
Iteration 148, loss = 0.41198680
Iteration 149, loss = 0.41217176
Iteration 150, loss = 0.41166818
Iteration 151, loss = 0.41229222
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--pca_10d_kmeans_augment-iter-1.pkl
"ds1 (pca_10d_kmeans_augment)",0.808,0.829,0.782,0.769,0.797,0.795,0.816,0.767,0.755,0.783,525.3506572246552
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-pca-10.pkl
Iteration 1, loss = 0.49272073
Iteration 2, loss = 0.46881461
Iteration 3, loss = 0.45715758
Iteration 4, loss = 0.45147792
Iteration 5, loss = 0.44773286
Iteration 6, loss = 0.44466451
Iteration 7, loss = 0.44354645
Iteration 8, loss = 0.44102558
Iteration 9, loss = 0.44034186
Iteration 10, loss = 0.44039592
Iteration 11, loss = 0.43908152
Iteration 12, loss = 0.43821673
Iteration 13, loss = 0.43808078
Iteration 14, loss = 0.43693683
Iteration 15, loss = 0.43647511
Iteration 16, loss = 0.43601266
Iteration 17, loss = 0.43489623
Iteration 18, loss = 0.43449528
Iteration 19, loss = 0.43452492
Iteration 20, loss = 0.43335385
Iteration 21, loss = 0.43470556
Iteration 22, loss = 0.43342295
Iteration 23, loss = 0.43354893
Iteration 24, loss = 0.43238213
Iteration 25, loss = 0.43313748
Iteration 26, loss = 0.43167718
Iteration 27, loss = 0.43081337
Iteration 28, loss = 0.43070858
Iteration 29, loss = 0.43062232
Iteration 30, loss = 0.43020006
Iteration 31, loss = 0.42916662
Iteration 32, loss = 0.43022581
Iteration 33, loss = 0.42917283
Iteration 34, loss = 0.42832379
Iteration 35, loss = 0.42780526
Iteration 36, loss = 0.42898084
Iteration 37, loss = 0.42787581
Iteration 38, loss = 0.42744394
Iteration 39, loss = 0.42733995
Iteration 40, loss = 0.42825050
Iteration 41, loss = 0.42697015
Iteration 42, loss = 0.42613117
Iteration 43, loss = 0.42687967
Iteration 44, loss = 0.42610512
Iteration 45, loss = 0.42599644
Iteration 46, loss = 0.42532952
Iteration 47, loss = 0.42647073
Iteration 48, loss = 0.42573924
Iteration 49, loss = 0.42493492
Iteration 50, loss = 0.42575646
Iteration 51, loss = 0.42482094
Iteration 52, loss = 0.42499474
Iteration 53, loss = 0.42488746
Iteration 54, loss = 0.42382703
Iteration 55, loss = 0.42391833
Iteration 56, loss = 0.42408459
Iteration 57, loss = 0.42363009
Iteration 58, loss = 0.42386010
Iteration 59, loss = 0.42232066
Iteration 60, loss = 0.42330298
Iteration 61, loss = 0.42262138
Iteration 62, loss = 0.42283702
Iteration 63, loss = 0.42240347
Iteration 64, loss = 0.42273781
Iteration 65, loss = 0.42173754
Iteration 66, loss = 0.42183522
Iteration 67, loss = 0.42268044
Iteration 68, loss = 0.42191351
Iteration 69, loss = 0.42143682
Iteration 70, loss = 0.42141272
Iteration 71, loss = 0.42132376
Iteration 72, loss = 0.42096232
Iteration 73, loss = 0.42146226
Iteration 74, loss = 0.42102216
Iteration 75, loss = 0.42091283
Iteration 76, loss = 0.42064503
Iteration 77, loss = 0.42074097
Iteration 78, loss = 0.41975645
Iteration 79, loss = 0.42055704
Iteration 80, loss = 0.41988691
Iteration 81, loss = 0.42027672
Iteration 82, loss = 0.41958327
Iteration 83, loss = 0.41918877
Iteration 84, loss = 0.41905931
Iteration 85, loss = 0.41892431
Iteration 86, loss = 0.41906951
Iteration 87, loss = 0.41900237
Iteration 88, loss = 0.41855289
Iteration 89, loss = 0.41822461
Iteration 90, loss = 0.41811171
Iteration 91, loss = 0.41827748
Iteration 92, loss = 0.41902676
Iteration 93, loss = 0.41796535
Iteration 94, loss = 0.41846257
Iteration 95, loss = 0.41803013
Iteration 96, loss = 0.41787646
Iteration 97, loss = 0.41695989
Iteration 98, loss = 0.41820177
Iteration 99, loss = 0.41781983
Iteration 100, loss = 0.41701947
Iteration 101, loss = 0.41747410
Iteration 102, loss = 0.41687925
Iteration 103, loss = 0.41699373
Iteration 104, loss = 0.41676256
Iteration 105, loss = 0.41611658
Iteration 106, loss = 0.41633841
Iteration 107, loss = 0.41678402
Iteration 108, loss = 0.41640840
Iteration 109, loss = 0.41610507
Iteration 110, loss = 0.41668519
Iteration 111, loss = 0.41619129
Iteration 112, loss = 0.41691277
Iteration 113, loss = 0.41499740
Iteration 114, loss = 0.41585983
Iteration 115, loss = 0.41524634
Iteration 116, loss = 0.41477634
Iteration 117, loss = 0.41470811
Iteration 118, loss = 0.41471994
Iteration 119, loss = 0.41501884
Iteration 120, loss = 0.41419214
Iteration 121, loss = 0.41456348
Iteration 122, loss = 0.41467362
Iteration 123, loss = 0.41446078
Iteration 124, loss = 0.41492747
Iteration 125, loss = 0.41450348
Iteration 126, loss = 0.41461291
Iteration 127, loss = 0.41420071
Iteration 128, loss = 0.41424512
Iteration 129, loss = 0.41431922
Iteration 130, loss = 0.41411811
Iteration 131, loss = 0.41316498
Iteration 132, loss = 0.41344770
Iteration 133, loss = 0.41394380
Iteration 134, loss = 0.41356579
Iteration 135, loss = 0.41328616
Iteration 136, loss = 0.41374969
Iteration 137, loss = 0.41223499
Iteration 138, loss = 0.41214476
Iteration 139, loss = 0.41283132
Iteration 140, loss = 0.41179208
Iteration 141, loss = 0.41319337
Iteration 142, loss = 0.41245218
Iteration 143, loss = 0.41246383
Iteration 144, loss = 0.41250951
Iteration 145, loss = 0.41172789
Iteration 146, loss = 0.41219744
Iteration 147, loss = 0.41240804
Iteration 148, loss = 0.41198680
Iteration 149, loss = 0.41217176
Iteration 150, loss = 0.41166818
Iteration 151, loss = 0.41229222
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--pca_10d_gmm_augment-iter-1.pkl
"ds1 (pca_10d_gmm_augment)",0.808,0.829,0.782,0.769,0.797,0.795,0.816,0.767,0.755,0.783,527.4964978694916
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-rp-10.pkl
Iteration 1, loss = 0.49708751
Iteration 2, loss = 0.47798733
Iteration 3, loss = 0.46824989
Iteration 4, loss = 0.46090861
Iteration 5, loss = 0.45488662
Iteration 6, loss = 0.45270099
Iteration 7, loss = 0.44961993
Iteration 8, loss = 0.44658381
Iteration 9, loss = 0.44573693
Iteration 10, loss = 0.44582279
Iteration 11, loss = 0.44305325
Iteration 12, loss = 0.44218140
Iteration 13, loss = 0.44310165
Iteration 14, loss = 0.44148564
Iteration 15, loss = 0.44057968
Iteration 16, loss = 0.44147924
Iteration 17, loss = 0.44015900
Iteration 18, loss = 0.43854647
Iteration 19, loss = 0.43813330
Iteration 20, loss = 0.43692479
Iteration 21, loss = 0.43711050
Iteration 22, loss = 0.43647774
Iteration 23, loss = 0.43667940
Iteration 24, loss = 0.43551723
Iteration 25, loss = 0.43615494
Iteration 26, loss = 0.43441845
Iteration 27, loss = 0.43321007
Iteration 28, loss = 0.43323554
Iteration 29, loss = 0.43327799
Iteration 30, loss = 0.43322853
Iteration 31, loss = 0.43261632
Iteration 32, loss = 0.43234737
Iteration 33, loss = 0.43168397
Iteration 34, loss = 0.43165050
Iteration 35, loss = 0.43022508
Iteration 36, loss = 0.43025409
Iteration 37, loss = 0.43065162
Iteration 38, loss = 0.42900829
Iteration 39, loss = 0.43002887
Iteration 40, loss = 0.43000029
Iteration 41, loss = 0.43001619
Iteration 42, loss = 0.42758869
Iteration 43, loss = 0.42839421
Iteration 44, loss = 0.42829607
Iteration 45, loss = 0.42844588
Iteration 46, loss = 0.42736120
Iteration 47, loss = 0.42777320
Iteration 48, loss = 0.42691546
Iteration 49, loss = 0.42777339
Iteration 50, loss = 0.42711761
Iteration 51, loss = 0.42625111
Iteration 52, loss = 0.42679664
Iteration 53, loss = 0.42820706
Iteration 54, loss = 0.42600969
Iteration 55, loss = 0.42690663
Iteration 56, loss = 0.42507609
Iteration 57, loss = 0.42538202
Iteration 58, loss = 0.42549477
Iteration 59, loss = 0.42434902
Iteration 60, loss = 0.42537245
Iteration 61, loss = 0.42345991
Iteration 62, loss = 0.42515908
Iteration 63, loss = 0.42386532
Iteration 64, loss = 0.42407617
Iteration 65, loss = 0.42413095
Iteration 66, loss = 0.42323999
Iteration 67, loss = 0.42393126
Iteration 68, loss = 0.42377691
Iteration 69, loss = 0.42265657
Iteration 70, loss = 0.42273110
Iteration 71, loss = 0.42278034
Iteration 72, loss = 0.42198890
Iteration 73, loss = 0.42231191
Iteration 74, loss = 0.42189989
Iteration 75, loss = 0.42187950
Iteration 76, loss = 0.42222884
Iteration 77, loss = 0.42179028
Iteration 78, loss = 0.42074996
Iteration 79, loss = 0.42275751
Iteration 80, loss = 0.42139659
Iteration 81, loss = 0.42194579
Iteration 82, loss = 0.42172733
Iteration 83, loss = 0.41994925
Iteration 84, loss = 0.42017487
Iteration 85, loss = 0.42323790
Iteration 86, loss = 0.42233591
Iteration 87, loss = 0.42027190
Iteration 88, loss = 0.41905359
Iteration 89, loss = 0.41965783
Iteration 90, loss = 0.41980104
Iteration 91, loss = 0.41993060
Iteration 92, loss = 0.42067263
Iteration 93, loss = 0.41936474
Iteration 94, loss = 0.41897628
Iteration 95, loss = 0.41854877
Iteration 96, loss = 0.41891903
Iteration 97, loss = 0.41872600
Iteration 98, loss = 0.41857423
Iteration 99, loss = 0.41822924
Iteration 100, loss = 0.41893502
Iteration 101, loss = 0.41815509
Iteration 102, loss = 0.41862991
Iteration 103, loss = 0.41758355
Iteration 104, loss = 0.41769447
Iteration 105, loss = 0.41711355
Iteration 106, loss = 0.41672479
Iteration 107, loss = 0.41701573
Iteration 108, loss = 0.41744438
Iteration 109, loss = 0.41657393
Iteration 110, loss = 0.41734466
Iteration 111, loss = 0.41635092
Iteration 112, loss = 0.41731420
Iteration 113, loss = 0.41617559
Iteration 114, loss = 0.41748493
Iteration 115, loss = 0.41616781
Iteration 116, loss = 0.41692212
Iteration 117, loss = 0.41617978
Iteration 118, loss = 0.41572512
Iteration 119, loss = 0.41579697
Iteration 120, loss = 0.41498747
Iteration 121, loss = 0.41544730
Iteration 122, loss = 0.41484022
Iteration 123, loss = 0.41483800
Iteration 124, loss = 0.41484458
Iteration 125, loss = 0.41566466
Iteration 126, loss = 0.41487182
Iteration 127, loss = 0.41513045
Iteration 128, loss = 0.41616027
Iteration 129, loss = 0.41531355
Iteration 130, loss = 0.41488518
Iteration 131, loss = 0.41475584
Iteration 132, loss = 0.41364201
Iteration 133, loss = 0.41463965
Iteration 134, loss = 0.41337287
Iteration 135, loss = 0.41390844
Iteration 136, loss = 0.41461486
Iteration 137, loss = 0.41331667
Iteration 138, loss = 0.41282367
Iteration 139, loss = 0.41413578
Iteration 140, loss = 0.41263765
Iteration 141, loss = 0.41456683
Iteration 142, loss = 0.41307234
Iteration 143, loss = 0.41296742
Iteration 144, loss = 0.41380023
Iteration 145, loss = 0.41320432
Iteration 146, loss = 0.41278329
Iteration 147, loss = 0.41263347
Iteration 148, loss = 0.41171481
Iteration 149, loss = 0.41239134
Iteration 150, loss = 0.41327522
Iteration 151, loss = 0.41131567
Iteration 152, loss = 0.41231675
Iteration 153, loss = 0.41217538
Iteration 154, loss = 0.41115081
Iteration 155, loss = 0.41132187
Iteration 156, loss = 0.41205317
Iteration 157, loss = 0.41114332
Iteration 158, loss = 0.41165000
Iteration 159, loss = 0.41188674
Iteration 160, loss = 0.41137733
Iteration 161, loss = 0.41048142
Iteration 162, loss = 0.41095714
Iteration 163, loss = 0.41193413
Iteration 164, loss = 0.41055999
Iteration 165, loss = 0.41040319
Iteration 166, loss = 0.41095852
Iteration 167, loss = 0.41099340
Iteration 168, loss = 0.41046803
Iteration 169, loss = 0.41003210
Iteration 170, loss = 0.40949803
Iteration 171, loss = 0.41085393
Iteration 172, loss = 0.40999503
Iteration 173, loss = 0.40958707
Iteration 174, loss = 0.40993149
Iteration 175, loss = 0.40914105
Iteration 176, loss = 0.40991384
Iteration 177, loss = 0.40980573
Iteration 178, loss = 0.40896211
Iteration 179, loss = 0.40977656
Iteration 180, loss = 0.40924877
Iteration 181, loss = 0.40906909
Iteration 182, loss = 0.40929544
Iteration 183, loss = 0.40812785
Iteration 184, loss = 0.40849683
Iteration 185, loss = 0.41030486
Iteration 186, loss = 0.40856298
Iteration 187, loss = 0.40960889
Iteration 188, loss = 0.40818108
Iteration 189, loss = 0.40831243
Iteration 190, loss = 0.40786371
Iteration 191, loss = 0.40810147
Iteration 192, loss = 0.40818446
Iteration 193, loss = 0.40785341
Iteration 194, loss = 0.40838383
Iteration 195, loss = 0.40846166
Iteration 196, loss = 0.40898891
Iteration 197, loss = 0.40964887
Iteration 198, loss = 0.40820200
Iteration 199, loss = 0.40621022
Iteration 200, loss = 0.40851142
Iteration 201, loss = 0.40759340
Iteration 202, loss = 0.40646021
Iteration 203, loss = 0.40726187
Iteration 204, loss = 0.40636566
Iteration 205, loss = 0.40793608
Iteration 206, loss = 0.40673402
Iteration 207, loss = 0.40738162
Iteration 208, loss = 0.41029775
Iteration 209, loss = 0.40791954
Iteration 210, loss = 0.40591962
Iteration 211, loss = 0.40607006
Iteration 212, loss = 0.40730120
Iteration 213, loss = 0.40719689
Iteration 214, loss = 0.40691888
Iteration 215, loss = 0.40620310
Iteration 216, loss = 0.40606300
Iteration 217, loss = 0.40703338
Iteration 218, loss = 0.40704800
Iteration 219, loss = 0.40613132
Iteration 220, loss = 0.40632105
Iteration 221, loss = 0.40596473
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--rp_10d_kmeans_augment-iter-1.pkl
"ds1 (rp_10d_kmeans_augment)",0.811,0.834,0.785,0.772,0.800,0.797,0.819,0.768,0.755,0.785,756.767541885376
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-rp-10.pkl
Iteration 1, loss = 0.49708751
Iteration 2, loss = 0.47798733
Iteration 3, loss = 0.46824989
Iteration 4, loss = 0.46090861
Iteration 5, loss = 0.45488662
Iteration 6, loss = 0.45270099
Iteration 7, loss = 0.44961993
Iteration 8, loss = 0.44658381
Iteration 9, loss = 0.44573693
Iteration 10, loss = 0.44582279
Iteration 11, loss = 0.44305325
Iteration 12, loss = 0.44218140
Iteration 13, loss = 0.44310165
Iteration 14, loss = 0.44148564
Iteration 15, loss = 0.44057968
Iteration 16, loss = 0.44147924
Iteration 17, loss = 0.44015900
Iteration 18, loss = 0.43854647
Iteration 19, loss = 0.43813330
Iteration 20, loss = 0.43692479
Iteration 21, loss = 0.43711050
Iteration 22, loss = 0.43647774
Iteration 23, loss = 0.43667940
Iteration 24, loss = 0.43551723
Iteration 25, loss = 0.43615494
Iteration 26, loss = 0.43441845
Iteration 27, loss = 0.43321007
Iteration 28, loss = 0.43323554
Iteration 29, loss = 0.43327799
Iteration 30, loss = 0.43322853
Iteration 31, loss = 0.43261632
Iteration 32, loss = 0.43234737
Iteration 33, loss = 0.43168397
Iteration 34, loss = 0.43165050
Iteration 35, loss = 0.43022508
Iteration 36, loss = 0.43025409
Iteration 37, loss = 0.43065162
Iteration 38, loss = 0.42900829
Iteration 39, loss = 0.43002887
Iteration 40, loss = 0.43000029
Iteration 41, loss = 0.43001619
Iteration 42, loss = 0.42758869
Iteration 43, loss = 0.42839421
Iteration 44, loss = 0.42829607
Iteration 45, loss = 0.42844588
Iteration 46, loss = 0.42736120
Iteration 47, loss = 0.42777320
Iteration 48, loss = 0.42691546
Iteration 49, loss = 0.42777339
Iteration 50, loss = 0.42711761
Iteration 51, loss = 0.42625111
Iteration 52, loss = 0.42679664
Iteration 53, loss = 0.42820706
Iteration 54, loss = 0.42600969
Iteration 55, loss = 0.42690663
Iteration 56, loss = 0.42507609
Iteration 57, loss = 0.42538202
Iteration 58, loss = 0.42549477
Iteration 59, loss = 0.42434902
Iteration 60, loss = 0.42537245
Iteration 61, loss = 0.42345991
Iteration 62, loss = 0.42515908
Iteration 63, loss = 0.42386532
Iteration 64, loss = 0.42407617
Iteration 65, loss = 0.42413095
Iteration 66, loss = 0.42323999
Iteration 67, loss = 0.42393126
Iteration 68, loss = 0.42377691
Iteration 69, loss = 0.42265657
Iteration 70, loss = 0.42273110
Iteration 71, loss = 0.42278034
Iteration 72, loss = 0.42198890
Iteration 73, loss = 0.42231191
Iteration 74, loss = 0.42189989
Iteration 75, loss = 0.42187950
Iteration 76, loss = 0.42222884
Iteration 77, loss = 0.42179028
Iteration 78, loss = 0.42074996
Iteration 79, loss = 0.42275751
Iteration 80, loss = 0.42139659
Iteration 81, loss = 0.42194579
Iteration 82, loss = 0.42172733
Iteration 83, loss = 0.41994925
Iteration 84, loss = 0.42017487
Iteration 85, loss = 0.42323790
Iteration 86, loss = 0.42233591
Iteration 87, loss = 0.42027190
Iteration 88, loss = 0.41905359
Iteration 89, loss = 0.41965783
Iteration 90, loss = 0.41980104
Iteration 91, loss = 0.41993060
Iteration 92, loss = 0.42067263
Iteration 93, loss = 0.41936474
Iteration 94, loss = 0.41897628
Iteration 95, loss = 0.41854877
Iteration 96, loss = 0.41891903
Iteration 97, loss = 0.41872600
Iteration 98, loss = 0.41857423
Iteration 99, loss = 0.41822924
Iteration 100, loss = 0.41893502
Iteration 101, loss = 0.41815509
Iteration 102, loss = 0.41862991
Iteration 103, loss = 0.41758355
Iteration 104, loss = 0.41769447
Iteration 105, loss = 0.41711355
Iteration 106, loss = 0.41672479
Iteration 107, loss = 0.41701573
Iteration 108, loss = 0.41744438
Iteration 109, loss = 0.41657393
Iteration 110, loss = 0.41734466
Iteration 111, loss = 0.41635092
Iteration 112, loss = 0.41731420
Iteration 113, loss = 0.41617559
Iteration 114, loss = 0.41748493
Iteration 115, loss = 0.41616781
Iteration 116, loss = 0.41692212
Iteration 117, loss = 0.41617978
Iteration 118, loss = 0.41572512
Iteration 119, loss = 0.41579697
Iteration 120, loss = 0.41498747
Iteration 121, loss = 0.41544730
Iteration 122, loss = 0.41484022
Iteration 123, loss = 0.41483800
Iteration 124, loss = 0.41484458
Iteration 125, loss = 0.41566466
Iteration 126, loss = 0.41487182
Iteration 127, loss = 0.41513045
Iteration 128, loss = 0.41616027
Iteration 129, loss = 0.41531355
Iteration 130, loss = 0.41488518
Iteration 131, loss = 0.41475584
Iteration 132, loss = 0.41364201
Iteration 133, loss = 0.41463965
Iteration 134, loss = 0.41337287
Iteration 135, loss = 0.41390844
Iteration 136, loss = 0.41461486
Iteration 137, loss = 0.41331667
Iteration 138, loss = 0.41282367
Iteration 139, loss = 0.41413578
Iteration 140, loss = 0.41263765
Iteration 141, loss = 0.41456683
Iteration 142, loss = 0.41307234
Iteration 143, loss = 0.41296742
Iteration 144, loss = 0.41380023
Iteration 145, loss = 0.41320432
Iteration 146, loss = 0.41278329
Iteration 147, loss = 0.41263347
Iteration 148, loss = 0.41171481
Iteration 149, loss = 0.41239134
Iteration 150, loss = 0.41327522
Iteration 151, loss = 0.41131567
Iteration 152, loss = 0.41231675
Iteration 153, loss = 0.41217538
Iteration 154, loss = 0.41115081
Iteration 155, loss = 0.41132187
Iteration 156, loss = 0.41205317
Iteration 157, loss = 0.41114332
Iteration 158, loss = 0.41165000
Iteration 159, loss = 0.41188674
Iteration 160, loss = 0.41137733
Iteration 161, loss = 0.41048142
Iteration 162, loss = 0.41095714
Iteration 163, loss = 0.41193413
Iteration 164, loss = 0.41055999
Iteration 165, loss = 0.41040319
Iteration 166, loss = 0.41095852
Iteration 167, loss = 0.41099340
Iteration 168, loss = 0.41046803
Iteration 169, loss = 0.41003210
Iteration 170, loss = 0.40949803
Iteration 171, loss = 0.41085393
Iteration 172, loss = 0.40999503
Iteration 173, loss = 0.40958707
Iteration 174, loss = 0.40993149
Iteration 175, loss = 0.40914105
Iteration 176, loss = 0.40991384
Iteration 177, loss = 0.40980573
Iteration 178, loss = 0.40896211
Iteration 179, loss = 0.40977656
Iteration 180, loss = 0.40924877
Iteration 181, loss = 0.40906909
Iteration 182, loss = 0.40929544
Iteration 183, loss = 0.40812785
Iteration 184, loss = 0.40849683
Iteration 185, loss = 0.41030486
Iteration 186, loss = 0.40856298
Iteration 187, loss = 0.40960889
Iteration 188, loss = 0.40818108
Iteration 189, loss = 0.40831243
Iteration 190, loss = 0.40786371
Iteration 191, loss = 0.40810147
Iteration 192, loss = 0.40818446
Iteration 193, loss = 0.40785341
Iteration 194, loss = 0.40838383
Iteration 195, loss = 0.40846166
Iteration 196, loss = 0.40898891
Iteration 197, loss = 0.40964887
Iteration 198, loss = 0.40820200
Iteration 199, loss = 0.40621022
Iteration 200, loss = 0.40851142
Iteration 201, loss = 0.40759340
Iteration 202, loss = 0.40646021
Iteration 203, loss = 0.40726187
Iteration 204, loss = 0.40636566
Iteration 205, loss = 0.40793608
Iteration 206, loss = 0.40673402
Iteration 207, loss = 0.40738162
Iteration 208, loss = 0.41029775
Iteration 209, loss = 0.40791954
Iteration 210, loss = 0.40591962
Iteration 211, loss = 0.40607006
Iteration 212, loss = 0.40730120
Iteration 213, loss = 0.40719689
Iteration 214, loss = 0.40691888
Iteration 215, loss = 0.40620310
Iteration 216, loss = 0.40606300
Iteration 217, loss = 0.40703338
Iteration 218, loss = 0.40704800
Iteration 219, loss = 0.40613132
Iteration 220, loss = 0.40632105
Iteration 221, loss = 0.40596473
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--rp_10d_gmm_augment-iter-1.pkl
"ds1 (rp_10d_gmm_augment)",0.811,0.834,0.785,0.772,0.800,0.797,0.819,0.768,0.755,0.785,759.9720532894135
Write part5_losdos_report.csv
