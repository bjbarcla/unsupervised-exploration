"Dataset","Xval Accuracy","Xval Precision","Xval F1 Score","Xval Recall","Xval Aggregate","Test Set Accuracy","Test Set Precision","Test Set F1 Score","Test Set Recall","Test Set Aggregate","Train Time"
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.pkl
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.traintime
"ds4 (Unmodified)",0.885,0.871,0.827,0.799,0.846,0.845,0.803,0.765,0.743,0.789,154.79517817497253
** -> kmeans fit for replace transformer for dataset ds4 elapsed seconds 1.914597749710083 <- took 1.0 second
Iteration 1, loss = 0.52696436
Iteration 2, loss = 0.48992831
Iteration 3, loss = 0.47115844
Iteration 4, loss = 0.46019144
Iteration 5, loss = 0.45372215
Iteration 6, loss = 0.44985276
Iteration 7, loss = 0.44758766
Iteration 8, loss = 0.44625807
Iteration 9, loss = 0.44548693
Iteration 10, loss = 0.44500186
Iteration 11, loss = 0.44473544
Iteration 12, loss = 0.44457523
Iteration 13, loss = 0.44446654
Iteration 14, loss = 0.44442266
Iteration 15, loss = 0.44435865
Iteration 16, loss = 0.44435463
Iteration 17, loss = 0.44432526
Iteration 18, loss = 0.44434512
Iteration 19, loss = 0.44435518
Iteration 20, loss = 0.44434240
Iteration 21, loss = 0.44433427
Iteration 22, loss = 0.44431898
Iteration 23, loss = 0.44425086
Iteration 24, loss = 0.44434331
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--kmeans_replace-iter-1.pkl
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
"ds4 (kmeans_replace)",0.760,0.380,0.432,0.500,0.518,0.760,0.380,0.432,0.500,0.518,4.357971906661987
** -> kmeans fit for augment transformer for dataset ds4 elapsed seconds 1.8750677108764648 <- took 1.0 second
Iteration 1, loss = 0.74528397
Iteration 2, loss = 0.62336567
Iteration 3, loss = 0.54575726
Iteration 4, loss = 0.49329923
Iteration 5, loss = 0.45590204
Iteration 6, loss = 0.42947264
Iteration 7, loss = 0.40867255
Iteration 8, loss = 0.39336058
Iteration 9, loss = 0.38117258
Iteration 10, loss = 0.37079415
Iteration 11, loss = 0.36260799
Iteration 12, loss = 0.35566799
Iteration 13, loss = 0.35043523
Iteration 14, loss = 0.34512443
Iteration 15, loss = 0.34106215
Iteration 16, loss = 0.33685798
Iteration 17, loss = 0.33306397
Iteration 18, loss = 0.32994608
Iteration 19, loss = 0.32742269
Iteration 20, loss = 0.32456933
Iteration 21, loss = 0.32184697
Iteration 22, loss = 0.32025777
Iteration 23, loss = 0.31765365
Iteration 24, loss = 0.31570623
Iteration 25, loss = 0.31466606
Iteration 26, loss = 0.31330409
Iteration 27, loss = 0.31094784
Iteration 28, loss = 0.30994651
Iteration 29, loss = 0.30925663
Iteration 30, loss = 0.30773924
Iteration 31, loss = 0.30735553
Iteration 32, loss = 0.30598392
Iteration 33, loss = 0.30404383
Iteration 34, loss = 0.30351087
Iteration 35, loss = 0.30276859
Iteration 36, loss = 0.30157665
Iteration 37, loss = 0.30060136
Iteration 38, loss = 0.30083741
Iteration 39, loss = 0.29924820
Iteration 40, loss = 0.29853141
Iteration 41, loss = 0.29783573
Iteration 42, loss = 0.29767541
Iteration 43, loss = 0.29679969
Iteration 44, loss = 0.29639483
Iteration 45, loss = 0.29511093
Iteration 46, loss = 0.29471222
Iteration 47, loss = 0.29419508
Iteration 48, loss = 0.29349221
Iteration 49, loss = 0.29314031
Iteration 50, loss = 0.29268270
Iteration 51, loss = 0.29122306
Iteration 52, loss = 0.29154168
Iteration 53, loss = 0.29146881
Iteration 54, loss = 0.29083398
Iteration 55, loss = 0.29007888
Iteration 56, loss = 0.28963142
Iteration 57, loss = 0.28891204
Iteration 58, loss = 0.28903884
Iteration 59, loss = 0.28907469
Iteration 60, loss = 0.28846895
Iteration 61, loss = 0.28831870
Iteration 62, loss = 0.28710028
Iteration 63, loss = 0.28736609
Iteration 64, loss = 0.28647852
Iteration 65, loss = 0.28607697
Iteration 66, loss = 0.28566378
Iteration 67, loss = 0.28544048
Iteration 68, loss = 0.28533222
Iteration 69, loss = 0.28481653
Iteration 70, loss = 0.28426318
Iteration 71, loss = 0.28367638
Iteration 72, loss = 0.28360245
Iteration 73, loss = 0.28355847
Iteration 74, loss = 0.28326764
Iteration 75, loss = 0.28211390
Iteration 76, loss = 0.28230480
Iteration 77, loss = 0.28234276
Iteration 78, loss = 0.28222028
Iteration 79, loss = 0.28180875
Iteration 80, loss = 0.28221272
Iteration 81, loss = 0.28155340
Iteration 82, loss = 0.28137561
Iteration 83, loss = 0.28127408
Iteration 84, loss = 0.27946147
Iteration 85, loss = 0.28037696
Iteration 86, loss = 0.28013249
Iteration 87, loss = 0.27953762
Iteration 88, loss = 0.27912631
Iteration 89, loss = 0.27884263
Iteration 90, loss = 0.27934419
Iteration 91, loss = 0.27843656
Iteration 92, loss = 0.27881078
Iteration 93, loss = 0.27782996
Iteration 94, loss = 0.27846857
Iteration 95, loss = 0.27779803
Iteration 96, loss = 0.27707547
Iteration 97, loss = 0.27683434
Iteration 98, loss = 0.27651820
Iteration 99, loss = 0.27682536
Iteration 100, loss = 0.27713836
Iteration 101, loss = 0.27651909
Iteration 102, loss = 0.27654836
Iteration 103, loss = 0.27703185
Iteration 104, loss = 0.27651689
Iteration 105, loss = 0.27655709
Iteration 106, loss = 0.27653332
Iteration 107, loss = 0.27632185
Iteration 108, loss = 0.27591531
Iteration 109, loss = 0.27446842
Iteration 110, loss = 0.27453463
Iteration 111, loss = 0.27425590
Iteration 112, loss = 0.27418321
Iteration 113, loss = 0.27366489
Iteration 114, loss = 0.27464583
Iteration 115, loss = 0.27345973
Iteration 116, loss = 0.27433063
Iteration 117, loss = 0.27379443
Iteration 118, loss = 0.27297755
Iteration 119, loss = 0.27330065
Iteration 120, loss = 0.27252215
Iteration 121, loss = 0.27282191
Iteration 122, loss = 0.27189959
Iteration 123, loss = 0.27201942
Iteration 124, loss = 0.27225944
Iteration 125, loss = 0.27231144
Iteration 126, loss = 0.27197999
Iteration 127, loss = 0.27116089
Iteration 128, loss = 0.27128911
Iteration 129, loss = 0.27121329
Iteration 130, loss = 0.27135050
Iteration 131, loss = 0.27118441
Iteration 132, loss = 0.27077316
Iteration 133, loss = 0.27179248
Iteration 134, loss = 0.27266842
Iteration 135, loss = 0.27122780
Iteration 136, loss = 0.27032903
Iteration 137, loss = 0.27029888
Iteration 138, loss = 0.27021843
Iteration 139, loss = 0.26978813
Iteration 140, loss = 0.27006505
Iteration 141, loss = 0.26969215
Iteration 142, loss = 0.26935166
Iteration 143, loss = 0.26917552
Iteration 144, loss = 0.26968687
Iteration 145, loss = 0.26968346
Iteration 146, loss = 0.26886224
Iteration 147, loss = 0.26822811
Iteration 148, loss = 0.26923242
Iteration 149, loss = 0.26855987
Iteration 150, loss = 0.26885050
Iteration 151, loss = 0.26930121
Iteration 152, loss = 0.26869765
Iteration 153, loss = 0.26857843
Iteration 154, loss = 0.26858510
Iteration 155, loss = 0.26773108
Iteration 156, loss = 0.26821418
Iteration 157, loss = 0.26790605
Iteration 158, loss = 0.26768093
Iteration 159, loss = 0.26794247
Iteration 160, loss = 0.26817133
Iteration 161, loss = 0.26835390
Iteration 162, loss = 0.26780837
Iteration 163, loss = 0.26852611
Iteration 164, loss = 0.26714714
Iteration 165, loss = 0.26784380
Iteration 166, loss = 0.26764985
Iteration 167, loss = 0.26689691
Iteration 168, loss = 0.26796283
Iteration 169, loss = 0.26732173
Iteration 170, loss = 0.26737766
Iteration 171, loss = 0.26707213
Iteration 172, loss = 0.26658077
Iteration 173, loss = 0.26623292
Iteration 174, loss = 0.26695416
Iteration 175, loss = 0.26702418
Iteration 176, loss = 0.26627559
Iteration 177, loss = 0.26655097
Iteration 178, loss = 0.26625486
Iteration 179, loss = 0.26644235
Iteration 180, loss = 0.26667097
Iteration 181, loss = 0.26614413
Iteration 182, loss = 0.26670100
Iteration 183, loss = 0.26615164
Iteration 184, loss = 0.26608991
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--kmeans_augment-iter-1.pkl
"ds4 (kmeans_augment)",0.893,0.861,0.849,0.838,0.860,0.843,0.788,0.777,0.767,0.794,123.55614233016968
** -> gmm fit for replace transformer for dataset ds4 elapsed seconds 0.6126511096954346 <- took less than 1 second
Iteration 1, loss = 0.52630590
Iteration 2, loss = 0.49015432
Iteration 3, loss = 0.47141113
Iteration 4, loss = 0.46048718
Iteration 5, loss = 0.45407784
Iteration 6, loss = 0.45027024
Iteration 7, loss = 0.44806225
Iteration 8, loss = 0.44677454
Iteration 9, loss = 0.44604025
Iteration 10, loss = 0.44558114
Iteration 11, loss = 0.44533699
Iteration 12, loss = 0.44519061
Iteration 13, loss = 0.44509217
Iteration 14, loss = 0.44505230
Iteration 15, loss = 0.44498791
Iteration 16, loss = 0.44498821
Iteration 17, loss = 0.44496508
Iteration 18, loss = 0.44498405
Iteration 19, loss = 0.44498485
Iteration 20, loss = 0.44498151
Iteration 21, loss = 0.44497282
Iteration 22, loss = 0.44495358
Iteration 23, loss = 0.44489415
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--gmm_replace-iter-1.pkl
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
"ds4 (gmm_replace)",0.760,0.380,0.432,0.500,0.518,0.760,0.380,0.432,0.500,0.518,4.239794015884399
** -> gmm fit for augment transformer for dataset ds4 elapsed seconds 0.6161150932312012 <- took less than 1 second
Iteration 1, loss = 0.74520306
Iteration 2, loss = 0.62353683
Iteration 3, loss = 0.54609604
Iteration 4, loss = 0.49357541
Iteration 5, loss = 0.45612857
Iteration 6, loss = 0.42969094
Iteration 7, loss = 0.40870569
Iteration 8, loss = 0.39340768
Iteration 9, loss = 0.38091645
Iteration 10, loss = 0.37052837
Iteration 11, loss = 0.36222978
Iteration 12, loss = 0.35523855
Iteration 13, loss = 0.34966416
Iteration 14, loss = 0.34452894
Iteration 15, loss = 0.34038611
Iteration 16, loss = 0.33612801
Iteration 17, loss = 0.33229880
Iteration 18, loss = 0.32913214
Iteration 19, loss = 0.32665667
Iteration 20, loss = 0.32391683
Iteration 21, loss = 0.32123042
Iteration 22, loss = 0.31965733
Iteration 23, loss = 0.31700123
Iteration 24, loss = 0.31522286
Iteration 25, loss = 0.31395515
Iteration 26, loss = 0.31262764
Iteration 27, loss = 0.31028905
Iteration 28, loss = 0.30932171
Iteration 29, loss = 0.30841525
Iteration 30, loss = 0.30718240
Iteration 31, loss = 0.30666809
Iteration 32, loss = 0.30541608
Iteration 33, loss = 0.30365686
Iteration 34, loss = 0.30301869
Iteration 35, loss = 0.30225533
Iteration 36, loss = 0.30114355
Iteration 37, loss = 0.30028450
Iteration 38, loss = 0.30060506
Iteration 39, loss = 0.29896831
Iteration 40, loss = 0.29844233
Iteration 41, loss = 0.29831310
Iteration 42, loss = 0.29796554
Iteration 43, loss = 0.29664621
Iteration 44, loss = 0.29638989
Iteration 45, loss = 0.29508784
Iteration 46, loss = 0.29498735
Iteration 47, loss = 0.29402392
Iteration 48, loss = 0.29341494
Iteration 49, loss = 0.29318925
Iteration 50, loss = 0.29260515
Iteration 51, loss = 0.29133746
Iteration 52, loss = 0.29179416
Iteration 53, loss = 0.29185667
Iteration 54, loss = 0.29122324
Iteration 55, loss = 0.29043566
Iteration 56, loss = 0.28972185
Iteration 57, loss = 0.28901717
Iteration 58, loss = 0.28905067
Iteration 59, loss = 0.28902098
Iteration 60, loss = 0.28837975
Iteration 61, loss = 0.28795228
Iteration 62, loss = 0.28772007
Iteration 63, loss = 0.28762253
Iteration 64, loss = 0.28726530
Iteration 65, loss = 0.28658291
Iteration 66, loss = 0.28623323
Iteration 67, loss = 0.28614356
Iteration 68, loss = 0.28580493
Iteration 69, loss = 0.28501152
Iteration 70, loss = 0.28511797
Iteration 71, loss = 0.28399229
Iteration 72, loss = 0.28436546
Iteration 73, loss = 0.28406610
Iteration 74, loss = 0.28370155
Iteration 75, loss = 0.28281512
Iteration 76, loss = 0.28293957
Iteration 77, loss = 0.28293069
Iteration 78, loss = 0.28250182
Iteration 79, loss = 0.28235927
Iteration 80, loss = 0.28262882
Iteration 81, loss = 0.28208655
Iteration 82, loss = 0.28231159
Iteration 83, loss = 0.28230958
Iteration 84, loss = 0.28019633
Iteration 85, loss = 0.28142448
Iteration 86, loss = 0.28082811
Iteration 87, loss = 0.28051258
Iteration 88, loss = 0.28016676
Iteration 89, loss = 0.27959892
Iteration 90, loss = 0.27955574
Iteration 91, loss = 0.27932654
Iteration 92, loss = 0.27942289
Iteration 93, loss = 0.27891618
Iteration 94, loss = 0.27940760
Iteration 95, loss = 0.27867373
Iteration 96, loss = 0.27847350
Iteration 97, loss = 0.27831294
Iteration 98, loss = 0.27778531
Iteration 99, loss = 0.27764751
Iteration 100, loss = 0.27794091
Iteration 101, loss = 0.27702027
Iteration 102, loss = 0.27705211
Iteration 103, loss = 0.27780355
Iteration 104, loss = 0.27669905
Iteration 105, loss = 0.27668041
Iteration 106, loss = 0.27701731
Iteration 107, loss = 0.27620874
Iteration 108, loss = 0.27681751
Iteration 109, loss = 0.27540736
Iteration 110, loss = 0.27627059
Iteration 111, loss = 0.27562476
Iteration 112, loss = 0.27573685
Iteration 113, loss = 0.27517146
Iteration 114, loss = 0.27572664
Iteration 115, loss = 0.27588026
Iteration 116, loss = 0.27534634
Iteration 117, loss = 0.27474443
Iteration 118, loss = 0.27467243
Iteration 119, loss = 0.27452514
Iteration 120, loss = 0.27426113
Iteration 121, loss = 0.27422513
Iteration 122, loss = 0.27344597
Iteration 123, loss = 0.27357561
Iteration 124, loss = 0.27323338
Iteration 125, loss = 0.27341243
Iteration 126, loss = 0.27391169
Iteration 127, loss = 0.27280399
Iteration 128, loss = 0.27326183
Iteration 129, loss = 0.27328129
Iteration 130, loss = 0.27249185
Iteration 131, loss = 0.27281728
Iteration 132, loss = 0.27243025
Iteration 133, loss = 0.27226071
Iteration 134, loss = 0.27233343
Iteration 135, loss = 0.27206164
Iteration 136, loss = 0.27180677
Iteration 137, loss = 0.27217001
Iteration 138, loss = 0.27150238
Iteration 139, loss = 0.27138608
Iteration 140, loss = 0.27162058
Iteration 141, loss = 0.27117288
Iteration 142, loss = 0.27083335
Iteration 143, loss = 0.27068024
Iteration 144, loss = 0.27055356
Iteration 145, loss = 0.27056508
Iteration 146, loss = 0.27071005
Iteration 147, loss = 0.26985441
Iteration 148, loss = 0.27062054
Iteration 149, loss = 0.27091784
Iteration 150, loss = 0.27030002
Iteration 151, loss = 0.27059649
Iteration 152, loss = 0.27026243
Iteration 153, loss = 0.26947581
Iteration 154, loss = 0.26999704
Iteration 155, loss = 0.26974320
Iteration 156, loss = 0.27037537
Iteration 157, loss = 0.26948025
Iteration 158, loss = 0.26937350
Iteration 159, loss = 0.26902968
Iteration 160, loss = 0.26942353
Iteration 161, loss = 0.26927011
Iteration 162, loss = 0.26912310
Iteration 163, loss = 0.26904815
Iteration 164, loss = 0.26816024
Iteration 165, loss = 0.26920651
Iteration 166, loss = 0.26856597
Iteration 167, loss = 0.26866825
Iteration 168, loss = 0.26917947
Iteration 169, loss = 0.26875819
Iteration 170, loss = 0.26821317
Iteration 171, loss = 0.26878146
Iteration 172, loss = 0.26828483
Iteration 173, loss = 0.26767941
Iteration 174, loss = 0.26834778
Iteration 175, loss = 0.26881612
Iteration 176, loss = 0.26767474
Iteration 177, loss = 0.26806960
Iteration 178, loss = 0.26735534
Iteration 179, loss = 0.26777863
Iteration 180, loss = 0.26743434
Iteration 181, loss = 0.26770464
Iteration 182, loss = 0.26742759
Iteration 183, loss = 0.26681279
Iteration 184, loss = 0.26726629
Iteration 185, loss = 0.26764669
Iteration 186, loss = 0.26725444
Iteration 187, loss = 0.26744465
Iteration 188, loss = 0.26641027
Iteration 189, loss = 0.26684429
Iteration 190, loss = 0.26733065
Iteration 191, loss = 0.26634884
Iteration 192, loss = 0.26681745
Iteration 193, loss = 0.26719252
Iteration 194, loss = 0.26657422
Iteration 195, loss = 0.26601326
Iteration 196, loss = 0.26658329
Iteration 197, loss = 0.26598130
Iteration 198, loss = 0.26596987
Iteration 199, loss = 0.26578825
Iteration 200, loss = 0.26623664
Iteration 201, loss = 0.26573358
Iteration 202, loss = 0.26560814
Iteration 203, loss = 0.26633492
Iteration 204, loss = 0.26567367
Iteration 205, loss = 0.26584322
Iteration 206, loss = 0.26593644
Iteration 207, loss = 0.26581709
Iteration 208, loss = 0.26556696
Iteration 209, loss = 0.26529067
Iteration 210, loss = 0.26575309
Iteration 211, loss = 0.26484964
Iteration 212, loss = 0.26528066
Iteration 213, loss = 0.26602814
Iteration 214, loss = 0.26493638
Iteration 215, loss = 0.26521268
Iteration 216, loss = 0.26480887
Iteration 217, loss = 0.26486271
Iteration 218, loss = 0.26555702
Iteration 219, loss = 0.26552505
Iteration 220, loss = 0.26486633
Iteration 221, loss = 0.26510228
Iteration 222, loss = 0.26431202
Iteration 223, loss = 0.26504013
Iteration 224, loss = 0.26421065
Iteration 225, loss = 0.26424302
Iteration 226, loss = 0.26547167
Iteration 227, loss = 0.26420597
Iteration 228, loss = 0.26467965
Iteration 229, loss = 0.26397644
Iteration 230, loss = 0.26426615
Iteration 231, loss = 0.26408818
Iteration 232, loss = 0.26455281
Iteration 233, loss = 0.26408554
Iteration 234, loss = 0.26350413
Iteration 235, loss = 0.26412378
Iteration 236, loss = 0.26342153
Iteration 237, loss = 0.26443172
Iteration 238, loss = 0.26367112
Iteration 239, loss = 0.26388853
Iteration 240, loss = 0.26332496
Iteration 241, loss = 0.26291470
Iteration 242, loss = 0.26381382
Iteration 243, loss = 0.26385518
Iteration 244, loss = 0.26427198
Iteration 245, loss = 0.26445158
Iteration 246, loss = 0.26436469
Iteration 247, loss = 0.26393063
Iteration 248, loss = 0.26295457
Iteration 249, loss = 0.26328515
Iteration 250, loss = 0.26330313
Iteration 251, loss = 0.26403407
Iteration 252, loss = 0.26322230
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--gmm_augment-iter-1.pkl
"ds4 (gmm_augment)",0.893,0.862,0.847,0.834,0.859,0.844,0.790,0.777,0.766,0.794,176.12755227088928
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.pkl
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.traintime
"ds1 (Unmodified)",0.815,0.834,0.791,0.778,0.804,0.798,0.814,0.773,0.761,0.787,703.2312424182892
** -> kmeans fit for replace transformer for dataset ds1 elapsed seconds 6.542476177215576 <- took 6.0 seconds
Iteration 1, loss = 0.55515886
Iteration 2, loss = 0.54725593
Iteration 3, loss = 0.54670953
Iteration 4, loss = 0.54647613
Iteration 5, loss = 0.54657376
Iteration 6, loss = 0.54619254
Iteration 7, loss = 0.54609493
Iteration 8, loss = 0.54637591
Iteration 9, loss = 0.54589209
Iteration 10, loss = 0.54620381
Iteration 11, loss = 0.54614356
Iteration 12, loss = 0.54613631
Iteration 13, loss = 0.54601267
Iteration 14, loss = 0.54623095
Iteration 15, loss = 0.54589536
Iteration 16, loss = 0.54599184
Iteration 17, loss = 0.54585080
Iteration 18, loss = 0.54583745
Iteration 19, loss = 0.54567627
Iteration 20, loss = 0.54593684
Iteration 21, loss = 0.54593015
Iteration 22, loss = 0.54590368
Iteration 23, loss = 0.54571664
Iteration 24, loss = 0.54563317
Iteration 25, loss = 0.54571778
Iteration 26, loss = 0.54594133
Iteration 27, loss = 0.54589840
Iteration 28, loss = 0.54564298
Iteration 29, loss = 0.54579799
Iteration 30, loss = 0.54547523
Iteration 31, loss = 0.54574046
Iteration 32, loss = 0.54556555
Iteration 33, loss = 0.54563435
Iteration 34, loss = 0.54552952
Iteration 35, loss = 0.54563778
Iteration 36, loss = 0.54557063
Iteration 37, loss = 0.54579044
Iteration 38, loss = 0.54559024
Iteration 39, loss = 0.54558013
Iteration 40, loss = 0.54556511
Iteration 41, loss = 0.54539216
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--kmeans_replace-iter-1.pkl
"ds1 (kmeans_replace)",0.712,0.698,0.684,0.680,0.694,0.711,0.698,0.683,0.679,0.693,148.82971048355103
** -> kmeans fit for augment transformer for dataset ds1 elapsed seconds 6.523278474807739 <- took 6.0 seconds
Iteration 1, loss = 0.49088392
Iteration 2, loss = 0.46637712
Iteration 3, loss = 0.45244088
Iteration 4, loss = 0.44941663
Iteration 5, loss = 0.44463146
Iteration 6, loss = 0.44130675
Iteration 7, loss = 0.44050810
Iteration 8, loss = 0.43885389
Iteration 9, loss = 0.43820290
Iteration 10, loss = 0.43786272
Iteration 11, loss = 0.43623663
Iteration 12, loss = 0.43469321
Iteration 13, loss = 0.43483192
Iteration 14, loss = 0.43288402
Iteration 15, loss = 0.43166673
Iteration 16, loss = 0.43233990
Iteration 17, loss = 0.43247184
Iteration 18, loss = 0.43226461
Iteration 19, loss = 0.43081598
Iteration 20, loss = 0.43066450
Iteration 21, loss = 0.43043916
Iteration 22, loss = 0.43020684
Iteration 23, loss = 0.42964615
Iteration 24, loss = 0.42779079
Iteration 25, loss = 0.42848018
Iteration 26, loss = 0.42750222
Iteration 27, loss = 0.42787957
Iteration 28, loss = 0.42628222
Iteration 29, loss = 0.42755746
Iteration 30, loss = 0.42692377
Iteration 31, loss = 0.42536749
Iteration 32, loss = 0.42553674
Iteration 33, loss = 0.42596679
Iteration 34, loss = 0.42554850
Iteration 35, loss = 0.42521317
Iteration 36, loss = 0.42405823
Iteration 37, loss = 0.42517548
Iteration 38, loss = 0.42480419
Iteration 39, loss = 0.42438325
Iteration 40, loss = 0.42326196
Iteration 41, loss = 0.42349359
Iteration 42, loss = 0.42274238
Iteration 43, loss = 0.42237195
Iteration 44, loss = 0.42186177
Iteration 45, loss = 0.42144018
Iteration 46, loss = 0.42133503
Iteration 47, loss = 0.42050721
Iteration 48, loss = 0.42075640
Iteration 49, loss = 0.42033196
Iteration 50, loss = 0.42154418
Iteration 51, loss = 0.42063587
Iteration 52, loss = 0.42121644
Iteration 53, loss = 0.42035499
Iteration 54, loss = 0.41965476
Iteration 55, loss = 0.41929236
Iteration 56, loss = 0.41901518
Iteration 57, loss = 0.41926087
Iteration 58, loss = 0.41881873
Iteration 59, loss = 0.41879489
Iteration 60, loss = 0.41818778
Iteration 61, loss = 0.41756966
Iteration 62, loss = 0.41827274
Iteration 63, loss = 0.41759895
Iteration 64, loss = 0.41736255
Iteration 65, loss = 0.41806547
Iteration 66, loss = 0.41745431
Iteration 67, loss = 0.41679781
Iteration 68, loss = 0.41646542
Iteration 69, loss = 0.41532053
Iteration 70, loss = 0.41685221
Iteration 71, loss = 0.41490873
Iteration 72, loss = 0.41591542
Iteration 73, loss = 0.41619929
Iteration 74, loss = 0.41580349
Iteration 75, loss = 0.41595393
Iteration 76, loss = 0.41529998
Iteration 77, loss = 0.41434298
Iteration 78, loss = 0.41485911
Iteration 79, loss = 0.41431460
Iteration 80, loss = 0.41471443
Iteration 81, loss = 0.41481094
Iteration 82, loss = 0.41482207
Iteration 83, loss = 0.41374825
Iteration 84, loss = 0.41290660
Iteration 85, loss = 0.41445767
Iteration 86, loss = 0.41205459
Iteration 87, loss = 0.41386347
Iteration 88, loss = 0.41228430
Iteration 89, loss = 0.41216103
Iteration 90, loss = 0.41228493
Iteration 91, loss = 0.41161283
Iteration 92, loss = 0.41133434
Iteration 93, loss = 0.41193167
Iteration 94, loss = 0.41150665
Iteration 95, loss = 0.41108067
Iteration 96, loss = 0.41049357
Iteration 97, loss = 0.40994034
Iteration 98, loss = 0.41031734
Iteration 99, loss = 0.41004902
Iteration 100, loss = 0.40974547
Iteration 101, loss = 0.41026555
Iteration 102, loss = 0.41046315
Iteration 103, loss = 0.40944700
Iteration 104, loss = 0.40978710
Iteration 105, loss = 0.40898667
Iteration 106, loss = 0.40939014
Iteration 107, loss = 0.40999932
Iteration 108, loss = 0.40883365
Iteration 109, loss = 0.40820188
Iteration 110, loss = 0.40992804
Iteration 111, loss = 0.40766456
Iteration 112, loss = 0.40886001
Iteration 113, loss = 0.40791093
Iteration 114, loss = 0.40836104
Iteration 115, loss = 0.40767944
Iteration 116, loss = 0.40772526
Iteration 117, loss = 0.40748383
Iteration 118, loss = 0.40753918
Iteration 119, loss = 0.40777132
Iteration 120, loss = 0.40778300
Iteration 121, loss = 0.40782703
Iteration 122, loss = 0.40680391
Iteration 123, loss = 0.40655514
Iteration 124, loss = 0.40613362
Iteration 125, loss = 0.40658953
Iteration 126, loss = 0.40577260
Iteration 127, loss = 0.40514623
Iteration 128, loss = 0.40518370
Iteration 129, loss = 0.40576906
Iteration 130, loss = 0.40622248
Iteration 131, loss = 0.40581130
Iteration 132, loss = 0.40498961
Iteration 133, loss = 0.40487612
Iteration 134, loss = 0.40554785
Iteration 135, loss = 0.40505072
Iteration 136, loss = 0.40459424
Iteration 137, loss = 0.40387383
Iteration 138, loss = 0.40372239
Iteration 139, loss = 0.40457186
Iteration 140, loss = 0.40585443
Iteration 141, loss = 0.40356998
Iteration 142, loss = 0.40468878
Iteration 143, loss = 0.40360521
Iteration 144, loss = 0.40371155
Iteration 145, loss = 0.40331956
Iteration 146, loss = 0.40278610
Iteration 147, loss = 0.40261229
Iteration 148, loss = 0.40399607
Iteration 149, loss = 0.40297333
Iteration 150, loss = 0.40310180
Iteration 151, loss = 0.40394646
Iteration 152, loss = 0.40311987
Iteration 153, loss = 0.40190980
Iteration 154, loss = 0.40338094
Iteration 155, loss = 0.40246099
Iteration 156, loss = 0.40328748
Iteration 157, loss = 0.40162275
Iteration 158, loss = 0.40288200
Iteration 159, loss = 0.40148220
Iteration 160, loss = 0.40186043
Iteration 161, loss = 0.40192642
Iteration 162, loss = 0.40115007
Iteration 163, loss = 0.40148850
Iteration 164, loss = 0.40278573
Iteration 165, loss = 0.40157357
Iteration 166, loss = 0.40060833
Iteration 167, loss = 0.40125184
Iteration 168, loss = 0.40030429
Iteration 169, loss = 0.40085102
Iteration 170, loss = 0.40067586
Iteration 171, loss = 0.40102347
Iteration 172, loss = 0.40046008
Iteration 173, loss = 0.39925827
Iteration 174, loss = 0.40009466
Iteration 175, loss = 0.40062358
Iteration 176, loss = 0.39955153
Iteration 177, loss = 0.40028596
Iteration 178, loss = 0.40066321
Iteration 179, loss = 0.39982088
Iteration 180, loss = 0.39957072
Iteration 181, loss = 0.39873512
Iteration 182, loss = 0.39865556
Iteration 183, loss = 0.39893022
Iteration 184, loss = 0.39889876
Iteration 185, loss = 0.39914516
Iteration 186, loss = 0.39900460
Iteration 187, loss = 0.39917280
Iteration 188, loss = 0.39884194
Iteration 189, loss = 0.39867827
Iteration 190, loss = 0.39873508
Iteration 191, loss = 0.39829178
Iteration 192, loss = 0.39862118
Iteration 193, loss = 0.40003395
Iteration 194, loss = 0.39922566
Iteration 195, loss = 0.39718186
Iteration 196, loss = 0.39892434
Iteration 197, loss = 0.39771249
Iteration 198, loss = 0.39706318
Iteration 199, loss = 0.39810931
Iteration 200, loss = 0.39781161
Iteration 201, loss = 0.39765243
Iteration 202, loss = 0.39784015
Iteration 203, loss = 0.39729644
Iteration 204, loss = 0.39867657
Iteration 205, loss = 0.39784641
Iteration 206, loss = 0.39799883
Iteration 207, loss = 0.39659192
Iteration 208, loss = 0.39767075
Iteration 209, loss = 0.39705291
Iteration 210, loss = 0.39720363
Iteration 211, loss = 0.39729472
Iteration 212, loss = 0.39652672
Iteration 213, loss = 0.39691600
Iteration 214, loss = 0.39651579
Iteration 215, loss = 0.39566371
Iteration 216, loss = 0.39687905
Iteration 217, loss = 0.39702176
Iteration 218, loss = 0.39588775
Iteration 219, loss = 0.39585821
Iteration 220, loss = 0.39599316
Iteration 221, loss = 0.39499906
Iteration 222, loss = 0.39649691
Iteration 223, loss = 0.39708892
Iteration 224, loss = 0.39634950
Iteration 225, loss = 0.39539704
Iteration 226, loss = 0.39546590
Iteration 227, loss = 0.39494239
Iteration 228, loss = 0.39539517
Iteration 229, loss = 0.39486918
Iteration 230, loss = 0.39528830
Iteration 231, loss = 0.39446367
Iteration 232, loss = 0.39583199
Iteration 233, loss = 0.39482440
Iteration 234, loss = 0.39561023
Iteration 235, loss = 0.39448965
Iteration 236, loss = 0.39684237
Iteration 237, loss = 0.39416985
Iteration 238, loss = 0.39480630
Iteration 239, loss = 0.39308333
Iteration 240, loss = 0.39364540
Iteration 241, loss = 0.39390105
Iteration 242, loss = 0.39417326
Iteration 243, loss = 0.39355089
Iteration 244, loss = 0.39351772
Iteration 245, loss = 0.39392172
Iteration 246, loss = 0.39430553
Iteration 247, loss = 0.39367945
Iteration 248, loss = 0.39252908
Iteration 249, loss = 0.39305212
Iteration 250, loss = 0.39425502
Iteration 251, loss = 0.39266480
Iteration 252, loss = 0.39282129
Iteration 253, loss = 0.39264108
Iteration 254, loss = 0.39329120
Iteration 255, loss = 0.39446581
Iteration 256, loss = 0.39293769
Iteration 257, loss = 0.39373815
Iteration 258, loss = 0.39249234
Iteration 259, loss = 0.39227629
Iteration 260, loss = 0.39283317
Iteration 261, loss = 0.39252332
Iteration 262, loss = 0.39233559
Iteration 263, loss = 0.39209538
Iteration 264, loss = 0.39195872
Iteration 265, loss = 0.39186866
Iteration 266, loss = 0.39227272
Iteration 267, loss = 0.39156267
Iteration 268, loss = 0.39277805
Iteration 269, loss = 0.39153019
Iteration 270, loss = 0.39278828
Iteration 271, loss = 0.39183745
Iteration 272, loss = 0.39032347
Iteration 273, loss = 0.39271570
Iteration 274, loss = 0.39208730
Iteration 275, loss = 0.39097861
Iteration 276, loss = 0.39177710
Iteration 277, loss = 0.39152738
Iteration 278, loss = 0.39093650
Iteration 279, loss = 0.39086795
Iteration 280, loss = 0.39158334
Iteration 281, loss = 0.38952752
Iteration 282, loss = 0.39084310
Iteration 283, loss = 0.39105044
Iteration 284, loss = 0.39031490
Iteration 285, loss = 0.39171471
Iteration 286, loss = 0.39069029
Iteration 287, loss = 0.39091054
Iteration 288, loss = 0.39024902
Iteration 289, loss = 0.39100625
Iteration 290, loss = 0.39268328
Iteration 291, loss = 0.39190495
Iteration 292, loss = 0.39134889
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--kmeans_augment-iter-1.pkl
"ds1 (kmeans_augment)",0.817,0.843,0.792,0.779,0.808,0.794,0.811,0.766,0.755,0.781,1071.3382034301758
** -> gmm fit for replace transformer for dataset ds1 elapsed seconds 5.645383596420288 <- took 5.0 seconds
Iteration 1, loss = 0.56094991
Iteration 2, loss = 0.55306446
Iteration 3, loss = 0.55258121
Iteration 4, loss = 0.55238367
Iteration 5, loss = 0.55242294
Iteration 6, loss = 0.55208567
Iteration 7, loss = 0.55198023
Iteration 8, loss = 0.55223245
Iteration 9, loss = 0.55176995
Iteration 10, loss = 0.55203748
Iteration 11, loss = 0.55202954
Iteration 12, loss = 0.55188318
Iteration 13, loss = 0.55187895
Iteration 14, loss = 0.55202953
Iteration 15, loss = 0.55175058
Iteration 16, loss = 0.55185123
Iteration 17, loss = 0.55163680
Iteration 18, loss = 0.55172268
Iteration 19, loss = 0.55152716
Iteration 20, loss = 0.55173674
Iteration 21, loss = 0.55184890
Iteration 22, loss = 0.55169339
Iteration 23, loss = 0.55161317
Iteration 24, loss = 0.55143606
Iteration 25, loss = 0.55160137
Iteration 26, loss = 0.55174216
Iteration 27, loss = 0.55168907
Iteration 28, loss = 0.55152769
Iteration 29, loss = 0.55170175
Iteration 30, loss = 0.55136165
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--gmm_replace-iter-1.pkl
"ds1 (gmm_replace)",0.710,0.696,0.682,0.678,0.692,0.710,0.696,0.682,0.677,0.691,110.46521997451782
** -> gmm fit for augment transformer for dataset ds1 elapsed seconds 5.650914192199707 <- took 5.0 seconds
Iteration 1, loss = 0.49121871
Iteration 2, loss = 0.46781585
Iteration 3, loss = 0.45354100
Iteration 4, loss = 0.44919691
Iteration 5, loss = 0.44527877
Iteration 6, loss = 0.44180031
Iteration 7, loss = 0.44084560
Iteration 8, loss = 0.43937115
Iteration 9, loss = 0.43834419
Iteration 10, loss = 0.43780393
Iteration 11, loss = 0.43656546
Iteration 12, loss = 0.43511664
Iteration 13, loss = 0.43518298
Iteration 14, loss = 0.43330760
Iteration 15, loss = 0.43246476
Iteration 16, loss = 0.43306491
Iteration 17, loss = 0.43255905
Iteration 18, loss = 0.43202701
Iteration 19, loss = 0.43079360
Iteration 20, loss = 0.43076215
Iteration 21, loss = 0.42985148
Iteration 22, loss = 0.42964733
Iteration 23, loss = 0.42952700
Iteration 24, loss = 0.42754335
Iteration 25, loss = 0.42828250
Iteration 26, loss = 0.42700889
Iteration 27, loss = 0.42760246
Iteration 28, loss = 0.42593331
Iteration 29, loss = 0.42731697
Iteration 30, loss = 0.42655563
Iteration 31, loss = 0.42514220
Iteration 32, loss = 0.42531802
Iteration 33, loss = 0.42544110
Iteration 34, loss = 0.42530485
Iteration 35, loss = 0.42452833
Iteration 36, loss = 0.42362436
Iteration 37, loss = 0.42427512
Iteration 38, loss = 0.42398461
Iteration 39, loss = 0.42345915
Iteration 40, loss = 0.42295327
Iteration 41, loss = 0.42340866
Iteration 42, loss = 0.42270893
Iteration 43, loss = 0.42217100
Iteration 44, loss = 0.42178464
Iteration 45, loss = 0.42119515
Iteration 46, loss = 0.42119455
Iteration 47, loss = 0.42040554
Iteration 48, loss = 0.42013726
Iteration 49, loss = 0.42040647
Iteration 50, loss = 0.42126198
Iteration 51, loss = 0.42046503
Iteration 52, loss = 0.42113039
Iteration 53, loss = 0.42011531
Iteration 54, loss = 0.41969822
Iteration 55, loss = 0.41950970
Iteration 56, loss = 0.41891504
Iteration 57, loss = 0.41918790
Iteration 58, loss = 0.41864229
Iteration 59, loss = 0.41871273
Iteration 60, loss = 0.41798012
Iteration 61, loss = 0.41769624
Iteration 62, loss = 0.41834422
Iteration 63, loss = 0.41755837
Iteration 64, loss = 0.41713172
Iteration 65, loss = 0.41826789
Iteration 66, loss = 0.41750142
Iteration 67, loss = 0.41705844
Iteration 68, loss = 0.41649170
Iteration 69, loss = 0.41565698
Iteration 70, loss = 0.41668979
Iteration 71, loss = 0.41533077
Iteration 72, loss = 0.41605408
Iteration 73, loss = 0.41606741
Iteration 74, loss = 0.41595575
Iteration 75, loss = 0.41577503
Iteration 76, loss = 0.41524160
Iteration 77, loss = 0.41434109
Iteration 78, loss = 0.41492788
Iteration 79, loss = 0.41457031
Iteration 80, loss = 0.41496887
Iteration 81, loss = 0.41505486
Iteration 82, loss = 0.41477659
Iteration 83, loss = 0.41415634
Iteration 84, loss = 0.41349791
Iteration 85, loss = 0.41462518
Iteration 86, loss = 0.41277415
Iteration 87, loss = 0.41373681
Iteration 88, loss = 0.41280174
Iteration 89, loss = 0.41251295
Iteration 90, loss = 0.41225798
Iteration 91, loss = 0.41237112
Iteration 92, loss = 0.41203176
Iteration 93, loss = 0.41182670
Iteration 94, loss = 0.41153176
Iteration 95, loss = 0.41119863
Iteration 96, loss = 0.41088710
Iteration 97, loss = 0.41078821
Iteration 98, loss = 0.41036187
Iteration 99, loss = 0.41074774
Iteration 100, loss = 0.41027970
Iteration 101, loss = 0.41081087
Iteration 102, loss = 0.41043358
Iteration 103, loss = 0.41066663
Iteration 104, loss = 0.41047203
Iteration 105, loss = 0.40993259
Iteration 106, loss = 0.41057376
Iteration 107, loss = 0.41064772
Iteration 108, loss = 0.40879413
Iteration 109, loss = 0.40820335
Iteration 110, loss = 0.41001423
Iteration 111, loss = 0.40803484
Iteration 112, loss = 0.40925388
Iteration 113, loss = 0.40891642
Iteration 114, loss = 0.40814758
Iteration 115, loss = 0.40822409
Iteration 116, loss = 0.40810378
Iteration 117, loss = 0.40812094
Iteration 118, loss = 0.40832556
Iteration 119, loss = 0.40885469
Iteration 120, loss = 0.40845413
Iteration 121, loss = 0.40858987
Iteration 122, loss = 0.40723795
Iteration 123, loss = 0.40764850
Iteration 124, loss = 0.40646250
Iteration 125, loss = 0.40787393
Iteration 126, loss = 0.40655783
Iteration 127, loss = 0.40595884
Iteration 128, loss = 0.40567729
Iteration 129, loss = 0.40566318
Iteration 130, loss = 0.40587943
Iteration 131, loss = 0.40677573
Iteration 132, loss = 0.40564169
Iteration 133, loss = 0.40509442
Iteration 134, loss = 0.40569085
Iteration 135, loss = 0.40527328
Iteration 136, loss = 0.40602389
Iteration 137, loss = 0.40473550
Iteration 138, loss = 0.40500513
Iteration 139, loss = 0.40483712
Iteration 140, loss = 0.40550976
Iteration 141, loss = 0.40455144
Iteration 142, loss = 0.40454488
Iteration 143, loss = 0.40403970
Iteration 144, loss = 0.40452250
Iteration 145, loss = 0.40453886
Iteration 146, loss = 0.40304486
Iteration 147, loss = 0.40298369
Iteration 148, loss = 0.40460618
Iteration 149, loss = 0.40297529
Iteration 150, loss = 0.40402589
Iteration 151, loss = 0.40387479
Iteration 152, loss = 0.40372359
Iteration 153, loss = 0.40277302
Iteration 154, loss = 0.40231607
Iteration 155, loss = 0.40312036
Iteration 156, loss = 0.40271634
Iteration 157, loss = 0.40246494
Iteration 158, loss = 0.40311780
Iteration 159, loss = 0.40267286
Iteration 160, loss = 0.40199480
Iteration 161, loss = 0.40336995
Iteration 162, loss = 0.40174181
Iteration 163, loss = 0.40223109
Iteration 164, loss = 0.40209719
Iteration 165, loss = 0.40093084
Iteration 166, loss = 0.40114264
Iteration 167, loss = 0.40179465
Iteration 168, loss = 0.40073807
Iteration 169, loss = 0.40094949
Iteration 170, loss = 0.40053382
Iteration 171, loss = 0.40081073
Iteration 172, loss = 0.40215227
Iteration 173, loss = 0.40170439
Iteration 174, loss = 0.40224647
Iteration 175, loss = 0.40125920
Iteration 176, loss = 0.39990104
Iteration 177, loss = 0.39988821
Iteration 178, loss = 0.40009955
Iteration 179, loss = 0.39989733
Iteration 180, loss = 0.39997358
Iteration 181, loss = 0.39941775
Iteration 182, loss = 0.39869281
Iteration 183, loss = 0.39981926
Iteration 184, loss = 0.39999153
Iteration 185, loss = 0.39890675
Iteration 186, loss = 0.39916802
Iteration 187, loss = 0.39965450
Iteration 188, loss = 0.39929599
Iteration 189, loss = 0.39839361
Iteration 190, loss = 0.39886739
Iteration 191, loss = 0.39908284
Iteration 192, loss = 0.39916853
Iteration 193, loss = 0.39926024
Iteration 194, loss = 0.39828733
Iteration 195, loss = 0.39814892
Iteration 196, loss = 0.39873364
Iteration 197, loss = 0.39826854
Iteration 198, loss = 0.39794442
Iteration 199, loss = 0.39798459
Iteration 200, loss = 0.39705132
Iteration 201, loss = 0.39901277
Iteration 202, loss = 0.39817535
Iteration 203, loss = 0.39743819
Iteration 204, loss = 0.39833069
Iteration 205, loss = 0.39726822
Iteration 206, loss = 0.39681340
Iteration 207, loss = 0.39776307
Iteration 208, loss = 0.39742887
Iteration 209, loss = 0.39655377
Iteration 210, loss = 0.39729138
Iteration 211, loss = 0.39628587
Iteration 212, loss = 0.39766944
Iteration 213, loss = 0.39715430
Iteration 214, loss = 0.39676528
Iteration 215, loss = 0.39714946
Iteration 216, loss = 0.39589940
Iteration 217, loss = 0.39691630
Iteration 218, loss = 0.39671330
Iteration 219, loss = 0.39668965
Iteration 220, loss = 0.39740190
Iteration 221, loss = 0.39503629
Iteration 222, loss = 0.39593283
Iteration 223, loss = 0.39616269
Iteration 224, loss = 0.39611667
Iteration 225, loss = 0.39601013
Iteration 226, loss = 0.39699677
Iteration 227, loss = 0.39467979
Iteration 228, loss = 0.39521688
Iteration 229, loss = 0.39576696
Iteration 230, loss = 0.39433614
Iteration 231, loss = 0.39508545
Iteration 232, loss = 0.39559534
Iteration 233, loss = 0.39437413
Iteration 234, loss = 0.39493624
Iteration 235, loss = 0.39446756
Iteration 236, loss = 0.39547970
Iteration 237, loss = 0.39495116
Iteration 238, loss = 0.39581316
Iteration 239, loss = 0.39398070
Iteration 240, loss = 0.39364236
Iteration 241, loss = 0.39481964
Iteration 242, loss = 0.39381052
Iteration 243, loss = 0.39346145
Iteration 244, loss = 0.39425525
Iteration 245, loss = 0.39274041
Iteration 246, loss = 0.39441276
Iteration 247, loss = 0.39495183
Iteration 248, loss = 0.39274904
Iteration 249, loss = 0.39379428
Iteration 250, loss = 0.39318294
Iteration 251, loss = 0.39338592
Iteration 252, loss = 0.39369558
Iteration 253, loss = 0.39310483
Iteration 254, loss = 0.39329765
Iteration 255, loss = 0.39472294
Iteration 256, loss = 0.39342684
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--gmm_augment-iter-1.pkl
"ds1 (gmm_augment)",0.815,0.824,0.795,0.784,0.804,0.798,0.805,0.776,0.766,0.786,916.715488910675
Write part5_report.csv
