"Dataset","Xval Accuracy","Xval Precision","Xval F1 Score","Xval Recall","Xval Aggregate","Test Set Accuracy","Test Set Precision","Test Set F1 Score","Test Set Recall","Test Set Aggregate","Train Time"
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.pkl
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.traintime
"ds4 (Unmodified)",0.885,0.871,0.827,0.799,0.846,0.845,0.803,0.765,0.743,0.789,154.79517817497253
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-30.pkl
Iteration 1, loss = 0.52721885
Iteration 2, loss = 0.43816442
Iteration 3, loss = 0.41326021
Iteration 4, loss = 0.39687683
Iteration 5, loss = 0.38534292
Iteration 6, loss = 0.37626501
Iteration 7, loss = 0.36938238
Iteration 8, loss = 0.36351645
Iteration 9, loss = 0.35884815
Iteration 10, loss = 0.35523206
Iteration 11, loss = 0.35216342
Iteration 12, loss = 0.34911500
Iteration 13, loss = 0.34698072
Iteration 14, loss = 0.34465197
Iteration 15, loss = 0.34316102
Iteration 16, loss = 0.34105160
Iteration 17, loss = 0.33979452
Iteration 18, loss = 0.33832988
Iteration 19, loss = 0.33683174
Iteration 20, loss = 0.33552757
Iteration 21, loss = 0.33419556
Iteration 22, loss = 0.33351626
Iteration 23, loss = 0.33249587
Iteration 24, loss = 0.33122236
Iteration 25, loss = 0.33009917
Iteration 26, loss = 0.32999986
Iteration 27, loss = 0.32910819
Iteration 28, loss = 0.32850083
Iteration 29, loss = 0.32783501
Iteration 30, loss = 0.32754520
Iteration 31, loss = 0.32683017
Iteration 32, loss = 0.32665067
Iteration 33, loss = 0.32556088
Iteration 34, loss = 0.32522611
Iteration 35, loss = 0.32438246
Iteration 36, loss = 0.32485486
Iteration 37, loss = 0.32409989
Iteration 38, loss = 0.32337248
Iteration 39, loss = 0.32286572
Iteration 40, loss = 0.32274215
Iteration 41, loss = 0.32239637
Iteration 42, loss = 0.32167948
Iteration 43, loss = 0.32131837
Iteration 44, loss = 0.32184441
Iteration 45, loss = 0.32070700
Iteration 46, loss = 0.32081990
Iteration 47, loss = 0.32046730
Iteration 48, loss = 0.31979466
Iteration 49, loss = 0.31925548
Iteration 50, loss = 0.31909189
Iteration 51, loss = 0.31887561
Iteration 52, loss = 0.31880710
Iteration 53, loss = 0.31871974
Iteration 54, loss = 0.31846778
Iteration 55, loss = 0.31812705
Iteration 56, loss = 0.31795224
Iteration 57, loss = 0.31750794
Iteration 58, loss = 0.31788960
Iteration 59, loss = 0.31685210
Iteration 60, loss = 0.31804258
Iteration 61, loss = 0.31666714
Iteration 62, loss = 0.31632087
Iteration 63, loss = 0.31639504
Iteration 64, loss = 0.31605015
Iteration 65, loss = 0.31572774
Iteration 66, loss = 0.31611639
Iteration 67, loss = 0.31633156
Iteration 68, loss = 0.31572302
Iteration 69, loss = 0.31476342
Iteration 70, loss = 0.31484296
Iteration 71, loss = 0.31520534
Iteration 72, loss = 0.31461359
Iteration 73, loss = 0.31531180
Iteration 74, loss = 0.31403008
Iteration 75, loss = 0.31376627
Iteration 76, loss = 0.31326772
Iteration 77, loss = 0.31401660
Iteration 78, loss = 0.31320113
Iteration 79, loss = 0.31359897
Iteration 80, loss = 0.31333339
Iteration 81, loss = 0.31323587
Iteration 82, loss = 0.31322168
Iteration 83, loss = 0.31260319
Iteration 84, loss = 0.31265837
Iteration 85, loss = 0.31265071
Iteration 86, loss = 0.31293517
Iteration 87, loss = 0.31203533
Iteration 88, loss = 0.31220152
Iteration 89, loss = 0.31218880
Iteration 90, loss = 0.31222759
Iteration 91, loss = 0.31146992
Iteration 92, loss = 0.31184401
Iteration 93, loss = 0.31121152
Iteration 94, loss = 0.31082210
Iteration 95, loss = 0.31123675
Iteration 96, loss = 0.31070033
Iteration 97, loss = 0.31075119
Iteration 98, loss = 0.31078837
Iteration 99, loss = 0.31079993
Iteration 100, loss = 0.31047057
Iteration 101, loss = 0.31073111
Iteration 102, loss = 0.30991741
Iteration 103, loss = 0.31067547
Iteration 104, loss = 0.31047062
Iteration 105, loss = 0.31058906
Iteration 106, loss = 0.30993109
Iteration 107, loss = 0.30982702
Iteration 108, loss = 0.30934541
Iteration 109, loss = 0.30944812
Iteration 110, loss = 0.30887005
Iteration 111, loss = 0.31020394
Iteration 112, loss = 0.30944482
Iteration 113, loss = 0.30956777
Iteration 114, loss = 0.30864696
Iteration 115, loss = 0.30947032
Iteration 116, loss = 0.30859702
Iteration 117, loss = 0.30924140
Iteration 118, loss = 0.30866187
Iteration 119, loss = 0.30800554
Iteration 120, loss = 0.30867743
Iteration 121, loss = 0.30808678
Iteration 122, loss = 0.30835876
Iteration 123, loss = 0.30830531
Iteration 124, loss = 0.30787382
Iteration 125, loss = 0.30781538
Iteration 126, loss = 0.30750663
Iteration 127, loss = 0.30771397
Iteration 128, loss = 0.30773279
Iteration 129, loss = 0.30696893
Iteration 130, loss = 0.30688400
Iteration 131, loss = 0.30762531
Iteration 132, loss = 0.30717479
Iteration 133, loss = 0.30694846
Iteration 134, loss = 0.30666710
Iteration 135, loss = 0.30695493
Iteration 136, loss = 0.30680670
Iteration 137, loss = 0.30668815
Iteration 138, loss = 0.30706488
Iteration 139, loss = 0.30672964
Iteration 140, loss = 0.30618405
Iteration 141, loss = 0.30625645
Iteration 142, loss = 0.30614037
Iteration 143, loss = 0.30560211
Iteration 144, loss = 0.30600577
Iteration 145, loss = 0.30611679
Iteration 146, loss = 0.30621537
Iteration 147, loss = 0.30580759
Iteration 148, loss = 0.30569703
Iteration 149, loss = 0.30567734
Iteration 150, loss = 0.30538542
Iteration 151, loss = 0.30505150
Iteration 152, loss = 0.30542454
Iteration 153, loss = 0.30489207
Iteration 154, loss = 0.30540491
Iteration 155, loss = 0.30566546
Iteration 156, loss = 0.30537494
Iteration 157, loss = 0.30599170
Iteration 158, loss = 0.30427691
Iteration 159, loss = 0.30531555
Iteration 160, loss = 0.30452352
Iteration 161, loss = 0.30475959
Iteration 162, loss = 0.30506549
Iteration 163, loss = 0.30406970
Iteration 164, loss = 0.30498472
Iteration 165, loss = 0.30433337
Iteration 166, loss = 0.30443744
Iteration 167, loss = 0.30435398
Iteration 168, loss = 0.30446830
Iteration 169, loss = 0.30363719
Iteration 170, loss = 0.30375858
Iteration 171, loss = 0.30349379
Iteration 172, loss = 0.30378959
Iteration 173, loss = 0.30397689
Iteration 174, loss = 0.30295122
Iteration 175, loss = 0.30368891
Iteration 176, loss = 0.30374639
Iteration 177, loss = 0.30359833
Iteration 178, loss = 0.30341768
Iteration 179, loss = 0.30386364
Iteration 180, loss = 0.30348709
Iteration 181, loss = 0.30336361
Iteration 182, loss = 0.30332485
Iteration 183, loss = 0.30297821
Iteration 184, loss = 0.30329255
Iteration 185, loss = 0.30311299
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_30d-iter-1.pkl
"ds4 (pca_30d)",0.870,0.829,0.815,0.803,0.829,0.849,0.797,0.784,0.773,0.801,126.17468476295471
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-55.pkl
Iteration 1, loss = 0.43516291
Iteration 2, loss = 0.37567558
Iteration 3, loss = 0.36333909
Iteration 4, loss = 0.35460009
Iteration 5, loss = 0.34705918
Iteration 6, loss = 0.34146296
Iteration 7, loss = 0.33672903
Iteration 8, loss = 0.33234691
Iteration 9, loss = 0.32916200
Iteration 10, loss = 0.32661552
Iteration 11, loss = 0.32405521
Iteration 12, loss = 0.32169498
Iteration 13, loss = 0.31960505
Iteration 14, loss = 0.31744935
Iteration 15, loss = 0.31629932
Iteration 16, loss = 0.31468866
Iteration 17, loss = 0.31427008
Iteration 18, loss = 0.31254802
Iteration 19, loss = 0.31101917
Iteration 20, loss = 0.31063655
Iteration 21, loss = 0.30888110
Iteration 22, loss = 0.30852425
Iteration 23, loss = 0.30740311
Iteration 24, loss = 0.30674678
Iteration 25, loss = 0.30583505
Iteration 26, loss = 0.30524563
Iteration 27, loss = 0.30452594
Iteration 28, loss = 0.30369637
Iteration 29, loss = 0.30302239
Iteration 30, loss = 0.30247798
Iteration 31, loss = 0.30186127
Iteration 32, loss = 0.30129328
Iteration 33, loss = 0.30084321
Iteration 34, loss = 0.30050560
Iteration 35, loss = 0.29935912
Iteration 36, loss = 0.29927478
Iteration 37, loss = 0.29846112
Iteration 38, loss = 0.29740092
Iteration 39, loss = 0.29791807
Iteration 40, loss = 0.29645468
Iteration 41, loss = 0.29677441
Iteration 42, loss = 0.29642593
Iteration 43, loss = 0.29563288
Iteration 44, loss = 0.29464292
