"Dataset","Xval Accuracy","Xval Precision","Xval F1 Score","Xval Recall","Xval Aggregate","Test Set Accuracy","Test Set Precision","Test Set F1 Score","Test Set Recall","Test Set Aggregate","Train Time"
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.pkl
Read datasets/ds4/mlp-model--50-20-1--Unmodified-iter-1.traintime
"ds4 (Unmodified)",0.885,0.871,0.827,0.799,0.846,0.845,0.803,0.765,0.743,0.789,154.79517817497253
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-30.pkl
Read datasets/ds4/mlp-model--50-20-1--pca_30d-iter-1.pkl
Read datasets/ds4/mlp-model--50-20-1--pca_30d-iter-1.traintime
"ds4 (pca_30d)",0.870,0.829,0.815,0.803,0.829,0.849,0.797,0.784,0.773,0.801,126.17468476295471
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-pca-55.pkl
Iteration 1, loss = 0.43516291
Iteration 2, loss = 0.37567558
Iteration 3, loss = 0.36333909
Iteration 4, loss = 0.35460009
Iteration 5, loss = 0.34705918
Iteration 6, loss = 0.34146296
Iteration 7, loss = 0.33672903
Iteration 8, loss = 0.33234691
Iteration 9, loss = 0.32916200
Iteration 10, loss = 0.32661552
Iteration 11, loss = 0.32405521
Iteration 12, loss = 0.32169498
Iteration 13, loss = 0.31960505
Iteration 14, loss = 0.31744935
Iteration 15, loss = 0.31629932
Iteration 16, loss = 0.31468866
Iteration 17, loss = 0.31427008
Iteration 18, loss = 0.31254802
Iteration 19, loss = 0.31101917
Iteration 20, loss = 0.31063655
Iteration 21, loss = 0.30888110
Iteration 22, loss = 0.30852425
Iteration 23, loss = 0.30740311
Iteration 24, loss = 0.30674678
Iteration 25, loss = 0.30583505
Iteration 26, loss = 0.30524563
Iteration 27, loss = 0.30452594
Iteration 28, loss = 0.30369637
Iteration 29, loss = 0.30302239
Iteration 30, loss = 0.30247798
Iteration 31, loss = 0.30186127
Iteration 32, loss = 0.30129328
Iteration 33, loss = 0.30084321
Iteration 34, loss = 0.30050560
Iteration 35, loss = 0.29935912
Iteration 36, loss = 0.29927478
Iteration 37, loss = 0.29846112
Iteration 38, loss = 0.29740092
Iteration 39, loss = 0.29791807
Iteration 40, loss = 0.29645468
Iteration 41, loss = 0.29677441
Iteration 42, loss = 0.29642593
Iteration 43, loss = 0.29563288
Iteration 44, loss = 0.29464292
Iteration 45, loss = 0.29524436
Iteration 46, loss = 0.29390488
Iteration 47, loss = 0.29435716
Iteration 48, loss = 0.29329767
Iteration 49, loss = 0.29284505
Iteration 50, loss = 0.29243589
Iteration 51, loss = 0.29209137
Iteration 52, loss = 0.29212697
Iteration 53, loss = 0.29193178
Iteration 54, loss = 0.29051318
Iteration 55, loss = 0.29056978
Iteration 56, loss = 0.29026803
Iteration 57, loss = 0.29070357
Iteration 58, loss = 0.28942799
Iteration 59, loss = 0.28892794
Iteration 60, loss = 0.28947865
Iteration 61, loss = 0.28844125
Iteration 62, loss = 0.28824421
Iteration 63, loss = 0.28811920
Iteration 64, loss = 0.28756966
Iteration 65, loss = 0.28732647
Iteration 66, loss = 0.28682466
Iteration 67, loss = 0.28724659
Iteration 68, loss = 0.28619035
Iteration 69, loss = 0.28601372
Iteration 70, loss = 0.28632128
Iteration 71, loss = 0.28575528
Iteration 72, loss = 0.28491686
Iteration 73, loss = 0.28507344
Iteration 74, loss = 0.28460305
Iteration 75, loss = 0.28365177
Iteration 76, loss = 0.28392169
Iteration 77, loss = 0.28350584
Iteration 78, loss = 0.28282434
Iteration 79, loss = 0.28307399
Iteration 80, loss = 0.28267298
Iteration 81, loss = 0.28268103
Iteration 82, loss = 0.28305509
Iteration 83, loss = 0.28186138
Iteration 84, loss = 0.28215151
Iteration 85, loss = 0.28151046
Iteration 86, loss = 0.28111844
Iteration 87, loss = 0.28075747
Iteration 88, loss = 0.28047538
Iteration 89, loss = 0.27978691
Iteration 90, loss = 0.28007953
Iteration 91, loss = 0.27975335
Iteration 92, loss = 0.27992303
Iteration 93, loss = 0.27981644
Iteration 94, loss = 0.27912512
Iteration 95, loss = 0.27877307
Iteration 96, loss = 0.27912546
Iteration 97, loss = 0.27834315
Iteration 98, loss = 0.27820988
Iteration 99, loss = 0.27912570
Iteration 100, loss = 0.27763392
Iteration 101, loss = 0.27794041
Iteration 102, loss = 0.27841508
Iteration 103, loss = 0.27757974
Iteration 104, loss = 0.27721038
Iteration 105, loss = 0.27734230
Iteration 106, loss = 0.27684123
Iteration 107, loss = 0.27663675
Iteration 108, loss = 0.27696837
Iteration 109, loss = 0.27619614
Iteration 110, loss = 0.27575949
Iteration 111, loss = 0.27631367
Iteration 112, loss = 0.27563923
Iteration 113, loss = 0.27569217
Iteration 114, loss = 0.27541570
Iteration 115, loss = 0.27531669
Iteration 116, loss = 0.27523640
Iteration 117, loss = 0.27493779
Iteration 118, loss = 0.27469139
Iteration 119, loss = 0.27438217
Iteration 120, loss = 0.27429638
Iteration 121, loss = 0.27396911
Iteration 122, loss = 0.27425347
Iteration 123, loss = 0.27398699
Iteration 124, loss = 0.27366515
Iteration 125, loss = 0.27336622
Iteration 126, loss = 0.27368636
Iteration 127, loss = 0.27337884
Iteration 128, loss = 0.27277511
Iteration 129, loss = 0.27276664
Iteration 130, loss = 0.27295983
Iteration 131, loss = 0.27163643
Iteration 132, loss = 0.27272240
Iteration 133, loss = 0.27248710
Iteration 134, loss = 0.27232405
Iteration 135, loss = 0.27236327
Iteration 136, loss = 0.27127834
Iteration 137, loss = 0.27220724
Iteration 138, loss = 0.27147114
Iteration 139, loss = 0.27069770
Iteration 140, loss = 0.27082684
Iteration 141, loss = 0.27145036
Iteration 142, loss = 0.27053826
Iteration 143, loss = 0.27114431
Iteration 144, loss = 0.27143325
Iteration 145, loss = 0.27084433
Iteration 146, loss = 0.27047702
Iteration 147, loss = 0.27005794
Iteration 148, loss = 0.26943454
Iteration 149, loss = 0.26978362
Iteration 150, loss = 0.26964057
Iteration 151, loss = 0.26944901
Iteration 152, loss = 0.27024695
Iteration 153, loss = 0.26995406
Iteration 154, loss = 0.26907205
Iteration 155, loss = 0.26915689
Iteration 156, loss = 0.26876551
Iteration 157, loss = 0.26987019
Iteration 158, loss = 0.26956809
Iteration 159, loss = 0.26895985
Iteration 160, loss = 0.26901764
Iteration 161, loss = 0.26865459
Iteration 162, loss = 0.26843599
Iteration 163, loss = 0.26799306
Iteration 164, loss = 0.26779579
Iteration 165, loss = 0.26803014
Iteration 166, loss = 0.26771023
Iteration 167, loss = 0.26843113
Iteration 168, loss = 0.26763602
Iteration 169, loss = 0.26810739
Iteration 170, loss = 0.26769997
Iteration 171, loss = 0.26771644
Iteration 172, loss = 0.26717248
Iteration 173, loss = 0.26751899
Iteration 174, loss = 0.26684218
Iteration 175, loss = 0.26751357
Iteration 176, loss = 0.26709618
Iteration 177, loss = 0.26666540
Iteration 178, loss = 0.26674574
Iteration 179, loss = 0.26596221
Iteration 180, loss = 0.26624437
Iteration 181, loss = 0.26640453
Iteration 182, loss = 0.26654267
Iteration 183, loss = 0.26626256
Iteration 184, loss = 0.26607509
Iteration 185, loss = 0.26587888
Iteration 186, loss = 0.26570316
Iteration 187, loss = 0.26484045
Iteration 188, loss = 0.26570583
Iteration 189, loss = 0.26553144
Iteration 190, loss = 0.26511596
Iteration 191, loss = 0.26496945
Iteration 192, loss = 0.26551457
Iteration 193, loss = 0.26550408
Iteration 194, loss = 0.26478766
Iteration 195, loss = 0.26556465
Iteration 196, loss = 0.26475331
Iteration 197, loss = 0.26455321
Iteration 198, loss = 0.26503598
Iteration 199, loss = 0.26457703
Iteration 200, loss = 0.26357610
Iteration 201, loss = 0.26446578
Iteration 202, loss = 0.26372968
Iteration 203, loss = 0.26474691
Iteration 204, loss = 0.26436070
Iteration 205, loss = 0.26447836
Iteration 206, loss = 0.26441950
Iteration 207, loss = 0.26333675
Iteration 208, loss = 0.26348796
Iteration 209, loss = 0.26390018
Iteration 210, loss = 0.26327354
Iteration 211, loss = 0.26353140
Iteration 212, loss = 0.26327746
Iteration 213, loss = 0.26393203
Iteration 214, loss = 0.26263390
Iteration 215, loss = 0.26367879
Iteration 216, loss = 0.26337890
Iteration 217, loss = 0.26362801
Iteration 218, loss = 0.26262788
Iteration 219, loss = 0.26292073
Iteration 220, loss = 0.26249806
Iteration 221, loss = 0.26331348
Iteration 222, loss = 0.26262575
Iteration 223, loss = 0.26336888
Iteration 224, loss = 0.26249197
Iteration 225, loss = 0.26286180
Iteration 226, loss = 0.26284656
Iteration 227, loss = 0.26272043
Iteration 228, loss = 0.26288397
Iteration 229, loss = 0.26219356
Iteration 230, loss = 0.26182525
Iteration 231, loss = 0.26220154
Iteration 232, loss = 0.26273902
Iteration 233, loss = 0.26184064
Iteration 234, loss = 0.26263671
Iteration 235, loss = 0.26242859
Iteration 236, loss = 0.26140673
Iteration 237, loss = 0.26197893
Iteration 238, loss = 0.26189518
Iteration 239, loss = 0.26121585
Iteration 240, loss = 0.26218191
Iteration 241, loss = 0.26145895
Iteration 242, loss = 0.26162041
Iteration 243, loss = 0.26137238
Iteration 244, loss = 0.26111215
Iteration 245, loss = 0.26104768
Iteration 246, loss = 0.26146147
Iteration 247, loss = 0.26237442
Iteration 248, loss = 0.26152494
Iteration 249, loss = 0.26095757
Iteration 250, loss = 0.26149137
Iteration 251, loss = 0.26131173
Iteration 252, loss = 0.26048882
Iteration 253, loss = 0.26135668
Iteration 254, loss = 0.26051963
Iteration 255, loss = 0.26083665
Iteration 256, loss = 0.26152797
Iteration 257, loss = 0.26070808
Iteration 258, loss = 0.26068244
Iteration 259, loss = 0.26118945
Iteration 260, loss = 0.26044383
Iteration 261, loss = 0.26060164
Iteration 262, loss = 0.26035046
Iteration 263, loss = 0.26054152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--pca_55d-iter-1.pkl
"ds4 (pca_55d)",0.892,0.849,0.853,0.857,0.863,0.838,0.778,0.778,0.779,0.793,231.06218481063843
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-ica-40.pkl
Iteration 1, loss = 0.55825819
Iteration 2, loss = 0.53610260
Iteration 3, loss = 0.44995843
Iteration 4, loss = 0.40020894
Iteration 5, loss = 0.38868016
Iteration 6, loss = 0.38101846
Iteration 7, loss = 0.37707266
Iteration 8, loss = 0.37476200
Iteration 9, loss = 0.37426429
Iteration 10, loss = 0.37187554
Iteration 11, loss = 0.37167397
Iteration 12, loss = 0.37010117
Iteration 13, loss = 0.36894860
Iteration 14, loss = 0.36854896
Iteration 15, loss = 0.36682302
Iteration 16, loss = 0.36659867
Iteration 17, loss = 0.36487113
Iteration 18, loss = 0.36484767
Iteration 19, loss = 0.36498870
Iteration 20, loss = 0.36415456
Iteration 21, loss = 0.36492040
Iteration 22, loss = 0.36361749
Iteration 23, loss = 0.36378803
Iteration 24, loss = 0.36391066
Iteration 25, loss = 0.36406435
Iteration 26, loss = 0.36245920
Iteration 27, loss = 0.36364060
Iteration 28, loss = 0.36157141
Iteration 29, loss = 0.36074995
Iteration 30, loss = 0.36153228
Iteration 31, loss = 0.36264656
Iteration 32, loss = 0.36244882
Iteration 33, loss = 0.36055489
Iteration 34, loss = 0.36079572
Iteration 35, loss = 0.36095356
Iteration 36, loss = 0.36116946
Iteration 37, loss = 0.36143647
Iteration 38, loss = 0.36135536
Iteration 39, loss = 0.36019294
Iteration 40, loss = 0.36002176
Iteration 41, loss = 0.36076350
Iteration 42, loss = 0.36275383
Iteration 43, loss = 0.36088652
Iteration 44, loss = 0.36100785
Iteration 45, loss = 0.36021962
Iteration 46, loss = 0.36055431
Iteration 47, loss = 0.35975280
Iteration 48, loss = 0.35947267
Iteration 49, loss = 0.35897584
Iteration 50, loss = 0.36087456
Iteration 51, loss = 0.35918242
Iteration 52, loss = 0.36037527
Iteration 53, loss = 0.35851594
Iteration 54, loss = 0.35957215
Iteration 55, loss = 0.36015549
Iteration 56, loss = 0.35896599
Iteration 57, loss = 0.36091057
Iteration 58, loss = 0.35840845
Iteration 59, loss = 0.36193038
Iteration 60, loss = 0.35971745
Iteration 61, loss = 0.36077842
Iteration 62, loss = 0.35876460
Iteration 63, loss = 0.35841242
Iteration 64, loss = 0.35984365
Iteration 65, loss = 0.35919920
Iteration 66, loss = 0.35993085
Iteration 67, loss = 0.35931945
Iteration 68, loss = 0.35880415
Iteration 69, loss = 0.35769763
Iteration 70, loss = 0.35978504
Iteration 71, loss = 0.36005907
Iteration 72, loss = 0.35908492
Iteration 73, loss = 0.35903907
Iteration 74, loss = 0.35898964
Iteration 75, loss = 0.35832687
Iteration 76, loss = 0.35870663
Iteration 77, loss = 0.36064236
Iteration 78, loss = 0.35942555
Iteration 79, loss = 0.35939540
Iteration 80, loss = 0.35881503
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--ica_40d-iter-1.pkl
"ds4 (ica_40d)",0.840,0.789,0.766,0.749,0.786,0.839,0.788,0.762,0.745,0.784,49.255122661590576
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-ica-57.pkl
Iteration 1, loss = 0.55005877
Iteration 2, loss = 0.47969172
Iteration 3, loss = 0.40744864
Iteration 4, loss = 0.38013807
Iteration 5, loss = 0.36834633
Iteration 6, loss = 0.36419246
Iteration 7, loss = 0.35883646
Iteration 8, loss = 0.35794495
Iteration 9, loss = 0.35622835
Iteration 10, loss = 0.35205901
Iteration 11, loss = 0.35212748
Iteration 12, loss = 0.35108502
Iteration 13, loss = 0.34966531
Iteration 14, loss = 0.34839712
Iteration 15, loss = 0.34817080
Iteration 16, loss = 0.34622301
Iteration 17, loss = 0.34554214
Iteration 18, loss = 0.34537185
Iteration 19, loss = 0.34474909
Iteration 20, loss = 0.34586537
Iteration 21, loss = 0.34436318
Iteration 22, loss = 0.34287328
Iteration 23, loss = 0.34378562
Iteration 24, loss = 0.34341392
Iteration 25, loss = 0.34402536
Iteration 26, loss = 0.34391922
Iteration 27, loss = 0.34132748
Iteration 28, loss = 0.34232270
Iteration 29, loss = 0.34384253
Iteration 30, loss = 0.34323359
Iteration 31, loss = 0.34197288
Iteration 32, loss = 0.34160652
Iteration 33, loss = 0.34190730
Iteration 34, loss = 0.34040964
Iteration 35, loss = 0.33985938
Iteration 36, loss = 0.34061846
Iteration 37, loss = 0.34232321
Iteration 38, loss = 0.34074694
Iteration 39, loss = 0.34124235
Iteration 40, loss = 0.34050688
Iteration 41, loss = 0.34108982
Iteration 42, loss = 0.34059131
Iteration 43, loss = 0.33956552
Iteration 44, loss = 0.33902769
Iteration 45, loss = 0.33961495
Iteration 46, loss = 0.33897933
Iteration 47, loss = 0.33966965
Iteration 48, loss = 0.34011007
Iteration 49, loss = 0.34125938
Iteration 50, loss = 0.34079242
Iteration 51, loss = 0.34075729
Iteration 52, loss = 0.33942665
Iteration 53, loss = 0.33845630
Iteration 54, loss = 0.33956529
Iteration 55, loss = 0.33928363
Iteration 56, loss = 0.33869440
Iteration 57, loss = 0.33896574
Iteration 58, loss = 0.33893150
Iteration 59, loss = 0.33959780
Iteration 60, loss = 0.33927172
Iteration 61, loss = 0.33974116
Iteration 62, loss = 0.33841188
Iteration 63, loss = 0.33892054
Iteration 64, loss = 0.34085348
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--ica_57d-iter-1.pkl
"ds4 (ica_57d)",0.850,0.811,0.775,0.754,0.797,0.568,0.528,0.512,0.538,0.536,40.25885057449341
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-30.pkl
Iteration 1, loss = 0.52994808
Iteration 2, loss = 0.43682526
Iteration 3, loss = 0.40968581
Iteration 4, loss = 0.39233117
Iteration 5, loss = 0.37953415
Iteration 6, loss = 0.37168819
Iteration 7, loss = 0.36482175
Iteration 8, loss = 0.35972097
Iteration 9, loss = 0.35494407
Iteration 10, loss = 0.35132040
Iteration 11, loss = 0.34829126
Iteration 12, loss = 0.34527125
Iteration 13, loss = 0.34323908
Iteration 14, loss = 0.34101084
Iteration 15, loss = 0.33962559
Iteration 16, loss = 0.33757819
Iteration 17, loss = 0.33679387
Iteration 18, loss = 0.33460060
Iteration 19, loss = 0.33338014
Iteration 20, loss = 0.33184934
Iteration 21, loss = 0.33057752
Iteration 22, loss = 0.32958391
Iteration 23, loss = 0.32808929
Iteration 24, loss = 0.32705081
Iteration 25, loss = 0.32569476
Iteration 26, loss = 0.32532283
Iteration 27, loss = 0.32461618
Iteration 28, loss = 0.32393561
Iteration 29, loss = 0.32350971
Iteration 30, loss = 0.32240885
Iteration 31, loss = 0.32185006
Iteration 32, loss = 0.32139411
Iteration 33, loss = 0.32039903
Iteration 34, loss = 0.32053457
Iteration 35, loss = 0.31895034
Iteration 36, loss = 0.31940421
Iteration 37, loss = 0.31901577
Iteration 38, loss = 0.31749582
Iteration 39, loss = 0.31719517
Iteration 40, loss = 0.31724244
Iteration 41, loss = 0.31643249
Iteration 42, loss = 0.31614454
Iteration 43, loss = 0.31534858
Iteration 44, loss = 0.31543906
Iteration 45, loss = 0.31475916
Iteration 46, loss = 0.31459694
Iteration 47, loss = 0.31407256
Iteration 48, loss = 0.31390490
Iteration 49, loss = 0.31361696
Iteration 50, loss = 0.31249229
Iteration 51, loss = 0.31310235
Iteration 52, loss = 0.31245696
Iteration 53, loss = 0.31189945
Iteration 54, loss = 0.31184073
Iteration 55, loss = 0.31161186
Iteration 56, loss = 0.31095037
Iteration 57, loss = 0.31079612
Iteration 58, loss = 0.31138354
Iteration 59, loss = 0.31078314
Iteration 60, loss = 0.31122461
Iteration 61, loss = 0.31025140
Iteration 62, loss = 0.30931959
Iteration 63, loss = 0.30979952
Iteration 64, loss = 0.30934670
Iteration 65, loss = 0.30872739
Iteration 66, loss = 0.30932254
Iteration 67, loss = 0.30907590
Iteration 68, loss = 0.30896623
Iteration 69, loss = 0.30765274
Iteration 70, loss = 0.30810780
Iteration 71, loss = 0.30812477
Iteration 72, loss = 0.30822518
Iteration 73, loss = 0.30808639
Iteration 74, loss = 0.30712063
Iteration 75, loss = 0.30742713
Iteration 76, loss = 0.30644533
Iteration 77, loss = 0.30695466
Iteration 78, loss = 0.30703370
Iteration 79, loss = 0.30667928
Iteration 80, loss = 0.30626934
Iteration 81, loss = 0.30613700
Iteration 82, loss = 0.30619137
Iteration 83, loss = 0.30645911
Iteration 84, loss = 0.30546172
Iteration 85, loss = 0.30520692
Iteration 86, loss = 0.30600517
Iteration 87, loss = 0.30490850
Iteration 88, loss = 0.30526088
Iteration 89, loss = 0.30522451
Iteration 90, loss = 0.30525657
Iteration 91, loss = 0.30480227
Iteration 92, loss = 0.30512221
Iteration 93, loss = 0.30476650
Iteration 94, loss = 0.30459454
Iteration 95, loss = 0.30449290
Iteration 96, loss = 0.30392967
Iteration 97, loss = 0.30431041
Iteration 98, loss = 0.30422241
Iteration 99, loss = 0.30381752
Iteration 100, loss = 0.30357287
Iteration 101, loss = 0.30348610
Iteration 102, loss = 0.30320359
Iteration 103, loss = 0.30407304
Iteration 104, loss = 0.30328680
Iteration 105, loss = 0.30394396
Iteration 106, loss = 0.30283043
Iteration 107, loss = 0.30330143
Iteration 108, loss = 0.30259996
Iteration 109, loss = 0.30275295
Iteration 110, loss = 0.30200982
Iteration 111, loss = 0.30338191
Iteration 112, loss = 0.30284465
Iteration 113, loss = 0.30229611
Iteration 114, loss = 0.30201131
Iteration 115, loss = 0.30271527
Iteration 116, loss = 0.30207947
Iteration 117, loss = 0.30212986
Iteration 118, loss = 0.30132029
Iteration 119, loss = 0.30137297
Iteration 120, loss = 0.30210984
Iteration 121, loss = 0.30153316
Iteration 122, loss = 0.30095715
Iteration 123, loss = 0.30157010
Iteration 124, loss = 0.30108693
Iteration 125, loss = 0.30100745
Iteration 126, loss = 0.30112081
Iteration 127, loss = 0.30081437
Iteration 128, loss = 0.30127956
Iteration 129, loss = 0.30049561
Iteration 130, loss = 0.30031512
Iteration 131, loss = 0.30094159
Iteration 132, loss = 0.30023409
Iteration 133, loss = 0.30024281
Iteration 134, loss = 0.29962763
Iteration 135, loss = 0.29999334
Iteration 136, loss = 0.29978594
Iteration 137, loss = 0.30002045
Iteration 138, loss = 0.30033471
Iteration 139, loss = 0.30044308
Iteration 140, loss = 0.29897369
Iteration 141, loss = 0.29899624
Iteration 142, loss = 0.29913127
Iteration 143, loss = 0.29896641
Iteration 144, loss = 0.29940269
Iteration 145, loss = 0.29957118
Iteration 146, loss = 0.30000118
Iteration 147, loss = 0.29877051
Iteration 148, loss = 0.29902325
Iteration 149, loss = 0.29961861
Iteration 150, loss = 0.29915434
Iteration 151, loss = 0.29847493
Iteration 152, loss = 0.29855528
Iteration 153, loss = 0.29795061
Iteration 154, loss = 0.29880697
Iteration 155, loss = 0.29821563
Iteration 156, loss = 0.29823013
Iteration 157, loss = 0.29901214
Iteration 158, loss = 0.29759087
Iteration 159, loss = 0.29841119
Iteration 160, loss = 0.29823044
Iteration 161, loss = 0.29759421
Iteration 162, loss = 0.29765281
Iteration 163, loss = 0.29765605
Iteration 164, loss = 0.29832467
Iteration 165, loss = 0.29738017
Iteration 166, loss = 0.29723760
Iteration 167, loss = 0.29693027
Iteration 168, loss = 0.29793431
Iteration 169, loss = 0.29721810
Iteration 170, loss = 0.29713689
Iteration 171, loss = 0.29653526
Iteration 172, loss = 0.29707424
Iteration 173, loss = 0.29684186
Iteration 174, loss = 0.29696099
Iteration 175, loss = 0.29740984
Iteration 176, loss = 0.29720639
Iteration 177, loss = 0.29669526
Iteration 178, loss = 0.29693839
Iteration 179, loss = 0.29670169
Iteration 180, loss = 0.29634517
Iteration 181, loss = 0.29683943
Iteration 182, loss = 0.29585374
Iteration 183, loss = 0.29696452
Iteration 184, loss = 0.29654679
Iteration 185, loss = 0.29603505
Iteration 186, loss = 0.29586879
Iteration 187, loss = 0.29585934
Iteration 188, loss = 0.29582211
Iteration 189, loss = 0.29578747
Iteration 190, loss = 0.29587003
Iteration 191, loss = 0.29556520
Iteration 192, loss = 0.29598449
Iteration 193, loss = 0.29585242
Iteration 194, loss = 0.29646007
Iteration 195, loss = 0.29538497
Iteration 196, loss = 0.29614131
Iteration 197, loss = 0.29585665
Iteration 198, loss = 0.29511871
Iteration 199, loss = 0.29484373
Iteration 200, loss = 0.29573276
Iteration 201, loss = 0.29475857
Iteration 202, loss = 0.29471171
Iteration 203, loss = 0.29556983
Iteration 204, loss = 0.29526075
Iteration 205, loss = 0.29531250
Iteration 206, loss = 0.29492341
Iteration 207, loss = 0.29558251
Iteration 208, loss = 0.29401410
Iteration 209, loss = 0.29540165
Iteration 210, loss = 0.29415942
Iteration 211, loss = 0.29476946
Iteration 212, loss = 0.29457302
Iteration 213, loss = 0.29468434
Iteration 214, loss = 0.29426324
Iteration 215, loss = 0.29480961
Iteration 216, loss = 0.29523870
Iteration 217, loss = 0.29405130
Iteration 218, loss = 0.29445452
Iteration 219, loss = 0.29404021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_30d-iter-1.pkl
"ds4 (rp_30d)",0.876,0.837,0.822,0.811,0.837,0.848,0.798,0.783,0.771,0.800,155.97638654708862
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-40.pkl
Iteration 1, loss = 0.43772468
Iteration 2, loss = 0.38052614
Iteration 3, loss = 0.36698488
Iteration 4, loss = 0.35852897
Iteration 5, loss = 0.35304066
Iteration 6, loss = 0.34781220
Iteration 7, loss = 0.34312854
Iteration 8, loss = 0.34032345
Iteration 9, loss = 0.33799740
Iteration 10, loss = 0.33613206
Iteration 11, loss = 0.33304873
Iteration 12, loss = 0.33193474
Iteration 13, loss = 0.32946402
Iteration 14, loss = 0.32818252
Iteration 15, loss = 0.32639887
Iteration 16, loss = 0.32440654
Iteration 17, loss = 0.32386404
Iteration 18, loss = 0.32304274
Iteration 19, loss = 0.32115593
Iteration 20, loss = 0.31971586
Iteration 21, loss = 0.32006739
Iteration 22, loss = 0.31830514
Iteration 23, loss = 0.31791194
Iteration 24, loss = 0.31703073
Iteration 25, loss = 0.31619894
Iteration 26, loss = 0.31642450
Iteration 27, loss = 0.31485557
Iteration 28, loss = 0.31383375
Iteration 29, loss = 0.31433292
Iteration 30, loss = 0.31252544
Iteration 31, loss = 0.31230033
Iteration 32, loss = 0.31195592
Iteration 33, loss = 0.31141613
Iteration 34, loss = 0.31156390
Iteration 35, loss = 0.31118822
Iteration 36, loss = 0.30992454
Iteration 37, loss = 0.30958646
Iteration 38, loss = 0.30927822
Iteration 39, loss = 0.30876475
Iteration 40, loss = 0.30990269
Iteration 41, loss = 0.30863352
Iteration 42, loss = 0.30799360
Iteration 43, loss = 0.30856783
Iteration 44, loss = 0.30758547
Iteration 45, loss = 0.30818423
Iteration 46, loss = 0.30699008
Iteration 47, loss = 0.30672496
Iteration 48, loss = 0.30708338
Iteration 49, loss = 0.30589709
Iteration 50, loss = 0.30563267
Iteration 51, loss = 0.30476310
Iteration 52, loss = 0.30552319
Iteration 53, loss = 0.30466592
Iteration 54, loss = 0.30454420
Iteration 55, loss = 0.30427571
Iteration 56, loss = 0.30435201
Iteration 57, loss = 0.30419537
Iteration 58, loss = 0.30390500
Iteration 59, loss = 0.30407747
Iteration 60, loss = 0.30359474
Iteration 61, loss = 0.30364955
Iteration 62, loss = 0.30205975
Iteration 63, loss = 0.30184715
Iteration 64, loss = 0.30239278
Iteration 65, loss = 0.30187988
Iteration 66, loss = 0.30281608
Iteration 67, loss = 0.30178925
Iteration 68, loss = 0.30162183
Iteration 69, loss = 0.30167304
Iteration 70, loss = 0.30132085
Iteration 71, loss = 0.30139217
Iteration 72, loss = 0.30142844
Iteration 73, loss = 0.30047689
Iteration 74, loss = 0.29994750
Iteration 75, loss = 0.30003427
Iteration 76, loss = 0.30004490
Iteration 77, loss = 0.30018968
Iteration 78, loss = 0.29936539
Iteration 79, loss = 0.30022596
Iteration 80, loss = 0.29922661
Iteration 81, loss = 0.29880571
Iteration 82, loss = 0.29890559
Iteration 83, loss = 0.29865235
Iteration 84, loss = 0.29862290
Iteration 85, loss = 0.29816750
Iteration 86, loss = 0.29834902
Iteration 87, loss = 0.29881543
Iteration 88, loss = 0.29853724
Iteration 89, loss = 0.29771182
Iteration 90, loss = 0.29788347
Iteration 91, loss = 0.29853818
Iteration 92, loss = 0.29762509
Iteration 93, loss = 0.29772605
Iteration 94, loss = 0.29653461
Iteration 95, loss = 0.29733327
Iteration 96, loss = 0.29689816
Iteration 97, loss = 0.29688846
Iteration 98, loss = 0.29592608
Iteration 99, loss = 0.29641457
Iteration 100, loss = 0.29677936
Iteration 101, loss = 0.29538203
Iteration 102, loss = 0.29680250
Iteration 103, loss = 0.29592944
Iteration 104, loss = 0.29449166
Iteration 105, loss = 0.29675076
Iteration 106, loss = 0.29600733
Iteration 107, loss = 0.29594887
Iteration 108, loss = 0.29554715
Iteration 109, loss = 0.29489214
Iteration 110, loss = 0.29563705
Iteration 111, loss = 0.29432957
Iteration 112, loss = 0.29461103
Iteration 113, loss = 0.29443343
Iteration 114, loss = 0.29504163
Iteration 115, loss = 0.29465522
Iteration 116, loss = 0.29487355
Iteration 117, loss = 0.29471506
Iteration 118, loss = 0.29447126
Iteration 119, loss = 0.29478046
Iteration 120, loss = 0.29462984
Iteration 121, loss = 0.29385337
Iteration 122, loss = 0.29311645
Iteration 123, loss = 0.29390093
Iteration 124, loss = 0.29329230
Iteration 125, loss = 0.29339724
Iteration 126, loss = 0.29396782
Iteration 127, loss = 0.29372632
Iteration 128, loss = 0.29381899
Iteration 129, loss = 0.29324074
Iteration 130, loss = 0.29319969
Iteration 131, loss = 0.29250638
Iteration 132, loss = 0.29283883
Iteration 133, loss = 0.29259288
Iteration 134, loss = 0.29226725
Iteration 135, loss = 0.29238461
Iteration 136, loss = 0.29205285
Iteration 137, loss = 0.29237939
Iteration 138, loss = 0.29300934
Iteration 139, loss = 0.29256342
Iteration 140, loss = 0.29155368
Iteration 141, loss = 0.29247078
Iteration 142, loss = 0.29180108
Iteration 143, loss = 0.29161470
Iteration 144, loss = 0.29160725
Iteration 145, loss = 0.29199973
Iteration 146, loss = 0.29169245
Iteration 147, loss = 0.29156610
Iteration 148, loss = 0.29185644
Iteration 149, loss = 0.29096730
Iteration 150, loss = 0.29137418
Iteration 151, loss = 0.29087048
Iteration 152, loss = 0.29148174
Iteration 153, loss = 0.28986533
Iteration 154, loss = 0.29140616
Iteration 155, loss = 0.29061646
Iteration 156, loss = 0.29131107
Iteration 157, loss = 0.29110864
Iteration 158, loss = 0.29048368
Iteration 159, loss = 0.29063132
Iteration 160, loss = 0.29069712
Iteration 161, loss = 0.29069859
Iteration 162, loss = 0.29018778
Iteration 163, loss = 0.29071550
Iteration 164, loss = 0.29034778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_40d-iter-1.pkl
"ds4 (rp_40d)",0.878,0.843,0.825,0.811,0.839,0.848,0.798,0.781,0.768,0.799,110.5900776386261
Read /home/bb/cs7641/proj3/datasets/ds4/dimreducer-rp-57.pkl
Iteration 1, loss = 0.42747304
Iteration 2, loss = 0.38328030
Iteration 3, loss = 0.36889547
Iteration 4, loss = 0.35893433
Iteration 5, loss = 0.35264565
Iteration 6, loss = 0.34745763
Iteration 7, loss = 0.34400342
Iteration 8, loss = 0.34024353
Iteration 9, loss = 0.33731696
Iteration 10, loss = 0.33509760
Iteration 11, loss = 0.33250354
Iteration 12, loss = 0.33151405
Iteration 13, loss = 0.32957575
Iteration 14, loss = 0.32807836
Iteration 15, loss = 0.32560149
Iteration 16, loss = 0.32508509
Iteration 17, loss = 0.32353988
Iteration 18, loss = 0.32307456
Iteration 19, loss = 0.32137493
Iteration 20, loss = 0.32102015
Iteration 21, loss = 0.31929578
Iteration 22, loss = 0.31869752
Iteration 23, loss = 0.31786213
Iteration 24, loss = 0.31703174
Iteration 25, loss = 0.31607813
Iteration 26, loss = 0.31612341
Iteration 27, loss = 0.31472217
Iteration 28, loss = 0.31504735
Iteration 29, loss = 0.31402623
Iteration 30, loss = 0.31344845
Iteration 31, loss = 0.31216857
Iteration 32, loss = 0.31197169
Iteration 33, loss = 0.31214547
Iteration 34, loss = 0.31133005
Iteration 35, loss = 0.31156713
Iteration 36, loss = 0.30965489
Iteration 37, loss = 0.30897406
Iteration 38, loss = 0.30898303
Iteration 39, loss = 0.30841000
Iteration 40, loss = 0.30829076
Iteration 41, loss = 0.30732234
Iteration 42, loss = 0.30752783
Iteration 43, loss = 0.30616675
Iteration 44, loss = 0.30754747
Iteration 45, loss = 0.30550252
Iteration 46, loss = 0.30513040
Iteration 47, loss = 0.30538099
Iteration 48, loss = 0.30482635
Iteration 49, loss = 0.30438356
Iteration 50, loss = 0.30435694
Iteration 51, loss = 0.30352448
Iteration 52, loss = 0.30384153
Iteration 53, loss = 0.30292609
Iteration 54, loss = 0.30295289
Iteration 55, loss = 0.30126776
Iteration 56, loss = 0.30196262
Iteration 57, loss = 0.30154307
Iteration 58, loss = 0.30087793
Iteration 59, loss = 0.30089487
Iteration 60, loss = 0.30083520
Iteration 61, loss = 0.30103843
Iteration 62, loss = 0.29935646
Iteration 63, loss = 0.29983021
Iteration 64, loss = 0.29992048
Iteration 65, loss = 0.29961947
Iteration 66, loss = 0.29918857
Iteration 67, loss = 0.29817311
Iteration 68, loss = 0.29930329
Iteration 69, loss = 0.29876220
Iteration 70, loss = 0.29805499
Iteration 71, loss = 0.29678801
Iteration 72, loss = 0.29758252
Iteration 73, loss = 0.29730244
Iteration 74, loss = 0.29680486
Iteration 75, loss = 0.29740360
Iteration 76, loss = 0.29683322
Iteration 77, loss = 0.29700883
Iteration 78, loss = 0.29561421
Iteration 79, loss = 0.29596365
Iteration 80, loss = 0.29607222
Iteration 81, loss = 0.29566737
Iteration 82, loss = 0.29560057
Iteration 83, loss = 0.29612352
Iteration 84, loss = 0.29525840
Iteration 85, loss = 0.29570114
Iteration 86, loss = 0.29435176
Iteration 87, loss = 0.29415457
Iteration 88, loss = 0.29436606
Iteration 89, loss = 0.29421330
Iteration 90, loss = 0.29387421
Iteration 91, loss = 0.29445195
Iteration 92, loss = 0.29352000
Iteration 93, loss = 0.29314197
Iteration 94, loss = 0.29428830
Iteration 95, loss = 0.29415287
Iteration 96, loss = 0.29306517
Iteration 97, loss = 0.29243889
Iteration 98, loss = 0.29303959
Iteration 99, loss = 0.29213658
Iteration 100, loss = 0.29187426
Iteration 101, loss = 0.29256406
Iteration 102, loss = 0.29323398
Iteration 103, loss = 0.29125082
Iteration 104, loss = 0.29194473
Iteration 105, loss = 0.29146260
Iteration 106, loss = 0.29161295
Iteration 107, loss = 0.29195893
Iteration 108, loss = 0.29161014
Iteration 109, loss = 0.29153208
Iteration 110, loss = 0.29091228
Iteration 111, loss = 0.29113135
Iteration 112, loss = 0.29037632
Iteration 113, loss = 0.29061271
Iteration 114, loss = 0.29062877
Iteration 115, loss = 0.29039704
Iteration 116, loss = 0.29051294
Iteration 117, loss = 0.28947521
Iteration 118, loss = 0.28948541
Iteration 119, loss = 0.28982898
Iteration 120, loss = 0.29027915
Iteration 121, loss = 0.28919396
Iteration 122, loss = 0.28992735
Iteration 123, loss = 0.28944494
Iteration 124, loss = 0.28918724
Iteration 125, loss = 0.28929798
Iteration 126, loss = 0.28907712
Iteration 127, loss = 0.28870326
Iteration 128, loss = 0.28841708
Iteration 129, loss = 0.28922305
Iteration 130, loss = 0.28893410
Iteration 131, loss = 0.28824876
Iteration 132, loss = 0.28790719
Iteration 133, loss = 0.28866729
Iteration 134, loss = 0.28837231
Iteration 135, loss = 0.28844431
Iteration 136, loss = 0.28809320
Iteration 137, loss = 0.28777457
Iteration 138, loss = 0.28773126
Iteration 139, loss = 0.28795274
Iteration 140, loss = 0.28837300
Iteration 141, loss = 0.28791193
Iteration 142, loss = 0.28788656
Iteration 143, loss = 0.28765489
Iteration 144, loss = 0.28762560
Iteration 145, loss = 0.28728177
Iteration 146, loss = 0.28712102
Iteration 147, loss = 0.28697542
Iteration 148, loss = 0.28683845
Iteration 149, loss = 0.28619057
Iteration 150, loss = 0.28605415
Iteration 151, loss = 0.28623596
Iteration 152, loss = 0.28730798
Iteration 153, loss = 0.28638811
Iteration 154, loss = 0.28650356
Iteration 155, loss = 0.28608666
Iteration 156, loss = 0.28615468
Iteration 157, loss = 0.28627144
Iteration 158, loss = 0.28695218
Iteration 159, loss = 0.28622761
Iteration 160, loss = 0.28586580
Iteration 161, loss = 0.28574970
Iteration 162, loss = 0.28583121
Iteration 163, loss = 0.28480726
Iteration 164, loss = 0.28556409
Iteration 165, loss = 0.28510283
Iteration 166, loss = 0.28596412
Iteration 167, loss = 0.28557734
Iteration 168, loss = 0.28531573
Iteration 169, loss = 0.28546118
Iteration 170, loss = 0.28475634
Iteration 171, loss = 0.28519233
Iteration 172, loss = 0.28456750
Iteration 173, loss = 0.28456614
Iteration 174, loss = 0.28561308
Iteration 175, loss = 0.28448121
Iteration 176, loss = 0.28385982
Iteration 177, loss = 0.28497331
Iteration 178, loss = 0.28555115
Iteration 179, loss = 0.28348313
Iteration 180, loss = 0.28407834
Iteration 181, loss = 0.28374031
Iteration 182, loss = 0.28489449
Iteration 183, loss = 0.28387163
Iteration 184, loss = 0.28380297
Iteration 185, loss = 0.28367085
Iteration 186, loss = 0.28368302
Iteration 187, loss = 0.28384287
Iteration 188, loss = 0.28385250
Iteration 189, loss = 0.28486741
Iteration 190, loss = 0.28395959
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--rp_57d-iter-1.pkl
"ds4 (rp_57d)",0.881,0.855,0.825,0.805,0.842,0.845,0.797,0.770,0.752,0.791,130.15832805633545
k=30
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")
** -> lda projection calculation for ds4, k=30 elapsed seconds 0.9731762409210205 <- took less than 1 second
Wrote /home/bb/cs7641/proj3/datasets/ds4/dimreducer-lda-30.pkl
Iteration 1, loss = 0.65402392
Iteration 2, loss = 0.55762172
Iteration 3, loss = 0.50023246
Iteration 4, loss = 0.46181912
Iteration 5, loss = 0.43489369
Iteration 6, loss = 0.41573100
Iteration 7, loss = 0.40155031
Iteration 8, loss = 0.39103909
Iteration 9, loss = 0.38286349
Iteration 10, loss = 0.37664510
Iteration 11, loss = 0.37156226
Iteration 12, loss = 0.36744078
Iteration 13, loss = 0.36398924
Iteration 14, loss = 0.36129173
Iteration 15, loss = 0.35951039
Iteration 16, loss = 0.35721659
Iteration 17, loss = 0.35566654
Iteration 18, loss = 0.35451217
Iteration 19, loss = 0.35337096
Iteration 20, loss = 0.35230643
Iteration 21, loss = 0.35105168
Iteration 22, loss = 0.35053391
Iteration 23, loss = 0.34950430
Iteration 24, loss = 0.34921924
Iteration 25, loss = 0.34869623
Iteration 26, loss = 0.34796023
Iteration 27, loss = 0.34764271
Iteration 28, loss = 0.34715823
Iteration 29, loss = 0.34695101
Iteration 30, loss = 0.34681356
Iteration 31, loss = 0.34612224
Iteration 32, loss = 0.34596205
Iteration 33, loss = 0.34591970
Iteration 34, loss = 0.34577438
Iteration 35, loss = 0.34533031
Iteration 36, loss = 0.34550310
Iteration 37, loss = 0.34494692
Iteration 38, loss = 0.34522626
Iteration 39, loss = 0.34505929
Iteration 40, loss = 0.34474571
Iteration 41, loss = 0.34456222
Iteration 42, loss = 0.34447442
Iteration 43, loss = 0.34452155
Iteration 44, loss = 0.34416252
Iteration 45, loss = 0.34448728
Iteration 46, loss = 0.34432926
Iteration 47, loss = 0.34469045
Iteration 48, loss = 0.34403086
Iteration 49, loss = 0.34406590
Iteration 50, loss = 0.34405005
Iteration 51, loss = 0.34444300
Iteration 52, loss = 0.34402516
Iteration 53, loss = 0.34421605
Iteration 54, loss = 0.34389524
Iteration 55, loss = 0.34386468
Iteration 56, loss = 0.34389559
Iteration 57, loss = 0.34374105
Iteration 58, loss = 0.34399769
Iteration 59, loss = 0.34421186
Iteration 60, loss = 0.34397124
Iteration 61, loss = 0.34390568
Iteration 62, loss = 0.34386539
Iteration 63, loss = 0.34397223
Iteration 64, loss = 0.34403063
Iteration 65, loss = 0.34406558
Iteration 66, loss = 0.34399714
Iteration 67, loss = 0.34358525
Iteration 68, loss = 0.34379470
Iteration 69, loss = 0.34380499
Iteration 70, loss = 0.34378233
Iteration 71, loss = 0.34371650
Iteration 72, loss = 0.34380993
Iteration 73, loss = 0.34378736
Iteration 74, loss = 0.34358703
Iteration 75, loss = 0.34380363
Iteration 76, loss = 0.34373136
Iteration 77, loss = 0.34380691
Iteration 78, loss = 0.34378712
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--lda_30d-iter-1.pkl
"ds4 (lda_30d)",0.841,0.790,0.767,0.751,0.787,0.842,0.791,0.768,0.752,0.788,45.61694288253784
k=40
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")
** -> lda projection calculation for ds4, k=40 elapsed seconds 0.9504556655883789 <- took less than 1 second
Wrote /home/bb/cs7641/proj3/datasets/ds4/dimreducer-lda-40.pkl
Iteration 1, loss = 0.65402392
Iteration 2, loss = 0.55762172
Iteration 3, loss = 0.50023246
Iteration 4, loss = 0.46181912
Iteration 5, loss = 0.43489369
Iteration 6, loss = 0.41573100
Iteration 7, loss = 0.40155031
Iteration 8, loss = 0.39103909
Iteration 9, loss = 0.38286349
Iteration 10, loss = 0.37664510
Iteration 11, loss = 0.37156226
Iteration 12, loss = 0.36744078
Iteration 13, loss = 0.36398924
Iteration 14, loss = 0.36129173
Iteration 15, loss = 0.35951039
Iteration 16, loss = 0.35721659
Iteration 17, loss = 0.35566654
Iteration 18, loss = 0.35451217
Iteration 19, loss = 0.35337096
Iteration 20, loss = 0.35230643
Iteration 21, loss = 0.35105168
Iteration 22, loss = 0.35053391
Iteration 23, loss = 0.34950430
Iteration 24, loss = 0.34921924
Iteration 25, loss = 0.34869623
Iteration 26, loss = 0.34796023
Iteration 27, loss = 0.34764271
Iteration 28, loss = 0.34715823
Iteration 29, loss = 0.34695101
Iteration 30, loss = 0.34681356
Iteration 31, loss = 0.34612224
Iteration 32, loss = 0.34596205
Iteration 33, loss = 0.34591970
Iteration 34, loss = 0.34577438
Iteration 35, loss = 0.34533031
Iteration 36, loss = 0.34550310
Iteration 37, loss = 0.34494692
Iteration 38, loss = 0.34522626
Iteration 39, loss = 0.34505929
Iteration 40, loss = 0.34474571
Iteration 41, loss = 0.34456222
Iteration 42, loss = 0.34447442
Iteration 43, loss = 0.34452155
Iteration 44, loss = 0.34416252
Iteration 45, loss = 0.34448728
Iteration 46, loss = 0.34432926
Iteration 47, loss = 0.34469045
Iteration 48, loss = 0.34403086
Iteration 49, loss = 0.34406590
Iteration 50, loss = 0.34405005
Iteration 51, loss = 0.34444300
Iteration 52, loss = 0.34402516
Iteration 53, loss = 0.34421605
Iteration 54, loss = 0.34389524
Iteration 55, loss = 0.34386468
Iteration 56, loss = 0.34389559
Iteration 57, loss = 0.34374105
Iteration 58, loss = 0.34399769
Iteration 59, loss = 0.34421186
Iteration 60, loss = 0.34397124
Iteration 61, loss = 0.34390568
Iteration 62, loss = 0.34386539
Iteration 63, loss = 0.34397223
Iteration 64, loss = 0.34403063
Iteration 65, loss = 0.34406558
Iteration 66, loss = 0.34399714
Iteration 67, loss = 0.34358525
Iteration 68, loss = 0.34379470
Iteration 69, loss = 0.34380499
Iteration 70, loss = 0.34378233
Iteration 71, loss = 0.34371650
Iteration 72, loss = 0.34380993
Iteration 73, loss = 0.34378736
Iteration 74, loss = 0.34358703
Iteration 75, loss = 0.34380363
Iteration 76, loss = 0.34373136
Iteration 77, loss = 0.34380691
Iteration 78, loss = 0.34378712
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--lda_40d-iter-1.pkl
"ds4 (lda_40d)",0.841,0.790,0.767,0.751,0.787,0.842,0.791,0.768,0.752,0.788,45.66824817657471
k=57
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")
** -> lda projection calculation for ds4, k=57 elapsed seconds 0.6818602085113525 <- took less than 1 second
Wrote /home/bb/cs7641/proj3/datasets/ds4/dimreducer-lda-57.pkl
Iteration 1, loss = 0.65402392
Iteration 2, loss = 0.55762172
Iteration 3, loss = 0.50023246
Iteration 4, loss = 0.46181912
Iteration 5, loss = 0.43489369
Iteration 6, loss = 0.41573100
Iteration 7, loss = 0.40155031
Iteration 8, loss = 0.39103909
Iteration 9, loss = 0.38286349
Iteration 10, loss = 0.37664510
Iteration 11, loss = 0.37156226
Iteration 12, loss = 0.36744078
Iteration 13, loss = 0.36398924
Iteration 14, loss = 0.36129173
Iteration 15, loss = 0.35951039
Iteration 16, loss = 0.35721659
Iteration 17, loss = 0.35566654
Iteration 18, loss = 0.35451217
Iteration 19, loss = 0.35337096
Iteration 20, loss = 0.35230643
Iteration 21, loss = 0.35105168
Iteration 22, loss = 0.35053391
Iteration 23, loss = 0.34950430
Iteration 24, loss = 0.34921924
Iteration 25, loss = 0.34869623
Iteration 26, loss = 0.34796023
Iteration 27, loss = 0.34764271
Iteration 28, loss = 0.34715823
Iteration 29, loss = 0.34695101
Iteration 30, loss = 0.34681356
Iteration 31, loss = 0.34612224
Iteration 32, loss = 0.34596205
Iteration 33, loss = 0.34591970
Iteration 34, loss = 0.34577438
Iteration 35, loss = 0.34533031
Iteration 36, loss = 0.34550310
Iteration 37, loss = 0.34494692
Iteration 38, loss = 0.34522626
Iteration 39, loss = 0.34505929
Iteration 40, loss = 0.34474571
Iteration 41, loss = 0.34456222
Iteration 42, loss = 0.34447442
Iteration 43, loss = 0.34452155
Iteration 44, loss = 0.34416252
Iteration 45, loss = 0.34448728
Iteration 46, loss = 0.34432926
Iteration 47, loss = 0.34469045
Iteration 48, loss = 0.34403086
Iteration 49, loss = 0.34406590
Iteration 50, loss = 0.34405005
Iteration 51, loss = 0.34444300
Iteration 52, loss = 0.34402516
Iteration 53, loss = 0.34421605
Iteration 54, loss = 0.34389524
Iteration 55, loss = 0.34386468
Iteration 56, loss = 0.34389559
Iteration 57, loss = 0.34374105
Iteration 58, loss = 0.34399769
Iteration 59, loss = 0.34421186
Iteration 60, loss = 0.34397124
Iteration 61, loss = 0.34390568
Iteration 62, loss = 0.34386539
Iteration 63, loss = 0.34397223
Iteration 64, loss = 0.34403063
Iteration 65, loss = 0.34406558
Iteration 66, loss = 0.34399714
Iteration 67, loss = 0.34358525
Iteration 68, loss = 0.34379470
Iteration 69, loss = 0.34380499
Iteration 70, loss = 0.34378233
Iteration 71, loss = 0.34371650
Iteration 72, loss = 0.34380993
Iteration 73, loss = 0.34378736
Iteration 74, loss = 0.34358703
Iteration 75, loss = 0.34380363
Iteration 76, loss = 0.34373136
Iteration 77, loss = 0.34380691
Iteration 78, loss = 0.34378712
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds4/mlp-model--50-20-1--lda_57d-iter-1.pkl
"ds4 (lda_57d)",0.841,0.790,0.767,0.751,0.787,0.842,0.791,0.768,0.752,0.788,36.64154553413391
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.pkl
Read datasets/ds1/mlp-model--200-200-200--Unmodified-iter-1.traintime
"ds1 (Unmodified)",0.815,0.834,0.791,0.778,0.804,0.798,0.814,0.773,0.761,0.787,703.2312424182892
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-pca-5.pkl
Iteration 1, loss = 0.52549323
Iteration 2, loss = 0.49689140
Iteration 3, loss = 0.49116181
Iteration 4, loss = 0.48887919
Iteration 5, loss = 0.48719149
Iteration 6, loss = 0.48391905
Iteration 7, loss = 0.48250967
Iteration 8, loss = 0.47929575
Iteration 9, loss = 0.48055237
Iteration 10, loss = 0.47852242
Iteration 11, loss = 0.47394942
Iteration 12, loss = 0.47680985
Iteration 13, loss = 0.47608900
Iteration 14, loss = 0.47450015
Iteration 15, loss = 0.47157573
Iteration 16, loss = 0.46763791
Iteration 17, loss = 0.46650231
Iteration 18, loss = 0.46750838
Iteration 19, loss = 0.46574728
Iteration 20, loss = 0.46263712
Iteration 21, loss = 0.46136395
Iteration 22, loss = 0.45994557
Iteration 23, loss = 0.45980754
Iteration 24, loss = 0.46491707
Iteration 25, loss = 0.45846220
Iteration 26, loss = 0.46039383
Iteration 27, loss = 0.45613452
Iteration 28, loss = 0.45567932
Iteration 29, loss = 0.45563279
Iteration 30, loss = 0.45574164
Iteration 31, loss = 0.45291672
Iteration 32, loss = 0.45284357
Iteration 33, loss = 0.45147891
Iteration 34, loss = 0.45195898
Iteration 35, loss = 0.45161537
Iteration 36, loss = 0.45058801
Iteration 37, loss = 0.44917622
Iteration 38, loss = 0.44915664
Iteration 39, loss = 0.44935830
Iteration 40, loss = 0.44769344
Iteration 41, loss = 0.44769914
Iteration 42, loss = 0.44694831
Iteration 43, loss = 0.44705247
Iteration 44, loss = 0.44521953
Iteration 45, loss = 0.44575305
Iteration 46, loss = 0.44505896
Iteration 47, loss = 0.44543759
Iteration 48, loss = 0.44778971
Iteration 49, loss = 0.44389708
Iteration 50, loss = 0.44615054
Iteration 51, loss = 0.44427882
Iteration 52, loss = 0.44445234
Iteration 53, loss = 0.44699565
Iteration 54, loss = 0.44363367
Iteration 55, loss = 0.44289914
Iteration 56, loss = 0.44260499
Iteration 57, loss = 0.44412604
Iteration 58, loss = 0.44318084
Iteration 59, loss = 0.44225463
Iteration 60, loss = 0.44057516
Iteration 61, loss = 0.44315188
Iteration 62, loss = 0.44047567
Iteration 63, loss = 0.44283374
Iteration 64, loss = 0.44039726
Iteration 65, loss = 0.44163052
Iteration 66, loss = 0.44116840
Iteration 67, loss = 0.44072566
Iteration 68, loss = 0.44083884
Iteration 69, loss = 0.44052391
Iteration 70, loss = 0.43978529
Iteration 71, loss = 0.43898295
Iteration 72, loss = 0.43983580
Iteration 73, loss = 0.44044335
Iteration 74, loss = 0.44073623
Iteration 75, loss = 0.43777560
Iteration 76, loss = 0.43869362
Iteration 77, loss = 0.43936071
Iteration 78, loss = 0.44266668
Iteration 79, loss = 0.43893858
Iteration 80, loss = 0.43914192
Iteration 81, loss = 0.43869386
Iteration 82, loss = 0.43674481
Iteration 83, loss = 0.43764971
Iteration 84, loss = 0.43783476
Iteration 85, loss = 0.43707032
Iteration 86, loss = 0.43705225
Iteration 87, loss = 0.43654742
Iteration 88, loss = 0.43802738
Iteration 89, loss = 0.43714981
Iteration 90, loss = 0.43731428
Iteration 91, loss = 0.43555216
Iteration 92, loss = 0.43872222
Iteration 93, loss = 0.43650489
Iteration 94, loss = 0.43593861
Iteration 95, loss = 0.43794309
Iteration 96, loss = 0.43712650
Iteration 97, loss = 0.43683162
Iteration 98, loss = 0.43577325
Iteration 99, loss = 0.43586242
Iteration 100, loss = 0.43662022
Iteration 101, loss = 0.43596797
Iteration 102, loss = 0.43450351
Iteration 103, loss = 0.43546145
Iteration 104, loss = 0.43577265
Iteration 105, loss = 0.43512576
Iteration 106, loss = 0.43455865
Iteration 107, loss = 0.43425203
Iteration 108, loss = 0.43491760
Iteration 109, loss = 0.43441663
Iteration 110, loss = 0.43423521
Iteration 111, loss = 0.43465205
Iteration 112, loss = 0.43741105
Iteration 113, loss = 0.43350055
Iteration 114, loss = 0.43493947
Iteration 115, loss = 0.43393339
Iteration 116, loss = 0.43507465
Iteration 117, loss = 0.43531711
Iteration 118, loss = 0.43360780
Iteration 119, loss = 0.43306811
Iteration 120, loss = 0.43421727
Iteration 121, loss = 0.43406030
Iteration 122, loss = 0.43356497
Iteration 123, loss = 0.43340577
Iteration 124, loss = 0.43441101
Iteration 125, loss = 0.43303338
Iteration 126, loss = 0.43382841
Iteration 127, loss = 0.43214835
Iteration 128, loss = 0.43207516
Iteration 129, loss = 0.43276106
Iteration 130, loss = 0.43246138
Iteration 131, loss = 0.43407671
Iteration 132, loss = 0.43370210
Iteration 133, loss = 0.43283003
Iteration 134, loss = 0.43133635
Iteration 135, loss = 0.43172753
Iteration 136, loss = 0.43266615
Iteration 137, loss = 0.43237894
Iteration 138, loss = 0.43264216
Iteration 139, loss = 0.43208621
Iteration 140, loss = 0.43276001
Iteration 141, loss = 0.43121620
Iteration 142, loss = 0.43123667
Iteration 143, loss = 0.43176864
Iteration 144, loss = 0.43048330
Iteration 145, loss = 0.43197320
Iteration 146, loss = 0.43113822
Iteration 147, loss = 0.43240457
Iteration 148, loss = 0.43195085
Iteration 149, loss = 0.43049137
Iteration 150, loss = 0.43037902
Iteration 151, loss = 0.43127424
Iteration 152, loss = 0.42985023
Iteration 153, loss = 0.43032288
Iteration 154, loss = 0.43009282
Iteration 155, loss = 0.43010210
Iteration 156, loss = 0.42918989
Iteration 157, loss = 0.43010749
Iteration 158, loss = 0.43065595
Iteration 159, loss = 0.42924107
Iteration 160, loss = 0.42922966
Iteration 161, loss = 0.43036533
Iteration 162, loss = 0.43011793
Iteration 163, loss = 0.43046914
Iteration 164, loss = 0.42889614
Iteration 165, loss = 0.43090643
Iteration 166, loss = 0.42997681
Iteration 167, loss = 0.42874038
Iteration 168, loss = 0.42921159
Iteration 169, loss = 0.42873070
Iteration 170, loss = 0.42981737
Iteration 171, loss = 0.43069343
Iteration 172, loss = 0.43071467
Iteration 173, loss = 0.42958351
Iteration 174, loss = 0.42968327
Iteration 175, loss = 0.42921684
Iteration 176, loss = 0.43050172
Iteration 177, loss = 0.42746195
Iteration 178, loss = 0.42907767
Iteration 179, loss = 0.42977445
Iteration 180, loss = 0.42980723
Iteration 181, loss = 0.42803720
Iteration 182, loss = 0.42816560
Iteration 183, loss = 0.42880147
Iteration 184, loss = 0.42774722
Iteration 185, loss = 0.42809682
Iteration 186, loss = 0.42758765
Iteration 187, loss = 0.42789482
Iteration 188, loss = 0.42837800
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--pca_5d-iter-1.pkl
"ds1 (pca_5d)",0.796,0.817,0.767,0.755,0.784,0.782,0.804,0.749,0.738,0.768,649.6259684562683
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-pca-10.pkl
Iteration 1, loss = 0.49272073
Iteration 2, loss = 0.46881461
Iteration 3, loss = 0.45715758
Iteration 4, loss = 0.45147792
Iteration 5, loss = 0.44773286
Iteration 6, loss = 0.44466451
Iteration 7, loss = 0.44354645
Iteration 8, loss = 0.44102558
Iteration 9, loss = 0.44034186
Iteration 10, loss = 0.44039592
Iteration 11, loss = 0.43908152
Iteration 12, loss = 0.43821673
Iteration 13, loss = 0.43808078
Iteration 14, loss = 0.43693683
Iteration 15, loss = 0.43647511
Iteration 16, loss = 0.43601266
Iteration 17, loss = 0.43489623
Iteration 18, loss = 0.43449528
Iteration 19, loss = 0.43452492
Iteration 20, loss = 0.43335385
Iteration 21, loss = 0.43470556
Iteration 22, loss = 0.43342295
Iteration 23, loss = 0.43354893
Iteration 24, loss = 0.43238213
Iteration 25, loss = 0.43313748
Iteration 26, loss = 0.43167718
Iteration 27, loss = 0.43081337
Iteration 28, loss = 0.43070858
Iteration 29, loss = 0.43062232
Iteration 30, loss = 0.43020006
Iteration 31, loss = 0.42916662
Iteration 32, loss = 0.43022581
Iteration 33, loss = 0.42917283
Iteration 34, loss = 0.42832379
Iteration 35, loss = 0.42780526
Iteration 36, loss = 0.42898084
Iteration 37, loss = 0.42787581
Iteration 38, loss = 0.42744394
Iteration 39, loss = 0.42733995
Iteration 40, loss = 0.42825050
Iteration 41, loss = 0.42697015
Iteration 42, loss = 0.42613117
Iteration 43, loss = 0.42687967
Iteration 44, loss = 0.42610512
Iteration 45, loss = 0.42599644
Iteration 46, loss = 0.42532952
Iteration 47, loss = 0.42647073
Iteration 48, loss = 0.42573924
Iteration 49, loss = 0.42493492
Iteration 50, loss = 0.42575646
Iteration 51, loss = 0.42482094
Iteration 52, loss = 0.42499474
Iteration 53, loss = 0.42488746
Iteration 54, loss = 0.42382703
Iteration 55, loss = 0.42391833
Iteration 56, loss = 0.42408459
Iteration 57, loss = 0.42363009
Iteration 58, loss = 0.42386010
Iteration 59, loss = 0.42232066
Iteration 60, loss = 0.42330298
Iteration 61, loss = 0.42262138
Iteration 62, loss = 0.42283702
Iteration 63, loss = 0.42240347
Iteration 64, loss = 0.42273781
Iteration 65, loss = 0.42173754
Iteration 66, loss = 0.42183522
Iteration 67, loss = 0.42268044
Iteration 68, loss = 0.42191351
Iteration 69, loss = 0.42143682
Iteration 70, loss = 0.42141272
Iteration 71, loss = 0.42132376
Iteration 72, loss = 0.42096232
Iteration 73, loss = 0.42146226
Iteration 74, loss = 0.42102216
Iteration 75, loss = 0.42091283
Iteration 76, loss = 0.42064503
Iteration 77, loss = 0.42074097
Iteration 78, loss = 0.41975645
Iteration 79, loss = 0.42055704
Iteration 80, loss = 0.41988691
Iteration 81, loss = 0.42027672
Iteration 82, loss = 0.41958327
Iteration 83, loss = 0.41918877
Iteration 84, loss = 0.41905931
Iteration 85, loss = 0.41892431
Iteration 86, loss = 0.41906951
Iteration 87, loss = 0.41900237
Iteration 88, loss = 0.41855289
Iteration 89, loss = 0.41822461
Iteration 90, loss = 0.41811171
Iteration 91, loss = 0.41827748
Iteration 92, loss = 0.41902676
Iteration 93, loss = 0.41796535
Iteration 94, loss = 0.41846257
Iteration 95, loss = 0.41803013
Iteration 96, loss = 0.41787646
Iteration 97, loss = 0.41695989
Iteration 98, loss = 0.41820177
Iteration 99, loss = 0.41781983
Iteration 100, loss = 0.41701947
Iteration 101, loss = 0.41747410
Iteration 102, loss = 0.41687925
Iteration 103, loss = 0.41699373
Iteration 104, loss = 0.41676256
Iteration 105, loss = 0.41611658
Iteration 106, loss = 0.41633841
Iteration 107, loss = 0.41678402
Iteration 108, loss = 0.41640840
Iteration 109, loss = 0.41610507
Iteration 110, loss = 0.41668519
Iteration 111, loss = 0.41619129
Iteration 112, loss = 0.41691277
Iteration 113, loss = 0.41499740
Iteration 114, loss = 0.41585983
Iteration 115, loss = 0.41524634
Iteration 116, loss = 0.41477634
Iteration 117, loss = 0.41470811
Iteration 118, loss = 0.41471994
Iteration 119, loss = 0.41501884
Iteration 120, loss = 0.41419214
Iteration 121, loss = 0.41456348
Iteration 122, loss = 0.41467362
Iteration 123, loss = 0.41446078
Iteration 124, loss = 0.41492747
Iteration 125, loss = 0.41450348
Iteration 126, loss = 0.41461291
Iteration 127, loss = 0.41420071
Iteration 128, loss = 0.41424512
Iteration 129, loss = 0.41431922
Iteration 130, loss = 0.41411811
Iteration 131, loss = 0.41316498
Iteration 132, loss = 0.41344770
Iteration 133, loss = 0.41394380
Iteration 134, loss = 0.41356579
Iteration 135, loss = 0.41328616
Iteration 136, loss = 0.41374969
Iteration 137, loss = 0.41223499
Iteration 138, loss = 0.41214476
Iteration 139, loss = 0.41283132
Iteration 140, loss = 0.41179208
Iteration 141, loss = 0.41319337
Iteration 142, loss = 0.41245218
Iteration 143, loss = 0.41246383
Iteration 144, loss = 0.41250951
Iteration 145, loss = 0.41172789
Iteration 146, loss = 0.41219744
Iteration 147, loss = 0.41240804
Iteration 148, loss = 0.41198680
Iteration 149, loss = 0.41217176
Iteration 150, loss = 0.41166818
Iteration 151, loss = 0.41229222
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--pca_10d-iter-1.pkl
"ds1 (pca_10d)",0.808,0.829,0.782,0.769,0.797,0.795,0.816,0.767,0.755,0.783,544.6156647205353
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-ica-5.pkl
Iteration 1, loss = 0.66259244
Iteration 2, loss = 0.57405574
Iteration 3, loss = 0.56294479
Iteration 4, loss = 0.56038800
Iteration 5, loss = 0.55935477
Iteration 6, loss = 0.55860488
Iteration 7, loss = 0.55854071
Iteration 8, loss = 0.55644712
Iteration 9, loss = 0.55603560
Iteration 10, loss = 0.55613484
Iteration 11, loss = 0.55221378
Iteration 12, loss = 0.55243734
Iteration 13, loss = 0.54946294
Iteration 14, loss = 0.54705745
Iteration 15, loss = 0.54783370
Iteration 16, loss = 0.54606105
Iteration 17, loss = 0.54089818
Iteration 18, loss = 0.53773070
Iteration 19, loss = 0.53412272
Iteration 20, loss = 0.53090469
Iteration 21, loss = 0.53054939
Iteration 22, loss = 0.52480844
Iteration 23, loss = 0.52118122
Iteration 24, loss = 0.52186934
Iteration 25, loss = 0.51934593
Iteration 26, loss = 0.51778095
Iteration 27, loss = 0.51662297
Iteration 28, loss = 0.51390387
Iteration 29, loss = 0.51364899
Iteration 30, loss = 0.51290857
Iteration 31, loss = 0.50982080
Iteration 32, loss = 0.51117555
Iteration 33, loss = 0.50906381
Iteration 34, loss = 0.50776914
Iteration 35, loss = 0.50786649
Iteration 36, loss = 0.50717439
Iteration 37, loss = 0.50551439
Iteration 38, loss = 0.50783265
Iteration 39, loss = 0.50652601
Iteration 40, loss = 0.50368750
Iteration 41, loss = 0.50369631
Iteration 42, loss = 0.50291793
Iteration 43, loss = 0.50239863
Iteration 44, loss = 0.50271051
Iteration 45, loss = 0.50184395
Iteration 46, loss = 0.50095344
Iteration 47, loss = 0.50138583
Iteration 48, loss = 0.50138860
Iteration 49, loss = 0.49960002
Iteration 50, loss = 0.49873408
Iteration 51, loss = 0.49883840
Iteration 52, loss = 0.49981087
Iteration 53, loss = 0.49908365
Iteration 54, loss = 0.50015444
Iteration 55, loss = 0.49767080
Iteration 56, loss = 0.49822579
Iteration 57, loss = 0.49775905
Iteration 58, loss = 0.49834908
Iteration 59, loss = 0.49875176
Iteration 60, loss = 0.49832983
Iteration 61, loss = 0.49953063
Iteration 62, loss = 0.49918681
Iteration 63, loss = 0.49595282
Iteration 64, loss = 0.49618983
Iteration 65, loss = 0.49701466
Iteration 66, loss = 0.49603444
Iteration 67, loss = 0.49305352
Iteration 68, loss = 0.49587113
Iteration 69, loss = 0.49588605
Iteration 70, loss = 0.49485566
Iteration 71, loss = 0.49390797
Iteration 72, loss = 0.49439394
Iteration 73, loss = 0.49725637
Iteration 74, loss = 0.49437432
Iteration 75, loss = 0.49410794
Iteration 76, loss = 0.49574931
Iteration 77, loss = 0.49378420
Iteration 78, loss = 0.49279233
Iteration 79, loss = 0.49467660
Iteration 80, loss = 0.49438997
Iteration 81, loss = 0.49532399
Iteration 82, loss = 0.49420503
Iteration 83, loss = 0.49450129
Iteration 84, loss = 0.49631413
Iteration 85, loss = 0.49495459
Iteration 86, loss = 0.49470580
Iteration 87, loss = 0.49410358
Iteration 88, loss = 0.49521588
Iteration 89, loss = 0.49320813
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--ica_5d-iter-1.pkl
"ds1 (ica_5d)",0.768,0.792,0.731,0.721,0.753,0.765,0.795,0.724,0.715,0.750,302.78720450401306
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-ica-10.pkl
Iteration 1, loss = 0.63472418
Iteration 2, loss = 0.53113131
Iteration 3, loss = 0.51927152
Iteration 4, loss = 0.51275902
Iteration 5, loss = 0.51122665
Iteration 6, loss = 0.50653141
Iteration 7, loss = 0.50187087
Iteration 8, loss = 0.50028104
Iteration 9, loss = 0.50280593
Iteration 10, loss = 0.49596087
Iteration 11, loss = 0.49456863
Iteration 12, loss = 0.49323496
Iteration 13, loss = 0.49269740
Iteration 14, loss = 0.49061571
Iteration 15, loss = 0.48922847
Iteration 16, loss = 0.48597065
Iteration 17, loss = 0.48334899
Iteration 18, loss = 0.48438899
Iteration 19, loss = 0.47975533
Iteration 20, loss = 0.48052564
Iteration 21, loss = 0.47826728
Iteration 22, loss = 0.47616663
Iteration 23, loss = 0.47586084
Iteration 24, loss = 0.47177126
Iteration 25, loss = 0.47119317
Iteration 26, loss = 0.47276518
Iteration 27, loss = 0.47149783
Iteration 28, loss = 0.47021160
Iteration 29, loss = 0.46731649
Iteration 30, loss = 0.46563426
Iteration 31, loss = 0.46760735
Iteration 32, loss = 0.46850243
Iteration 33, loss = 0.46687343
Iteration 34, loss = 0.46376179
Iteration 35, loss = 0.46116493
Iteration 36, loss = 0.46339587
Iteration 37, loss = 0.46473912
Iteration 38, loss = 0.46114013
Iteration 39, loss = 0.46163385
Iteration 40, loss = 0.46219939
Iteration 41, loss = 0.46031831
Iteration 42, loss = 0.46014024
Iteration 43, loss = 0.45950181
Iteration 44, loss = 0.45943191
Iteration 45, loss = 0.46132605
Iteration 46, loss = 0.45732421
Iteration 47, loss = 0.45753098
Iteration 48, loss = 0.45589967
Iteration 49, loss = 0.45653935
Iteration 50, loss = 0.46045208
Iteration 51, loss = 0.45518400
Iteration 52, loss = 0.45761261
Iteration 53, loss = 0.45418096
Iteration 54, loss = 0.45460323
Iteration 55, loss = 0.45507681
Iteration 56, loss = 0.45557089
Iteration 57, loss = 0.45497691
Iteration 58, loss = 0.45237990
Iteration 59, loss = 0.45307908
Iteration 60, loss = 0.45142567
Iteration 61, loss = 0.45519503
Iteration 62, loss = 0.45326878
Iteration 63, loss = 0.45023401
Iteration 64, loss = 0.45180196
Iteration 65, loss = 0.46905186
Iteration 66, loss = 0.45332471
Iteration 67, loss = 0.45222131
Iteration 68, loss = 0.45220799
Iteration 69, loss = 0.45130542
Iteration 70, loss = 0.45107642
Iteration 71, loss = 0.45051825
Iteration 72, loss = 0.45553866
Iteration 73, loss = 0.45167595
Iteration 74, loss = 0.44917703
Iteration 75, loss = 0.44840587
Iteration 76, loss = 0.44884751
Iteration 77, loss = 0.45360355
Iteration 78, loss = 0.44661240
Iteration 79, loss = 0.44865852
Iteration 80, loss = 0.44745403
Iteration 81, loss = 0.44849586
Iteration 82, loss = 0.45058123
Iteration 83, loss = 0.44839377
Iteration 84, loss = 0.44845219
Iteration 85, loss = 0.44757168
Iteration 86, loss = 0.44729960
Iteration 87, loss = 0.45035864
Iteration 88, loss = 0.44636528
Iteration 89, loss = 0.44583609
Iteration 90, loss = 0.44822372
Iteration 91, loss = 0.44753974
Iteration 92, loss = 0.44782545
Iteration 93, loss = 0.44672522
Iteration 94, loss = 0.44682286
Iteration 95, loss = 0.44650525
Iteration 96, loss = 0.44638754
Iteration 97, loss = 0.44523989
Iteration 98, loss = 0.44555688
Iteration 99, loss = 0.44428774
Iteration 100, loss = 0.44498807
Iteration 101, loss = 0.44552929
Iteration 102, loss = 0.44338641
Iteration 103, loss = 0.44365662
Iteration 104, loss = 0.44725814
Iteration 105, loss = 0.44460750
Iteration 106, loss = 0.44617861
Iteration 107, loss = 0.44386797
Iteration 108, loss = 0.44455440
Iteration 109, loss = 0.44260409
Iteration 110, loss = 0.44539789
Iteration 111, loss = 0.44470166
Iteration 112, loss = 0.44360853
Iteration 113, loss = 0.44319734
Iteration 114, loss = 0.44441104
Iteration 115, loss = 0.44530380
Iteration 116, loss = 0.44111240
Iteration 117, loss = 0.44391506
Iteration 118, loss = 0.44303601
Iteration 119, loss = 0.44257957
Iteration 120, loss = 0.44311281
Iteration 121, loss = 0.44287897
Iteration 122, loss = 0.44315091
Iteration 123, loss = 0.44398435
Iteration 124, loss = 0.44153240
Iteration 125, loss = 0.44347444
Iteration 126, loss = 0.44353822
Iteration 127, loss = 0.44040345
Iteration 128, loss = 0.44202705
Iteration 129, loss = 0.44116893
Iteration 130, loss = 0.44483973
Iteration 131, loss = 0.44239521
Iteration 132, loss = 0.44085857
Iteration 133, loss = 0.44423458
Iteration 134, loss = 0.44058380
Iteration 135, loss = 0.44221216
Iteration 136, loss = 0.44244987
Iteration 137, loss = 0.44135639
Iteration 138, loss = 0.44107599
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--ica_10d-iter-1.pkl
"ds1 (ica_10d)",0.790,0.829,0.753,0.741,0.778,0.787,0.826,0.750,0.738,0.775,480.24673295021057
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-rp-5.pkl
Iteration 1, loss = 0.55090695
Iteration 2, loss = 0.54055339
Iteration 3, loss = 0.53796377
Iteration 4, loss = 0.53643675
Iteration 5, loss = 0.53527911
Iteration 6, loss = 0.53457652
Iteration 7, loss = 0.53347793
Iteration 8, loss = 0.53314814
Iteration 9, loss = 0.53315865
Iteration 10, loss = 0.53281984
Iteration 11, loss = 0.53233444
Iteration 12, loss = 0.53193998
Iteration 13, loss = 0.53200090
Iteration 14, loss = 0.53191346
Iteration 15, loss = 0.53135805
Iteration 16, loss = 0.53097027
Iteration 17, loss = 0.53135769
Iteration 18, loss = 0.53096631
Iteration 19, loss = 0.53041880
Iteration 20, loss = 0.53084339
Iteration 21, loss = 0.53010125
Iteration 22, loss = 0.52996872
Iteration 23, loss = 0.53037756
Iteration 24, loss = 0.53011954
Iteration 25, loss = 0.52988609
Iteration 26, loss = 0.52923211
Iteration 27, loss = 0.52945362
Iteration 28, loss = 0.52893041
Iteration 29, loss = 0.52888583
Iteration 30, loss = 0.52893355
Iteration 31, loss = 0.52813190
Iteration 32, loss = 0.52766987
Iteration 33, loss = 0.52738035
Iteration 34, loss = 0.52664544
Iteration 35, loss = 0.52917913
Iteration 36, loss = 0.52880568
Iteration 37, loss = 0.52825258
Iteration 38, loss = 0.52772889
Iteration 39, loss = 0.52628234
Iteration 40, loss = 0.52514621
Iteration 41, loss = 0.52827221
Iteration 42, loss = 0.52596871
Iteration 43, loss = 0.52628503
Iteration 44, loss = 0.52465704
Iteration 45, loss = 0.52758088
Iteration 46, loss = 0.52791592
Iteration 47, loss = 0.52873138
Iteration 48, loss = 0.52777890
Iteration 49, loss = 0.52848257
Iteration 50, loss = 0.52829269
Iteration 51, loss = 0.52623613
Iteration 52, loss = 0.52543434
Iteration 53, loss = 0.52910552
Iteration 54, loss = 0.52777795
Iteration 55, loss = 0.52723850
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--rp_5d-iter-1.pkl
"ds1 (rp_5d)",0.729,0.769,0.668,0.667,0.708,0.725,0.763,0.663,0.663,0.704,192.01642227172852
Read /home/bb/cs7641/proj3/datasets/ds1/dimreducer-rp-10.pkl
Iteration 1, loss = 0.49708751
Iteration 2, loss = 0.47798733
Iteration 3, loss = 0.46824989
Iteration 4, loss = 0.46090861
Iteration 5, loss = 0.45488662
Iteration 6, loss = 0.45270099
Iteration 7, loss = 0.44961993
Iteration 8, loss = 0.44658381
Iteration 9, loss = 0.44573693
Iteration 10, loss = 0.44582279
Iteration 11, loss = 0.44305325
Iteration 12, loss = 0.44218140
Iteration 13, loss = 0.44310165
Iteration 14, loss = 0.44148564
Iteration 15, loss = 0.44057968
Iteration 16, loss = 0.44147924
Iteration 17, loss = 0.44015900
Iteration 18, loss = 0.43854647
Iteration 19, loss = 0.43813330
Iteration 20, loss = 0.43692479
Iteration 21, loss = 0.43711050
Iteration 22, loss = 0.43647774
Iteration 23, loss = 0.43667940
Iteration 24, loss = 0.43551723
Iteration 25, loss = 0.43615494
Iteration 26, loss = 0.43441845
Iteration 27, loss = 0.43321007
Iteration 28, loss = 0.43323554
Iteration 29, loss = 0.43327799
Iteration 30, loss = 0.43322853
Iteration 31, loss = 0.43261632
Iteration 32, loss = 0.43234737
Iteration 33, loss = 0.43168397
Iteration 34, loss = 0.43165050
Iteration 35, loss = 0.43022508
Iteration 36, loss = 0.43025409
Iteration 37, loss = 0.43065162
Iteration 38, loss = 0.42900829
Iteration 39, loss = 0.43002887
Iteration 40, loss = 0.43000029
Iteration 41, loss = 0.43001619
Iteration 42, loss = 0.42758869
Iteration 43, loss = 0.42839421
Iteration 44, loss = 0.42829607
Iteration 45, loss = 0.42844588
Iteration 46, loss = 0.42736120
Iteration 47, loss = 0.42777320
Iteration 48, loss = 0.42691546
Iteration 49, loss = 0.42777339
Iteration 50, loss = 0.42711761
Iteration 51, loss = 0.42625111
Iteration 52, loss = 0.42679664
Iteration 53, loss = 0.42820706
Iteration 54, loss = 0.42600969
Iteration 55, loss = 0.42690663
Iteration 56, loss = 0.42507609
Iteration 57, loss = 0.42538202
Iteration 58, loss = 0.42549477
Iteration 59, loss = 0.42434902
Iteration 60, loss = 0.42537245
Iteration 61, loss = 0.42345991
Iteration 62, loss = 0.42515908
Iteration 63, loss = 0.42386532
Iteration 64, loss = 0.42407617
Iteration 65, loss = 0.42413095
Iteration 66, loss = 0.42323999
Iteration 67, loss = 0.42393126
Iteration 68, loss = 0.42377691
Iteration 69, loss = 0.42265657
Iteration 70, loss = 0.42273110
Iteration 71, loss = 0.42278034
Iteration 72, loss = 0.42198890
Iteration 73, loss = 0.42231191
Iteration 74, loss = 0.42189989
Iteration 75, loss = 0.42187950
Iteration 76, loss = 0.42222884
Iteration 77, loss = 0.42179028
Iteration 78, loss = 0.42074996
Iteration 79, loss = 0.42275751
Iteration 80, loss = 0.42139659
Iteration 81, loss = 0.42194579
Iteration 82, loss = 0.42172733
Iteration 83, loss = 0.41994925
Iteration 84, loss = 0.42017487
Iteration 85, loss = 0.42323790
Iteration 86, loss = 0.42233591
Iteration 87, loss = 0.42027190
Iteration 88, loss = 0.41905359
Iteration 89, loss = 0.41965783
Iteration 90, loss = 0.41980104
Iteration 91, loss = 0.41993060
Iteration 92, loss = 0.42067263
Iteration 93, loss = 0.41936474
Iteration 94, loss = 0.41897628
Iteration 95, loss = 0.41854877
Iteration 96, loss = 0.41891903
Iteration 97, loss = 0.41872600
Iteration 98, loss = 0.41857423
Iteration 99, loss = 0.41822924
Iteration 100, loss = 0.41893502
Iteration 101, loss = 0.41815509
Iteration 102, loss = 0.41862991
Iteration 103, loss = 0.41758355
Iteration 104, loss = 0.41769447
Iteration 105, loss = 0.41711355
Iteration 106, loss = 0.41672479
Iteration 107, loss = 0.41701573
Iteration 108, loss = 0.41744438
Iteration 109, loss = 0.41657393
Iteration 110, loss = 0.41734466
Iteration 111, loss = 0.41635092
Iteration 112, loss = 0.41731420
Iteration 113, loss = 0.41617559
Iteration 114, loss = 0.41748493
Iteration 115, loss = 0.41616781
Iteration 116, loss = 0.41692212
Iteration 117, loss = 0.41617978
Iteration 118, loss = 0.41572512
Iteration 119, loss = 0.41579697
Iteration 120, loss = 0.41498747
Iteration 121, loss = 0.41544730
Iteration 122, loss = 0.41484022
Iteration 123, loss = 0.41483800
Iteration 124, loss = 0.41484458
Iteration 125, loss = 0.41566466
Iteration 126, loss = 0.41487182
Iteration 127, loss = 0.41513045
Iteration 128, loss = 0.41616027
Iteration 129, loss = 0.41531355
Iteration 130, loss = 0.41488518
Iteration 131, loss = 0.41475584
Iteration 132, loss = 0.41364201
Iteration 133, loss = 0.41463965
Iteration 134, loss = 0.41337287
Iteration 135, loss = 0.41390844
Iteration 136, loss = 0.41461486
Iteration 137, loss = 0.41331667
Iteration 138, loss = 0.41282367
Iteration 139, loss = 0.41413578
Iteration 140, loss = 0.41263765
Iteration 141, loss = 0.41456683
Iteration 142, loss = 0.41307234
Iteration 143, loss = 0.41296742
Iteration 144, loss = 0.41380023
Iteration 145, loss = 0.41320432
Iteration 146, loss = 0.41278329
Iteration 147, loss = 0.41263347
Iteration 148, loss = 0.41171481
Iteration 149, loss = 0.41239134
Iteration 150, loss = 0.41327522
Iteration 151, loss = 0.41131567
Iteration 152, loss = 0.41231675
Iteration 153, loss = 0.41217538
Iteration 154, loss = 0.41115081
Iteration 155, loss = 0.41132187
Iteration 156, loss = 0.41205317
Iteration 157, loss = 0.41114332
Iteration 158, loss = 0.41165000
Iteration 159, loss = 0.41188674
Iteration 160, loss = 0.41137733
Iteration 161, loss = 0.41048142
Iteration 162, loss = 0.41095714
Iteration 163, loss = 0.41193413
Iteration 164, loss = 0.41055999
Iteration 165, loss = 0.41040319
Iteration 166, loss = 0.41095852
Iteration 167, loss = 0.41099340
Iteration 168, loss = 0.41046803
Iteration 169, loss = 0.41003210
Iteration 170, loss = 0.40949803
Iteration 171, loss = 0.41085393
Iteration 172, loss = 0.40999503
Iteration 173, loss = 0.40958707
Iteration 174, loss = 0.40993149
Iteration 175, loss = 0.40914105
Iteration 176, loss = 0.40991384
Iteration 177, loss = 0.40980573
Iteration 178, loss = 0.40896211
Iteration 179, loss = 0.40977656
Iteration 180, loss = 0.40924877
Iteration 181, loss = 0.40906909
Iteration 182, loss = 0.40929544
Iteration 183, loss = 0.40812785
Iteration 184, loss = 0.40849683
Iteration 185, loss = 0.41030486
Iteration 186, loss = 0.40856298
Iteration 187, loss = 0.40960889
Iteration 188, loss = 0.40818108
Iteration 189, loss = 0.40831243
Iteration 190, loss = 0.40786371
Iteration 191, loss = 0.40810147
Iteration 192, loss = 0.40818446
Iteration 193, loss = 0.40785341
Iteration 194, loss = 0.40838383
Iteration 195, loss = 0.40846166
Iteration 196, loss = 0.40898891
Iteration 197, loss = 0.40964887
Iteration 198, loss = 0.40820200
Iteration 199, loss = 0.40621022
Iteration 200, loss = 0.40851142
Iteration 201, loss = 0.40759340
Iteration 202, loss = 0.40646021
Iteration 203, loss = 0.40726187
Iteration 204, loss = 0.40636566
Iteration 205, loss = 0.40793608
Iteration 206, loss = 0.40673402
Iteration 207, loss = 0.40738162
Iteration 208, loss = 0.41029775
Iteration 209, loss = 0.40791954
Iteration 210, loss = 0.40591962
Iteration 211, loss = 0.40607006
Iteration 212, loss = 0.40730120
Iteration 213, loss = 0.40719689
Iteration 214, loss = 0.40691888
Iteration 215, loss = 0.40620310
Iteration 216, loss = 0.40606300
Iteration 217, loss = 0.40703338
Iteration 218, loss = 0.40704800
Iteration 219, loss = 0.40613132
Iteration 220, loss = 0.40632105
Iteration 221, loss = 0.40596473
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--rp_10d-iter-1.pkl
"ds1 (rp_10d)",0.811,0.834,0.785,0.772,0.800,0.797,0.819,0.768,0.755,0.785,778.0225956439972
k=5
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")
** -> lda projection calculation for ds1, k=5 elapsed seconds 0.29186391830444336 <- took less than 1 second
Wrote /home/bb/cs7641/proj3/datasets/ds1/dimreducer-lda-5.pkl
Iteration 1, loss = 0.53503074
Iteration 2, loss = 0.52591223
Iteration 3, loss = 0.52396239
Iteration 4, loss = 0.52146978
Iteration 5, loss = 0.51992166
Iteration 6, loss = 0.51790100
Iteration 7, loss = 0.51689759
Iteration 8, loss = 0.51626106
Iteration 9, loss = 0.51590255
Iteration 10, loss = 0.51481256
Iteration 11, loss = 0.51526968
Iteration 12, loss = 0.51434658
Iteration 13, loss = 0.51398715
Iteration 14, loss = 0.51362870
Iteration 15, loss = 0.51286367
Iteration 16, loss = 0.51215691
Iteration 17, loss = 0.51179576
Iteration 18, loss = 0.51106481
Iteration 19, loss = 0.51045644
Iteration 20, loss = 0.50965308
Iteration 21, loss = 0.50881878
Iteration 22, loss = 0.50893584
Iteration 23, loss = 0.50764849
Iteration 24, loss = 0.50785797
Iteration 25, loss = 0.50675922
Iteration 26, loss = 0.50824195
Iteration 27, loss = 0.50749566
Iteration 28, loss = 0.50674279
Iteration 29, loss = 0.50794657
Iteration 30, loss = 0.50651265
Iteration 31, loss = 0.50740108
Iteration 32, loss = 0.50663649
Iteration 33, loss = 0.50689842
Iteration 34, loss = 0.50685170
Iteration 35, loss = 0.50721333
Iteration 36, loss = 0.50743754
Iteration 37, loss = 0.50605943
Iteration 38, loss = 0.50674505
Iteration 39, loss = 0.50580904
Iteration 40, loss = 0.50627351
Iteration 41, loss = 0.50607630
Iteration 42, loss = 0.50685624
Iteration 43, loss = 0.50644759
Iteration 44, loss = 0.50618915
Iteration 45, loss = 0.50599188
Iteration 46, loss = 0.50608177
Iteration 47, loss = 0.50590412
Iteration 48, loss = 0.50645708
Iteration 49, loss = 0.50617342
Iteration 50, loss = 0.50621621
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--lda_5d-iter-1.pkl
"ds1 (lda_5d)",0.764,0.802,0.720,0.711,0.749,0.765,0.802,0.720,0.711,0.749,163.1223750114441
k=10
/home/bb/cs7641/proj3/venv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")
** -> lda projection calculation for ds1, k=10 elapsed seconds 0.2724435329437256 <- took less than 1 second
Wrote /home/bb/cs7641/proj3/datasets/ds1/dimreducer-lda-10.pkl
Iteration 1, loss = 0.53503074
Iteration 2, loss = 0.52591223
Iteration 3, loss = 0.52396239
Iteration 4, loss = 0.52146978
Iteration 5, loss = 0.51992166
Iteration 6, loss = 0.51790100
Iteration 7, loss = 0.51689759
Iteration 8, loss = 0.51626106
Iteration 9, loss = 0.51590255
Iteration 10, loss = 0.51481256
Iteration 11, loss = 0.51526968
Iteration 12, loss = 0.51434658
Iteration 13, loss = 0.51398715
Iteration 14, loss = 0.51362870
Iteration 15, loss = 0.51286367
Iteration 16, loss = 0.51215691
Iteration 17, loss = 0.51179576
Iteration 18, loss = 0.51106481
Iteration 19, loss = 0.51045644
Iteration 20, loss = 0.50965308
Iteration 21, loss = 0.50881878
Iteration 22, loss = 0.50893584
Iteration 23, loss = 0.50764849
Iteration 24, loss = 0.50785797
Iteration 25, loss = 0.50675922
Iteration 26, loss = 0.50824195
Iteration 27, loss = 0.50749566
Iteration 28, loss = 0.50674279
Iteration 29, loss = 0.50794657
Iteration 30, loss = 0.50651265
Iteration 31, loss = 0.50740108
Iteration 32, loss = 0.50663649
Iteration 33, loss = 0.50689842
Iteration 34, loss = 0.50685170
Iteration 35, loss = 0.50721333
Iteration 36, loss = 0.50743754
Iteration 37, loss = 0.50605943
Iteration 38, loss = 0.50674505
Iteration 39, loss = 0.50580904
Iteration 40, loss = 0.50627351
Iteration 41, loss = 0.50607630
Iteration 42, loss = 0.50685624
Iteration 43, loss = 0.50644759
Iteration 44, loss = 0.50618915
Iteration 45, loss = 0.50599188
Iteration 46, loss = 0.50608177
Iteration 47, loss = 0.50590412
Iteration 48, loss = 0.50645708
Iteration 49, loss = 0.50617342
Iteration 50, loss = 0.50621621
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Wrote datasets/ds1/mlp-model--200-200-200--lda_10d-iter-1.pkl
"ds1 (lda_10d)",0.764,0.802,0.720,0.711,0.749,0.765,0.802,0.720,0.711,0.749,164.81114745140076
Write part4_report.csv
